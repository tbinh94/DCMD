{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11715637,"sourceType":"datasetVersion","datasetId":7353969},{"sourceId":11760202,"sourceType":"datasetVersion","datasetId":7382794}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/guijiejie/DCMD-main.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:10:01.076914Z","iopub.execute_input":"2025-06-03T01:10:01.077111Z","iopub.status.idle":"2025-06-03T01:10:01.890494Z","shell.execute_reply.started":"2025-06-03T01:10:01.077093Z","shell.execute_reply":"2025-06-03T01:10:01.889412Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DCMD-main'...\nremote: Enumerating objects: 74, done.\u001b[K\nremote: Counting objects: 100% (74/74), done.\u001b[K\nremote: Compressing objects: 100% (64/64), done.\u001b[K\nremote: Total 74 (delta 22), reused 48 (delta 7), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (74/74), 262.60 KiB | 3.55 MiB/s, done.\nResolving deltas: 100% (22/22), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Pip**","metadata":{}},{"cell_type":"code","source":"!pip install appdirs==1.4.4 docker-pycreds==0.4.0 gitdb==4.0.10 gitpython==3.1.32 joblib==1.3.1 numpy==1.25.2 pathtools==0.1.2 protobuf==4.23.4 scikit-learn==1.3.0 scipy==1.11.1 sentry-sdk==1.29.2 setproctitle==1.3.2 smmap==5.0.0 threadpoolctl==3.2.0 wandb==0.15.8\n!pip install --upgrade pytorch-lightning pyyaml wandb pandas numpy matplotlib matplotlib-inline scikit-learn tqdm\n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:10:09.642431Z","iopub.execute_input":"2025-06-03T01:10:09.643333Z","iopub.status.idle":"2025-06-03T01:15:18.097051Z","shell.execute_reply.started":"2025-06-03T01:10:09.643287Z","shell.execute_reply":"2025-06-03T01:15:18.096044Z"}},"outputs":[{"name":"stdout","text":"Collecting appdirs==1.4.4\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.11/dist-packages (0.4.0)\nCollecting gitdb==4.0.10\n  Downloading gitdb-4.0.10-py3-none-any.whl.metadata (1.1 kB)\nCollecting gitpython==3.1.32\n  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\nCollecting joblib==1.3.1\n  Downloading joblib-1.3.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting numpy==1.25.2\n  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting pathtools==0.1.2\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting protobuf==4.23.4\n  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\nCollecting scikit-learn==1.3.0\n  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting scipy==1.11.1\n  Downloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentry-sdk==1.29.2\n  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata (8.8 kB)\nCollecting setproctitle==1.3.2\n  Downloading setproctitle-1.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\nCollecting smmap==5.0.0\n  Downloading smmap-5.0.0-py3-none-any.whl.metadata (4.2 kB)\nCollecting threadpoolctl==3.2.0\n  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\nCollecting wandb==0.15.8\n  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds==0.4.0) (1.17.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from sentry-sdk==1.29.2) (2025.1.31)\nRequirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.11/dist-packages (from sentry-sdk==1.29.2) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (8.1.8)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (2.32.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (7.0.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (6.0.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (75.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.8) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.8) (3.10)\nDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading joblib-1.3.1-py3-none-any.whl (301 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setproctitle-1.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\nDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\nDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\nDownloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pathtools\n  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=492ea6045228194d82e1aea9f9d595836f67571dea2dd1269dd3f8ec30c7f21a\n  Stored in directory: /root/.cache/pip/wheels/ea/b7/8b/84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\nSuccessfully built pathtools\nInstalling collected packages: pathtools, appdirs, threadpoolctl, smmap, setproctitle, sentry-sdk, protobuf, numpy, joblib, scipy, gitdb, scikit-learn, gitpython, wandb\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: smmap\n    Found existing installation: smmap 5.0.2\n    Uninstalling smmap-5.0.2:\n      Successfully uninstalled smmap-5.0.2\n  Attempting uninstall: setproctitle\n    Found existing installation: setproctitle 1.3.4\n    Uninstalling setproctitle-1.3.4:\n      Successfully uninstalled setproctitle-1.3.4\n  Attempting uninstall: sentry-sdk\n    Found existing installation: sentry-sdk 2.21.0\n    Uninstalling sentry-sdk-2.21.0:\n      Successfully uninstalled sentry-sdk-2.21.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.4.2\n    Uninstalling joblib-1.4.2:\n      Successfully uninstalled joblib-1.4.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n  Attempting uninstall: gitdb\n    Found existing installation: gitdb 4.0.12\n    Uninstalling gitdb-4.0.12:\n      Successfully uninstalled gitdb-4.0.12\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: gitpython\n    Found existing installation: GitPython 3.1.44\n    Uninstalling GitPython-3.1.44:\n      Successfully uninstalled GitPython-3.1.44\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.6\n    Uninstalling wandb-0.19.6:\n      Successfully uninstalled wandb-0.19.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.23.4 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.0 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.23.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\nscikit-image 0.25.1 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.0 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.25.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 gitdb-4.0.10 gitpython-3.1.32 joblib-1.3.1 numpy-1.25.2 pathtools-0.1.2 protobuf-4.23.4 scikit-learn-1.3.0 scipy-1.11.1 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 threadpoolctl-3.2.0 wandb-0.15.8\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\nCollecting pytorch-lightning\n  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.15.8)\nCollecting wandb\n  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.25.2)\nCollecting numpy\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nCollecting matplotlib\n  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (0.1.7)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting scikit-learn\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.1)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.32)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.23.4)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nCollecting sentry-sdk>=2.0.0 (from wandb)\n  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline) (5.7.1)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.16)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nCollecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nDownloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.6/341.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentry-sdk, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, wandb, scikit-learn, nvidia-cusolver-cu12, matplotlib, pytorch-lightning\n  Attempting uninstall: sentry-sdk\n    Found existing installation: sentry-sdk 1.29.2\n    Uninstalling sentry-sdk-1.29.2:\n      Successfully uninstalled sentry-sdk-1.29.2\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.25.2\n    Uninstalling numpy-1.25.2:\n      Successfully uninstalled numpy-1.25.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.8\n    Uninstalling wandb-0.15.8:\n      Successfully uninstalled wandb-0.15.8\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.3.0\n    Uninstalling scikit-learn-1.3.0:\n      Successfully uninstalled scikit-learn-1.3.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.5.1\n    Uninstalling pytorch-lightning-2.5.1:\n      Successfully uninstalled pytorch-lightning-2.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.3 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.1 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed matplotlib-3.10.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1.post0 scikit-learn-1.6.1 sentry-sdk-2.29.1 wandb-0.19.11\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.3.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m999.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu124\n    Uninstalling torchaudio-2.5.1+cu124:\n      Successfully uninstalled torchaudio-2.5.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Replace code to run properly**","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml \n### Experiment configuration\n\n## General settings\nsplit: 'train' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'train_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'best.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: true\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: false # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (his+pre)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding \n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:33.261010Z","iopub.execute_input":"2025-06-03T01:15:33.261789Z","iopub.status.idle":"2025-06-03T01:15:33.269772Z","shell.execute_reply.started":"2025-06-03T01:15:33.261759Z","shell.execute_reply":"2025-06-03T01:15:33.269031Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/transformer.py\nimport torch\nimport torch.nn.functional as F\nfrom torch import layer_norm, nn\nimport numpy as np\nfrom typing import List, Tuple, Union\n\nimport math\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef set_requires_grad(nets, requires_grad=False):\n    \"\"\"Set requies_grad for all the networks.\n\n    Args:\n        nets (nn.Module | list[nn.Module]): A list of networks or a single\n            network.\n        requires_grad (bool): Whether the networks require gradients or not\n    \"\"\"\n    if not isinstance(nets, list):\n        nets = [nets]\n    for net in nets:\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass StylizationBlock(nn.Module):\n\n    def __init__(self, latent_dim, time_embed_dim, dropout):\n        super().__init__()\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, 2 * latent_dim),\n        )\n        self.norm = nn.LayerNorm(latent_dim)\n        self.out_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Linear(latent_dim, latent_dim)),\n        )\n\n    def forward(self, h, emb):\n        \"\"\"\n        h: B, T, D\n        emb: B, D\n        \"\"\"\n        # B, 1, 2D\n        emb_out = self.emb_layers(emb).unsqueeze(1)\n        # scale: B, 1, D / shift: B, 1, D\n        scale, shift = torch.chunk(emb_out, 2, dim=2)\n        # B, T, D\n        h = self.norm(h) * (1 + scale) + shift\n        h = self.out_layers(h)\n        return h\n\n\nclass FFN(nn.Module):\n\n    def __init__(self, latent_dim, ffn_dim, dropout, time_embed_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(latent_dim, ffn_dim)\n        self.linear2 = zero_module(nn.Linear(ffn_dim, latent_dim))\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n\n    def forward(self, x, emb):\n        \"\"\"\n            x: B, T, D (D=latent_dim)\n        \"\"\"\n        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        y = x + self.proj_out(y, emb)\n        return y\n\n\nclass TemporalSelfAttention(nn.Module):\n\n    def __init__(self, n_frames, latent_dim, num_head, dropout, time_embed_dim, output_attention = True):\n        super().__init__()\n        self.num_head = num_head\n        self.output_attention = output_attention\n        self.norm = nn.LayerNorm(latent_dim)\n        self.query = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.key = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.value = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.sigma_projection = nn.Linear(latent_dim, num_head, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n        n_frames = n_frames\n        self.distances = torch.zeros((n_frames, n_frames)).cuda(0)\n\n        for i in range(n_frames):\n            for j in range(n_frames):\n                self.distances[i][j] = abs(i - j)\n\n    def forward(self, x, emb):\n        \"\"\"\n        x: B, T, D (D=latent_dim)\n        \"\"\"\n        B, T, D = x.shape\n        H = self.num_head\n\n        ## series-association\n        # B, T, 1, D\n        query = self.query(self.norm(x)).unsqueeze(2)\n        # B, 1, T, D\n        key = self.key(self.norm(x)).unsqueeze(1)\n        # B, T, H, D/H\n        query = query.view(B, T, H, -1)\n        key = key.view(B, T, H, -1)\n        scale = 1. / math.sqrt(D/H)\n        # B, H, T, T\n        scores = torch.einsum('bnhd,bmhd->bhnm', query, key) / math.sqrt(D // H)\n        attention = scale * scores\n        # B, H, T, T\n        series = self.dropout(F.softmax(attention, dim=-1))\n\n        ## prior-association\n        sigma = self.sigma_projection(x).view(B, T, H)  # B, T, H\n        sigma = sigma.transpose(1, 2)  # B T H ->  B H T\n        sigma = torch.sigmoid(sigma * 5) + 1e-5\n        sigma = torch.pow(3, sigma) - 1\n        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, T)  # B, H, T, T\n        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1) # B, H, T, T\n        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2)).cuda(0) # B, H, T, T\n\n        # B, T, H, D/H\n        value = self.value(self.norm(x)).view(B, T, H, -1)\n        # B, T, D\n        y = torch.einsum('bhnm,bmhd->bnhd', series, value).reshape(B, T, D)\n        y = x + self.proj_out(y, emb)\n\n        if self.output_attention:\n            return y.contiguous(), series, prior, sigma\n        else:\n            return y.contiguous(), None\n\nclass TemporalDiffusionTransformerDecoderLayer(nn.Module):\n\n    def __init__(self,\n                 n_frames = 7,\n                 latent_dim=16,\n                 time_embed_dim=16,\n                 ffn_dim=32,\n                 num_head=4,\n                 dropout=0.5\n                 ):\n        super().__init__()\n        self.sa_block = TemporalSelfAttention(\n            n_frames, latent_dim, num_head, dropout, time_embed_dim)\n        self.ffn = FFN(latent_dim, ffn_dim, dropout, time_embed_dim)\n\n    def forward(self, x, emb):\n        x, series, prior, sigma = self.sa_block(x, emb)\n        x = self.ffn(x, emb)\n        return x, series, prior, sigma\n\n\nclass MotionTransformer(nn.Module):\n    def __init__(self,\n                 input_feats,\n                 num_frames=7,\n                 latent_dim=16,\n                 ff_size=32,\n                 num_layers=8,\n                 num_heads=8,\n                 dropout=0.2,\n                 activation=\"gelu\",\n                 output_attention = True,\n                 device: Union[str, torch.DeviceObjType] = 'cpu',\n                 inject_condition: bool = False,\n                 **kargs):\n        super().__init__()\n\n\n        self.input_feats = input_feats # 34\n        self.num_frames = num_frames\n        self.latent_dim = latent_dim\n        self.ff_size = ff_size\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.activation = activation\n        self.output_attention = output_attention\n        self.device = device\n        self.time_embed_dim = latent_dim\n        self.inject_condition = inject_condition\n\n        self.build_model()\n\n    def build_model(self):\n        self.sequence_embedding = nn.Parameter(torch.randn(self.num_frames, self.latent_dim))\n\n        # Input Embedding\n        self.joint_embed = nn.Linear(self.input_feats, self.latent_dim)\n        self.cond_embed = nn.Linear(256, self.time_embed_dim)\n\n        self.time_embed = nn.Sequential(\n            nn.Linear(self.latent_dim, self.time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(self.time_embed_dim, self.time_embed_dim),\n        )\n\n        self.temporal_decoder_blocks = nn.ModuleList()\n        for i in range(self.num_layers):\n            self.temporal_decoder_blocks.append(\n                TemporalDiffusionTransformerDecoderLayer(\n                    n_frames=self.num_frames,\n                    latent_dim=self.latent_dim,\n                    time_embed_dim=self.time_embed_dim,\n                    ffn_dim=self.ff_size,\n                    num_head=self.num_heads,\n                    dropout=self.dropout,\n                )\n            )\n        # Output Module\n        self.out = zero_module(nn.Linear(self.latent_dim, self.input_feats))\n\n\n    def forward(self, x, timesteps, condition_data:torch.Tensor=None):\n        \"\"\"\n        x: B, T, D (D=C*V)\n        \"\"\"\n        B, T = x.shape[0], x.shape[1]\n\n        # B, latent_dim\n        emb = self.time_embed(timestep_embedding(timesteps, self.latent_dim))\n\n        # Add conditioning signal\n        if self.inject_condition:\n            condition_data = self.cond_embed(condition_data)\n            emb = emb + condition_data\n\n        # B, T, latent_dim\n        h = self.joint_embed(x)\n        h = h + self.sequence_embedding.unsqueeze(0)[:, :T, :]\n        \n        i = 0\n        prelist = []\n        series_list = []\n        prior_list = []\n        sigma_list = []\n        for module in self.temporal_decoder_blocks:\n            if i < (self.num_layers // 2):\n                prelist.append(h)\n                h, series, prior, sigmas = module(h, emb) # B, T, latent_dim\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n            elif i >= (self.num_layers // 2):\n                h, series, prior, sigmas = module(h, emb)\n                h += prelist[-1]\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n                prelist.pop()\n            i += 1\n\n        # B, T, C*V\n        output = self.out(h).view(B, T, -1).contiguous()\n        if self.output_attention:\n            return output, series_list, prior_list, sigma_list\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:36.443696Z","iopub.execute_input":"2025-06-03T01:15:36.444327Z","iopub.status.idle":"2025-06-03T01:15:36.453352Z","shell.execute_reply.started":"2025-06-03T01:15:36.444303Z","shell.execute_reply":"2025-06-03T01:15:36.452516Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/transformer.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/train_DCMD.py\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport yaml\nfrom models.dcmd import DCMD\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.strategies import DDPStrategy\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\nfrom utils.ema import EMACallback\n\n\nif __name__== '__main__':\n\n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='Pose_AD_Experiment')\n    parser.add_argument('-c', '--config', type=str, required=True,\n                        default='/your_default_config_file_path')\n    \n    args = parser.parse_args()\n    config_path = args.config\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args) \n    # Save config file to ckpt_dir\n    os.system(f'cp {config_path} {os.path.join(args.ckpt_dir, \"config.yaml\")}')     \n    \n    # Set seeds    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed) \n    pl.seed_everything(args.seed)\n\n    # Set callbacks and logger\n    if (hasattr(args, 'diffusion_on_latent') and args.stage == 'pretrain'):\n        monitored_metric = 'pretrain_rec_loss'\n        metric_mode = 'min'\n    elif args.validation:\n        monitored_metric = 'AUC'\n        metric_mode = 'max'\n    else:\n        monitored_metric = 'loss'\n        metric_mode = 'min'\n    callbacks = [ModelCheckpoint(\n                    dirpath=args.ckpt_dir,\n                    save_top_k=2,\n                    save_last=True,\n                    monitor=monitored_metric,\n                    mode=metric_mode\n                )]\n\n    \n    callbacks += [EMACallback()] if args.use_ema else [] # Use to achieve exponential moving average\n    \n    if args.use_wandb:\n        callbacks += [LearningRateMonitor(logging_interval='step')]\n        wandb_logger = WandbLogger(project=args.project_name, group=args.group_name, entity=args.wandb_entity, \n                                   name=args.dir_name, config=vars(args), log_model='all')\n    else:\n        wandb_logger = False\n\n    # Get dataset and loaders\n    _, train_loader, _, val_loader = get_dataset_and_loader(args, split=args.split, validation=args.validation)\n    \n    # Initialize model and trainer\n    model = DCMD(args)\n    \n    trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices, default_root_dir=args.ckpt_dir, max_epochs=args.n_epochs, \n                         logger=wandb_logger, callbacks=callbacks, strategy=DDPStrategy(find_unused_parameters=False),\n                         log_every_n_steps=20, num_sanity_val_steps=0, deterministic=True)\n    \n    # Train the model    \n    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:41.203499Z","iopub.execute_input":"2025-06-03T01:15:41.204305Z","iopub.status.idle":"2025-06-03T01:15:41.209970Z","shell.execute_reply.started":"2025-06-03T01:15:41.204276Z","shell.execute_reply":"2025-06-03T01:15:41.209187Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/train_DCMD.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml\n### Experiment configuration\n\n## General settings\nsplit: 'test' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'test_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'last.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: false # if true, create a new directory for the current experiment\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: true # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (cond+noised)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding\n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:44.112696Z","iopub.execute_input":"2025-06-03T01:15:44.113427Z","iopub.status.idle":"2025-06-03T01:15:44.120077Z","shell.execute_reply.started":"2025-06-03T01:15:44.113403Z","shell.execute_reply":"2025-06-03T01:15:44.119108Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/dcmd.py\nimport argparse\nimport os\nfrom math import prod\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\n\nfrom models.stsae.stsae import STSAE\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nfrom utils.diffusion_utils import Diffusion\nfrom utils.eval_utils import (compute_var_matrix, filter_vectors_by_cond,\n                              get_avenue_mask, get_hr_ubnormal_mask, pad_scores, score_process)\nfrom utils.model_utils import processing_data, my_kl_loss\nfrom models.transformer import MotionTransformer\nfrom utils.tools import get_dct_matrix, generate_pad, padding_traj\n\n\nclass DCMD(pl.LightningModule):\n\n    losses = {'l1': nn.L1Loss, 'smooth_l1': nn.SmoothL1Loss, 'mse': nn.MSELoss}\n    conditioning_strategies = {'inject': 'inject'}\n\n    def __init__(self, args: argparse.Namespace) -> None:\n        \"\"\"\n        This class implements DCMD model.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n        \"\"\"\n\n        super(DCMD, self).__init__()\n\n        ## Log the hyperparameters of the model\n        self.save_hyperparameters(args)\n\n        ## Set the internal variables of the model\n        # Data parameters\n        self.n_frames = args.seg_len\n        self.num_coords = args.num_coords\n        self.n_joints = self._infer_number_of_joint(args)\n\n        ## Model parameters\n        # Main network\n        self.dropout = args.dropout\n        self.conditioning_strategy = self.conditioning_strategies[args.conditioning_strategy]\n        # Conditioning network\n        self.cond_h_dim = args.h_dim\n        self.cond_latent_dim = args.latent_dim\n        self.cond_channels = args.channels\n        self.cond_dropout = args.dropout\n\n        ## Training and inference parameters\n        self.learning_rate = args.opt_lr\n        self.loss_fn = self.losses[args.loss_fn](reduction='none')\n        self.noise_steps = args.noise_steps\n        self.aggregation_strategy = args.aggregation_strategy\n        self.n_generated_samples = args.n_generated_samples\n        self.model_return_value = args.model_return_value\n        self.gt_path = args.gt_path\n        self.split = args.split\n        self.use_hr = args.use_hr\n        self.ckpt_dir = args.ckpt_dir\n        self.save_tensors = args.save_tensors\n        self.num_transforms = args.num_transform\n        self.anomaly_score_pad_size = args.pad_size\n        self.anomaly_score_filter_kernel_size = args.filter_kernel_size\n        self.anomaly_score_frames_shift = args.frames_shift\n        self.dataset_name = args.dataset_choice\n\n        # New parameters\n        self.n_his = args.n_his\n        self.padding = args.padding\n        self.num_layers = args.num_layers\n        self.num_heads = args.num_heads\n        self.latent_dims = args.latent_dims\n        self.automatic_optimization = False\n        self.loss_1_series_weight = args.loss_1_series_weight\n        self.loss_1_prior_weight = args.loss_1_prior_weight\n        self.loss_2_series_weight = args.loss_2_series_weight\n        self.loss_2_prior_weight = args.loss_2_prior_weight\n        self.idx_pad, self.zero_index = generate_pad(self.padding, self.n_his, self.n_frames-self.n_his)\n\n        ## Set the noise scheduler for the diffusion process\n        self._set_diffusion_variables()\n\n        ## Build the model\n        self.build_model()\n\n\n    def build_model(self) -> None:\n        \"\"\"\n        Build the model according to the specified hyperparameters.\n        \"\"\"\n\n        # Prediction Model\n        pre_model = MotionTransformer(\n            input_feats=2 * self.n_joints,\n            num_frames=self.n_frames,\n            num_layers=self.num_layers,\n            num_heads=self.num_heads,\n            latent_dim=self.latent_dims,\n            dropout=self.dropout,\n            device=self.device,\n            inject_condition=(self.conditioning_strategy == 'inject')\n        )\n\n        # Reconstruction Model\n        rec_model = STSAE(\n            c_in=self.num_coords,\n            h_dim=self.cond_h_dim,\n            latent_dim=self.cond_latent_dim,\n            n_frames=self.n_his,\n            dropout=self.cond_dropout,\n            n_joints=self.n_joints,\n            layer_channels=self.cond_channels,\n            device=self.device)\n\n        self.pre_model, self.rec_model = pre_model, rec_model\n\n\n    def forward(self, input_data: List[torch.Tensor], aggr_strategy: str = None, return_: str = None) -> List[torch.Tensor]:\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n\n        ## Unpack data: tensor_data is the input data, meta_out is a list of metadata\n        tensor_data, meta_out = self._unpack_data(input_data)\n        B = tensor_data.shape[0]\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :]\n        x_0 = padding_traj(history_data, self.padding, self.idx_pad, self.zero_index)\n\n        generated_xs = []\n        # Generate m future predictions\n        for _ in range(self.n_generated_samples):\n\n            ## Reconstruction —— AE model\n            condition_embedding, rec_his_data = self.rec_model(history_data)\n\n            ## Prediction —— diffusion model\n            ## DCT transformation\n            dct_m, idct_m = get_dct_matrix(self.n_frames)\n            dct_m_all = dct_m.float().to(self.device)\n            idct_m_all = idct_m.float().to(self.device)\n            # (B, C, T, V) -> (B, T, V, C)\n            x = x_0.permute(0, 2, 3, 1).contiguous()\n            # (B, T, V, C) -> (B, T, C*V)\n            x = x.reshape([x.shape[0], self.n_frames, -1])\n            y = torch.matmul(dct_m_all, x)  # [B, T, C*V]\n\n            ## Generate gaussian noise of the same shape as the y\n            y_d = torch.randn_like(y, device=self.device)\n\n            ## (t ∈ T, T-1, ..., 1)\n            for i in reversed(range(1, self.noise_steps)):\n\n                ### Prediction (Two branches)\n                ## Set the time step\n                t = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev[0] = 0\n\n                ## Generate gaussian noise of the same shape as the predicted noise\n                noise_pre = torch.randn_like(y_d, device=self.device) if i > 1 else torch.zeros_like(y_d, device=self.device)\n\n                ## First branch\n                # Predict the noise\n                predicted_noise_pre, series, prior, _ = self.pre_model(y_d, t, condition_data=condition_embedding)\n                # Get the alpha and beta values and expand them to the shape of the predicted noise\n                alpha_pre = self._alpha[t][:, None, None]\n                alpha_hat_pre = self._alpha_hat[t][:, None, None]\n                beta_pre = self._beta[t][:, None, None]\n                # Recover the predicted sequence\n                y_d = (1 / torch.sqrt(alpha_pre)) * (y_d - ((1 - alpha_pre) / (torch.sqrt(1 - alpha_hat_pre))) * predicted_noise_pre) \\\n                    + torch.sqrt(beta_pre) * noise_pre\n                ## Second branch\n                alpha_hat_prev = self._alpha_hat[t_prev][:, None, None]\n                # Add noise\n                y_n = (torch.sqrt(alpha_hat_prev) * y) + (torch.sqrt(1 - alpha_hat_prev) * noise_pre)\n                ## Mask completion\n                # Get M values\n                mask = torch.zeros_like(x, device=self.device) # [batch, T, C*V]\n                for m in range(0, self.n_his):\n                    mask[:, m, :] = 1\n                # iDCT transformation\n                y_d_idct = torch.matmul(idct_m_all, y_d)\n                y_n_idct = torch.matmul(idct_m_all, y_n)\n                # mask-mul\n                m_mul_y_n = torch.mul(mask, y_n_idct)\n                m_mul_y_d = torch.mul((1-mask), y_d_idct)\n                # together\n                m_y = m_mul_y_d + m_mul_y_n\n                # DCT again\n                y_d = torch.matmul(dct_m_all, m_y)\n\n            # iDCT\n            pre_future_data = torch.matmul(idct_m_all, y_d)\n            # (B, T, C*V) -> (B, T, V, C)\n            pre_future_data = pre_future_data.reshape(pre_future_data.shape[0], pre_future_data.shape[1], -1, 2)\n            # (B, T, V, C) -> (B, C, T, V)\n            pre_future_data = pre_future_data.permute(0, 3, 1, 2).contiguous()\n            # select future sequences\n            pre_future_data = pre_future_data[:,:,self.n_his:,:]\n\n            ## Reconstruction + Prediction\n            xs = torch.cat((rec_his_data, pre_future_data), dim=2)\n\n            generated_xs.append(xs)\n\n        selected_x, loss_of_selected_x = self._aggregation_strategy(generated_xs, tensor_data, aggr_strategy)\n\n        return self._pack_out_data(selected_x, loss_of_selected_x, [tensor_data] + meta_out, return_=return_)\n\n\n    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.float32:\n        \"\"\"\n        Training step of the model.\n        \"\"\"\n\n        ## Get the optimizer returned in configuration_optimizers()\n        opt = self.optimizers()\n\n        ## Unpack data: tensor_data is the input data\n        tensor_data, _ = self._unpack_data(batch)\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :] # Used for rec (first n_his)\n        x_0 = tensor_data # Used for pre（all）\n\n        ## Reconstruction\n        # Encode the history data\n        condition_embedding, rec_his_data = self.rec_model(history_data)\n        # Compute the rec_loss\n        rec_loss = torch.mean(self.loss_fn(rec_his_data, history_data))\n        self.log('rec_loss', rec_loss)\n\n        ## Prediction\n        # DCT transformation\n        dct_m, _ = get_dct_matrix(self.n_frames)\n        dct_m_all = dct_m.float().to(self.device)\n        # (B, C, T, V) -> (B, T, V, C)\n        x = x_0.permute(0, 2, 3, 1).contiguous()\n        # (B, T, V, C) -> (B, T, C*V)\n        x = x.reshape([x.shape[0], self.n_frames, -1]) # [batch, T, C*V]\n        y_0 = torch.matmul(dct_m_all, x)\n\n        # Sample the time steps and corrupt the data\n        t = self.noise_scheduler.sample_timesteps(y_0.shape[0]).to(self.device)\n        y_t, pre_noise = self.noise_scheduler.noise_motion(y_0, t) # (B, T, C*(V-1))\n\n        # Predict the noise\n        pre_predicted_noise, series, prior, _ = self.pre_model(y_t, t, condition_data=condition_embedding)\n\n        # Compute the pre_loss\n        # Calculate Association discrepancy\n        series_loss = 0.0\n        prior_loss = 0.0\n        for u in range(len(prior)):\n            # Pdetach, S <-> Maximize\n            series_loss += \\\n                (torch.mean(my_kl_loss(\n                    series[u],\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach()))\n                + torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach(),\n                    series[u])))\n            # P, Sdetach <-> Minimize\n            prior_loss += \\\n                (torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)),\n                    series[u].detach()))\n                + torch.mean(my_kl_loss(\n                    series[u].detach(),\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)))))\n        series_loss = series_loss / len(prior)\n        prior_loss = prior_loss / len(prior)\n\n        pre_loss = torch.mean(self.loss_fn(pre_predicted_noise, pre_noise))\n        self.log('pre_loss', pre_loss)\n\n        ## Compute loss1 & loss2\n        loss1 = rec_loss + pre_loss \\\n                - self.loss_1_series_weight * series_loss \\\n                + self.loss_1_prior_weight * prior_loss\n        self.log('loss1', loss1)\n        loss2 = rec_loss + pre_loss \\\n                + self.loss_2_prior_weight * prior_loss \\\n                + self.loss_2_series_weight * series_loss\n        self.log('loss2', loss2)\n\n        ## Minimax strategy\n        self.manual_backward(loss1, retain_graph=True)\n        self.manual_backward(loss2)\n        opt.step()\n        opt.zero_grad()\n\n\n    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Test step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._test_output_list.append(self.forward(batch))\n        return\n\n\n    def on_test_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_test_epoch_start()\n        self._test_output_list = []\n        return\n\n\n    def on_test_epoch_end(self) -> float:\n        \"\"\"\n        Test epoch end of the model.\n\n        Returns:\n            float: test auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._test_output_list)\n        del self._test_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score)\n        return auc_score\n\n\n    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Validation step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._validation_output_list.append(self.forward(batch))\n        return\n\n\n    def on_validation_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_validation_epoch_start()\n        self._validation_output_list = []\n        return\n\n\n    def on_validation_epoch_end(self) -> float:\n        \"\"\"\n        Validation epoch end of the model.\n\n        Returns:\n            float: validation auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._validation_output_list)\n        del self._validation_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score, sync_dist=True)\n        return auc_score\n\n\n    def configure_optimizers(self) -> Dict:\n        \"\"\"\n        Configure the optimizers and the learning rate schedulers.\n\n        Returns:\n            Dict: dictionary containing the optimizers, the learning rate schedulers and the metric to monitor\n        \"\"\"\n\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, last_epoch=-1)\n\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'AUC'}\n\n\n    def post_processing(self, out: np.ndarray, gt_data: np.ndarray, trans: np.ndarray, meta: np.ndarray, frames: np.ndarray) -> float:\n        \"\"\"\n        Post processing of the model.\n\n        Args:\n            out (np.ndarray): output of the model\n            gt_data (np.ndarray): ground truth data\n            trans (np.ndarray): transformation index\n            meta (np.ndarray): metadata\n            frames (np.ndarray): frame indexes of the data\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        all_gts = [file_name for file_name in os.listdir(self.gt_path) if file_name.endswith('.npy')]\n        all_gts = sorted(all_gts)\n        scene_clips = [(int(fn.split('_')[0]), int(fn.split('_')[1].split('.')[0])) for fn in all_gts]\n        hr_ubnormal_masked_clips = get_hr_ubnormal_mask(self.split) if (self.use_hr and (self.dataset_name == 'UBnormal')) else {}\n        hr_avenue_masked_clips = get_avenue_mask() if self.dataset_name == 'HR-Avenue' else {}\n\n        num_transform = self.num_transforms\n        model_scores_transf = {}\n        dataset_gt_transf = {}\n\n        for transformation in tqdm(range(num_transform)):\n            # iterating over each transformation T\n\n            dataset_gt = []\n            model_scores = []\n            cond_transform = (trans == transformation)\n\n            out_transform, gt_data_transform, meta_transform, frames_transform = filter_vectors_by_cond([out, gt_data, meta, frames], cond_transform)\n\n            for idx in range(len(all_gts)):\n                # iterating over each clip C with transformation T\n\n                scene_idx, clip_idx = scene_clips[idx]\n\n                gt = np.load(os.path.join(self.gt_path, all_gts[idx]))\n                n_frames = gt.shape[0]\n\n                cond_scene_clip = (meta_transform[:, 0] == scene_idx) & (meta_transform[:, 1] == clip_idx)\n                out_scene_clip, gt_scene_clip, meta_scene_clip, frames_scene_clip = filter_vectors_by_cond([out_transform, gt_data_transform,\n                                                                                                            meta_transform, frames_transform],\n                                                                                                           cond_scene_clip)\n\n                figs_ids = sorted(list(set(meta_scene_clip[:, 2])))\n                error_per_person = []\n                error_per_person_max_loss = []\n\n                for fig in figs_ids:\n                    # iterating over each actor A in each clip C with transformation T\n\n                    cond_fig = (meta_scene_clip[:, 2] == fig)\n                    out_fig, _, frames_fig = filter_vectors_by_cond([out_scene_clip, gt_scene_clip, frames_scene_clip], cond_fig)\n                    loss_matrix = compute_var_matrix(out_fig, frames_fig, n_frames)\n                    fig_reconstruction_loss = np.nanmax(loss_matrix, axis=0)\n                    if self.anomaly_score_pad_size != -1:\n                        fig_reconstruction_loss = pad_scores(fig_reconstruction_loss, gt, self.anomaly_score_pad_size)\n\n                    error_per_person.append(fig_reconstruction_loss)\n                    error_per_person_max_loss.append(max(fig_reconstruction_loss))\n\n                clip_score = np.stack(error_per_person, axis=0)\n                clip_score_log = np.log1p(clip_score)\n                clip_score = np.mean(clip_score, axis=0) + (np.amax(clip_score_log, axis=0)-np.amin(clip_score_log, axis=0))\n\n                # removing the non-HR frames for UBnormal dataset\n                if (scene_idx, clip_idx) in hr_ubnormal_masked_clips:\n                    clip_score = clip_score[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n                    gt = gt[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n\n                # removing the non-HR frames for Avenue dataset\n                if clip_idx in hr_avenue_masked_clips:\n                    clip_score = clip_score[np.array(hr_avenue_masked_clips[clip_idx])==1]\n                    gt = gt[np.array(hr_avenue_masked_clips[clip_idx])==1]\n\n                # Abnormal score per frame\n                clip_score = score_process(clip_score, self.anomaly_score_frames_shift, self.anomaly_score_filter_kernel_size)\n                model_scores.append(clip_score)\n\n                dataset_gt.append(gt)\n\n            model_scores = np.concatenate(model_scores, axis=0)\n\n            dataset_gt = np.concatenate(dataset_gt, axis=0)\n\n            model_scores_transf[transformation] = model_scores\n            dataset_gt_transf[transformation] = dataset_gt\n\n        # aggregating the anomaly scores for all transformations\n        pds = np.mean(np.stack(list(model_scores_transf.values()), 0), 0)\n        gt = dataset_gt_transf[0]\n\n        # computing the AUC\n        auc = roc_auc_score(gt, pds)\n\n        return auc\n\n\n    def test_on_saved_tensors(self, split_name: str) -> float:\n        \"\"\"\n        Skip the prediction step and test the model on the saved tensors.\n\n        Args:\n            split_name (str): split name (val, test)\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        tensors = self._load_tensors(split_name, self.aggregation_strategy, self.n_generated_samples)\n        auc_score = self.post_processing(tensors['prediction'], tensors['gt_data'], tensors['trans'],\n                                         tensors['metadata'], tensors['frames'])\n        print(f'AUC score: {auc_score:.6f}')\n        return auc_score\n\n\n    ## Helper functions\n\n    def _aggregation_strategy(self, generated_xs: List[torch.Tensor], input_sequence: torch.Tensor, aggr_strategy: str) -> Tuple[torch.Tensor]:\n        \"\"\"\n        Aggregates the generated samples and returns the selected one and its reconstruction error.\n        Strategies:\n            - all: returns all the generated samples\n            - random: returns a random sample\n            - best: returns the sample with the lowest reconstruction loss\n            - worst: returns the sample with the highest reconstruction loss\n            - mean: returns the mean of the losses of the generated samples\n            - median: returns the median of the losses of the generated samples\n            - mean_pose: returns the mean of the generated samples\n            - median_pose: returns the median of the generated samples\n\n        Args:\n            generated_xs (List[torch.Tensor]): list of generated samples\n            input_sequence (torch.Tensor): ground truth sequence\n            aggr_strategy (str): aggregation strategy\n\n        Raises:\n            ValueError: if the aggregation strategy is not valid\n\n        Returns:\n            Tuple[torch.Tensor]: selected sample and its reconstruction error\n        \"\"\"\n\n        aggr_strategy = self.aggregation_strategy if aggr_strategy is None else aggr_strategy\n        if aggr_strategy == 'random':\n            return generated_xs[np.random.randint(len(generated_xs))], None # Added None as it was missing\n\n        B, repr_shape = input_sequence.shape[0], input_sequence.shape[1:]\n        compute_loss = lambda x: torch.mean(self.loss_fn(x, input_sequence).reshape(-1, prod(repr_shape)), dim=-1)\n        losses = [compute_loss(x) for x in generated_xs]\n\n        if aggr_strategy == 'all':\n            dims_idxs = list(range(2, len(repr_shape)+2))\n            dims_idxs = [1, 0] + dims_idxs\n            selected_x = torch.stack(generated_xs).permute(*dims_idxs)\n            loss_of_selected_x = torch.stack(losses).permute(1, 0)\n        elif aggr_strategy == 'mean':\n            selected_x = None\n            loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n        elif aggr_strategy == 'mean_pose':\n            selected_x = torch.mean(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'median':\n            loss_of_selected_x, _ = torch.median(torch.stack(losses), dim=0)\n            selected_x = None\n        elif aggr_strategy == 'median_pose':\n            selected_x, _ = torch.median(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'best' or aggr_strategy == 'worst':\n            strategy = (lambda x, y: x < y) if aggr_strategy == 'best' else (lambda x, y: x > y)\n            loss_of_selected_x = torch.full((B,), fill_value=(1e10 if aggr_strategy == 'best' else -1.), device=self.device)\n            selected_x = torch.zeros((B, *repr_shape)).to(self.device)\n\n            for i in range(len(generated_xs)):\n                mask = strategy(losses[i], loss_of_selected_x)\n                loss_of_selected_x[mask] = losses[i][mask]\n                selected_x[mask] = generated_xs[i][mask]\n        elif 'quantile' in aggr_strategy:\n            q = float(aggr_strategy.split(':')[-1])\n            loss_of_selected_x = torch.quantile(torch.stack(losses), q, dim=0)\n            selected_x = None\n        else:\n            raise ValueError(f'Unknown aggregation strategy {aggr_strategy}')\n\n        # Ensuring selected_x and loss_of_selected_x are always returned\n        if selected_x is None and loss_of_selected_x is None:\n             # Default to mean loss if strategy doesn't return both\n             loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n\n\n        return selected_x, loss_of_selected_x\n\n\n    def _infer_number_of_joint(self, args: argparse.Namespace) -> int:\n        \"\"\"\n        Infer the number of joints based on the dataset parameters.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n\n        Returns:\n            int: number of joints\n        \"\"\"\n\n        if args.headless:\n            joints_to_consider = 14\n        elif args.kp18_format:\n            joints_to_consider = 18\n        else:\n            joints_to_consider = 17\n        return joints_to_consider\n\n\n    def _load_tensors(self, split_name: str, aggr_strategy: str, n_gen: int) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Loads the tensors from the experiment directory.\n\n        Args:\n            split_name (str): name of the split (train, val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n\n        Returns:\n            Dict[str, torch.Tensor]: dictionary containing the tensors. The keys are inferred from the file names.\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        tensor_files = os.listdir(path)\n        tensors = {}\n        for t_file in tensor_files:\n            t_name = t_file.split('.')[0]\n            tensors[t_name] = torch.load(os.path.join(path, t_file))\n        return tensors\n\n\n    def _pack_out_data(self, selected_x: torch.Tensor, loss_of_selected_x: torch.Tensor, additional_out: List[torch.Tensor], return_: str) -> List[torch.Tensor]:\n        \"\"\"\n        Packs the output data according to the return_ argument.\n\n        Args:\n            selected_x (torch.Tensor): generated samples selected among the others according to the aggregation strategy\n            loss_of_selected_x (torch.Tensor): loss of the selected samples\n            additional_out (List[torch.Tensor]): additional output data (ground truth, meta-data, etc.)\n            return_ (str): return strategy. Can be 'pose', 'loss', 'all'\n\n        Raises:\n            ValueError: if return_ is None and self.model_return_value is None\n\n        Returns:\n            List[torch.Tensor]: output data\n        \"\"\"\n\n        if return_ is None:\n            if self.model_return_value is None:\n                raise ValueError('Either return_ or self.model_return_value must be set')\n            else:\n                return_ = self.model_return_value\n\n        if return_ == 'poses':\n            out = [selected_x]\n        elif return_ == 'loss':\n            out = [loss_of_selected_x]\n        elif return_ == 'all':\n            # Check if both are available before adding to the list\n            out = []\n            if loss_of_selected_x is not None:\n                out.append(loss_of_selected_x)\n            if selected_x is not None:\n                 out.append(selected_x)\n\n        return out + additional_out\n\n\n    def _save_tensors(self, tensors: Dict[str, torch.Tensor], split_name: str, aggr_strategy: str, n_gen: int) -> None:\n        \"\"\"\n        Saves the tensors in the experiment directory.\n\n        Args:\n            tensors (Dict[str, torch.Tensor]): tensors to save\n            split_name (str): name of the split (val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        for t_name, tensor in tensors.items():\n            torch.save(tensor, os.path.join(path, t_name + '.pt'))\n\n\n    def _set_diffusion_variables(self) -> None:\n        \"\"\"\n        Sets the diffusion variables.\n        \"\"\"\n\n        self.noise_scheduler = Diffusion(noise_steps=self.noise_steps, n_joints=self.n_joints,\n                                         device=self.device, time=self.n_frames)\n        self._beta_ = self.noise_scheduler.schedule_noise()\n        self._alpha_ = (1. - self._beta_)\n        self._alpha_hat_ = torch.cumprod(self._alpha_, dim=0)\n\n    def _unpack_data(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        Unpacks the data.\n\n        Args:\n            x (torch.Tensor): list containing the input data, the transformation index, the metadata and the actual frames.\n\n        Returns:\n            Tuple[torch.Tensor, List[torch.Tensor]]: input data, list containing the transformation index, the metadata and the actual frames.\n        \"\"\"\n        tensor_data = x[0].to(self.device)\n        transformation_idx = x[1]\n        metadata = x[2]\n        actual_frames = x[3]\n        meta_out = [transformation_idx, metadata, actual_frames]\n        return tensor_data, meta_out\n\n\n    @property\n    def _beta(self) -> torch.Tensor:\n        return self._beta_.to(self.device)\n\n\n    @property\n    def _alpha(self) -> torch.Tensor:\n        return self._alpha_.to(self.device)\n\n\n    @property\n    def _alpha_hat(self) -> torch.Tensor:\n        return self._alpha_hat_.to(self.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:47.079043Z","iopub.execute_input":"2025-06-03T01:15:47.079672Z","iopub.status.idle":"2025-06-03T01:15:47.097200Z","shell.execute_reply.started":"2025-06-03T01:15:47.079648Z","shell.execute_reply":"2025-06-03T01:15:47.096346Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/dcmd.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/eval_DCMD.py\nimport argparse\nimport os\n\nimport pytorch_lightning as pl\nimport yaml\nfrom models.dcmd import DCMD\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\n\n\n\nif __name__== '__main__':\n    \n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='DCMD')\n    parser.add_argument('-c', '--config', type=str, required=True)\n    args = parser.parse_args()\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args)\n\n    # Initialize the model\n    model = DCMD(args)\n    \n    if args.load_tensors:\n        # Load tensors and test\n        model.test_on_saved_tensors(split_name=args.split)\n    else:\n        # Load test data\n        print('Loading data and creating loaders.....')\n        ckpt_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/last.ckpt'\n        dataset, loader, _, _ = get_dataset_and_loader(args, split=args.split)\n        \n        # Initialize trainer and test\n        trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices[:1],\n                             default_root_dir=args.ckpt_dir, max_epochs=1, logger=False)\n        out = trainer.test(model, dataloaders=loader, ckpt_path=ckpt_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:15:57.882771Z","iopub.execute_input":"2025-06-03T01:15:57.883074Z","iopub.status.idle":"2025-06-03T01:15:57.889180Z","shell.execute_reply.started":"2025-06-03T01:15:57.883048Z","shell.execute_reply":"2025-06-03T01:15:57.888389Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/eval_DCMD.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/utils/get_robust_data.py\nimport os\nimport numpy as np\nimport pickle\n\nfrom copy import deepcopy\n\nimport torch\n\nfrom utils.data import load_trajectories, extract_global_features\nfrom utils.data import change_coordinate_system, scale_trajectories, aggregate_autoencoder_data\nfrom utils.data import input_trajectories_missing_steps\nfrom utils.preprocessing import remove_short_trajectories, aggregate_rnn_autoencoder_data\n\n\ndef save_scaler(scaler, path):\n    with open(path, 'wb') as scaler_file:\n        pickle.dump(scaler, scaler_file)\n    \n        \ndef load_scaler(path):\n    with open(path, 'rb') as scaler_file:\n        scaler = pickle.load(scaler_file)\n    return scaler\n\n\n\n# Load trajectory data and convert it into a format suitable for RNN autoencoders for training and testing of joint models\ndef data_of_combined_model(**args):\n    # General\n    exp_dir = args.get('exp_dir', '')\n    split = args.get('split', 'train')\n    normalize_pose = args.get('normalize_pose', False)\n    trajectories_path = args.get('trajectories_path', '')\n    include_global = args.get('include_global', True)\n    debug = args.get('debug', False)\n    if 'train' in split:\n      # đổi thành path khác trong folder data nếu train dataset khác\n        subfolder = '/kaggle/input/avenue/Avenue/training'\n    elif 'test' in split:\n        subfolder = '/kaggle/input/avenue/Avenue/testing'\n    else:\n        subfolder = 'validating'\n    trajectories_path = os.path.join(trajectories_path, f'{subfolder}/trajectories')\n    video_resolution = args.get('vid_res', (1080,720))\n    video_resolution = np.array(video_resolution, dtype=np.float32)\n    # Architecture\n    reconstruct_original_data = args.get('reconstruct_original_data', False) \n    input_length = args.get('seg_len', 12)\n    seg_stride = args.get('seg_stride', 1) - 1 \n    pred_length = 0 \n    # Training\n    input_missing_steps = False # args.input_missing_steps\n    \n    if normalize_pose == True:\n        global_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        local_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        out_normalisation_strategy = args.get('normalization_strategy', 'robust')\n\n\n    trajectories = load_trajectories(trajectories_path, debug=debug, split=split)\n    print('\\nLoaded %d trajectories.' % len(trajectories))\n\n    trajectories = remove_short_trajectories(trajectories, input_length=input_length,\n                                             input_gap=seg_stride, pred_length=pred_length)\n    print('\\nRemoved short trajectories. Number of trajectories left: %d.' % len(trajectories))\n\n    # trajectories, trajectories_val = split_into_train_and_test(trajectories, train_ratio=0.8, seed=42)\n\n    if input_missing_steps:\n        trajectories = input_trajectories_missing_steps(trajectories)\n        print('\\nInputted missing steps of trajectories.')\n\n    # Global\n    if include_global:\n        global_trajectories = extract_global_features(deepcopy(trajectories), video_resolution=video_resolution)\n\n        global_trajectories = change_coordinate_system(global_trajectories, video_resolution=video_resolution,\n                                                        coordinate_system='global', invert=False)\n\n        print('\\nChanged global trajectories\\'s coordinate system to global.')\n        \n        X_global, y_global, X_global_meta, y_global_meta = aggregate_rnn_autoencoder_data(global_trajectories, \n                                                                                        input_length=input_length,\n                                                                                        input_gap=seg_stride, pred_length=pred_length, \n                                                                                        return_ids=True)\n        \n        if normalize_pose == True:\n            # nếu test avenue thì dùng dòng này\n            #/content/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\n            # default: scaler_path = os.path.join(exp_dir, f'global_{global_normalisation_strategy}.pickle')\n            scaler_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle'\n            if split == 'train':\n                _, global_scaler = scale_trajectories(aggregate_autoencoder_data(global_trajectories),\n                                                    strategy=global_normalisation_strategy)\n                save_scaler(global_scaler, scaler_path)\n            else:\n                global_scaler = load_scaler(scaler_path)\n\n            X_global, _ = scale_trajectories(X_global, scaler=global_scaler, strategy=global_normalisation_strategy)\n            \n            if y_global is not None:\n                y_global, _ = scale_trajectories(y_global, scaler=global_scaler,\n                                                strategy=global_normalisation_strategy)\n                \n            print('\\nNormalised global trajectories using the %s normalisation strategy.' % global_normalisation_strategy)\n    \n    else:\n        X_global, X_global_meta = None, None\n    \n    # Local\n    local_trajectories = deepcopy(trajectories) if reconstruct_original_data else trajectories\n\n    local_trajectories = change_coordinate_system(local_trajectories, video_resolution=video_resolution,\n                                                  coordinate_system='bounding_box_centre', invert=False)\n\n    print('\\nChanged local trajectories\\'s coordinate system to bounding_box_centre.')\n\n    X_local, y_local, X_local_meta, y_local_meta = aggregate_rnn_autoencoder_data(local_trajectories, input_length=input_length, \n                                                                                  input_gap=seg_stride, pred_length=pred_length,\n                                                                                  return_ids=True)\n    \n    if normalize_pose == True:\n        #scaler_path = '/content/drive/MyDrive/DCMD-main-main/checkpoints/Avenue/test_experiment/local_robust.pickle'\n        scaler_path = os.path.join(exp_dir, f'local_{local_normalisation_strategy}.pickle')\n\n        if split == 'train':\n            _, local_scaler = scale_trajectories(aggregate_autoencoder_data(local_trajectories),\n                                                strategy=local_normalisation_strategy)\n            save_scaler(local_scaler, scaler_path)\n        else:\n            local_scaler = load_scaler(\"/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\")\n\n        X_local, _ = scale_trajectories(X_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n\n        if y_local is not None:\n            y_local, _ = scale_trajectories(y_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n        \n        print('\\nNormalised local trajectories using the %s normalisation strategy.' % local_normalisation_strategy)\n\n    # (Optional) Reconstruct the original data\n    if reconstruct_original_data:\n        print('\\nReconstruction/Prediction target is the original data.')\n        out_trajectories = trajectories\n        \n        out_trajectories = change_coordinate_system(out_trajectories, video_resolution=video_resolution,\n                                                    coordinate_system='global', invert=False)\n    \n        print('\\nChanged target trajectories\\'s coordinate system to global.')\n        \n        scaler_path = os.path.join(exp_dir, f'out_{out_normalisation_strategy}.pickle')\n    \n        if split == 'train':\n            _, out_scaler = scale_trajectories(aggregate_autoencoder_data(out_trajectories),\n                                               strategy=out_normalisation_strategy)\n            save_scaler(out_scaler, scaler_path)\n        else:\n            out_scaler = load_scaler(scaler_path)\n        \n        ######## X_out_{}, y_out_{} numpy arrays\n\n        X_out, y_out, X_out_meta, y_out_meta = aggregate_rnn_autoencoder_data(out_trajectories, input_length=input_length, \n                                                                              input_gap=seg_stride, pred_length=pred_length,\n                                                                              return_ids=True)\n\n        X_out, _ = scale_trajectories(X_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n        \n        if y_out is not None:\n            y_out, _ = scale_trajectories(y_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n            \n        print('\\nNormalised target trajectories using the %s normalisation strategy.' % out_normalisation_strategy)\n        \n            \n    if pred_length > 0:\n        \n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta), \\\n                   (y_out, y_out_meta) \n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta)\n    else:\n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta)\n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:16:02.616668Z","iopub.execute_input":"2025-06-03T01:16:02.616924Z","iopub.status.idle":"2025-06-03T01:16:02.626684Z","shell.execute_reply.started":"2025-06-03T01:16:02.616904Z","shell.execute_reply":"2025-06-03T01:16:02.625860Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/utils/get_robust_data.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Train** ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/train_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:52:00.098063Z","iopub.execute_input":"2025-06-03T01:52:00.098359Z","iopub.status.idle":"2025-06-03T01:52:27.972118Z","shell.execute_reply.started":"2025-06-03T01:52:00.098338Z","shell.execute_reply":"2025-06-03T01:52:27.971199Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Experiment directories created in /kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment\n\nLoaded 2786 trajectories.\n\nRemoved short trajectories. Number of trajectories left: 1735.\n\nChanged local trajectories's coordinate system to bounding_box_centre.\n\nNormalised local trajectories using the robust normalisation strategy.\nSTSE: Adjacency matrix not provided. Initializing Graph with layout='h36m'.\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment exists and is not empty.\nEpoch 0:   0%|                                          | 0/185 [00:00<?, ?it/s]Warning: Encoder output T,V (2,17) differs from expected (1,17). Check strides and n_joints.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/DCMD-main/train_DCMD.py\", line 78, in <module>\n[rank0]:     trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path)\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n[rank0]:     results = self._run_stage()\n[rank0]:               ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n[rank0]:     self.fit_loop.run()\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n[rank0]:     self.advance()\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n[rank0]:     self.epoch_loop.run(self._data_fetcher)\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n[rank0]:     self.advance(data_fetcher)\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 322, in advance\n[rank0]:     batch_output = self.manual_optimization.run(kwargs)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/manual.py\", line 94, in run\n[rank0]:     self.advance(kwargs)\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/manual.py\", line 114, in advance\n[rank0]:     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n[rank0]:     output = fn(*args, **kwargs)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 390, in training_step\n[rank0]:     return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 641, in __call__\n[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1637, in forward\n[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1464, in _run_ddp_forward\n[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 634, in wrapped_forward\n[rank0]:     out = method(*_args, **_kwargs)\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/DCMD-main/models/dcmd.py\", line 242, in training_step\n[rank0]:     condition_embedding, rec_his_data = self.rec_model(history_data)\n[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 296, in forward\n[rank0]:     Z, shape_before_flatten = self.encode(X, return_shape=True, t_diffusion=t_diffusion)\n[rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 191, in encode\n[rank0]:     Z = self.btlnk(flattened_features) # (N_actual, self.latent_dim)\n[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n[rank0]:     return F.linear(input, self.weight, self.bias)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: RuntimeError: mat1 and mat2 shapes cannot be multiplied (2048x17408 and 8704x256)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"If download checkpoint needed, run this code and download checkpoints.zip file in Output","metadata":{}},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n# 2. (tuỳ chọn) kiểm tra xem zip đã tạo xong chưa\n!ls -lh /kaggle/working/checkpoints.zip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:43:30.055859Z","iopub.execute_input":"2025-05-26T09:43:30.056096Z","iopub.status.idle":"2025-05-26T09:43:30.059988Z","shell.execute_reply.started":"2025-05-26T09:43:30.056080Z","shell.execute_reply":"2025-05-26T09:43:30.059235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/eval_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-05-26T09:43:32.799055Z","iopub.execute_input":"2025-05-26T09:43:32.799593Z","iopub.status.idle":"2025-05-26T13:05:29.018946Z","shell.execute_reply.started":"2025-05-26T09:43:32.799569Z","shell.execute_reply":"2025-05-26T13:05:29.017808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T13:10:50.210286Z","iopub.execute_input":"2025-05-26T13:10:50.210565Z","iopub.status.idle":"2025-05-26T13:10:59.549864Z","shell.execute_reply.started":"2025-05-26T13:10:50.210540Z","shell.execute_reply":"2025-05-26T13:10:59.549175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Đổi phương pháp cho STGCN","metadata":{}},{"cell_type":"code","source":"!pip install torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:16:09.741382Z","iopub.execute_input":"2025-06-03T01:16:09.741651Z","iopub.status.idle":"2025-06-03T01:16:14.135348Z","shell.execute_reply.started":"2025-06-03T01:16:09.741633Z","shell.execute_reply":"2025-06-03T01:16:14.134625Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/stsae/stsae.py\nimport torch.nn as nn\nimport torch\nimport numpy as np # Cần thiết nếu adjacency_matrix là numpy array ban đầu\nfrom typing import List, Tuple, Union\n\n# Đảm bảo đường dẫn import này chính xác dựa trên cấu trúc thư mục của bạn\nfrom models.common.components import Encoder, Decoder \n# Graph sẽ được import bên trong STSE nếu adjacency_matrix is None\n\nclass STSE(nn.Module):\n    def __init__(\n        self,\n        c_in: int,\n        h_dim: int = 32, # hidden_dimension cho output của Encoder GAT-TCN stack\n        latent_dim: int = 64, # Kích thước của bottleneck latent space Z\n        n_frames: int = 12,\n        n_joints: int = 17,\n        layer_channels: List[int] = [64, 128, 256], # Channels cho các block trung gian\n        dropout: float = 0.3,\n        device: Union[str, torch.device] = 'cpu', # Sửa DeviceObjType thành torch.device\n        adjacency_matrix=None,\n        num_heads_gat: int = 4,\n        encoder_temporal_strides: List[int] = None, # Thêm cấu hình strides\n        decoder_temporal_strides: List[int] = None  # Thêm cấu hình strides\n    ) -> None:\n        super(STSE, self).__init__()\n        # Store parameters\n        self.input_dim = c_in # c_in là số channels đầu vào (ví dụ: 3 cho XYZ)\n        self.hidden_dimension = h_dim # h_dim là số channels output của Encoder GAT-TCN stack\n        self.latent_dim = latent_dim # latent_dim là kích thước của vector Z sau bottleneck\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.layer_channels_config = layer_channels # Channels cho các GAT_TCN_Block trung gian\n        self.dropout = dropout\n        self.device = device\n        \n        # Load adjacency matrix if not provided\n        if adjacency_matrix is None:\n            try:\n                from models.gcae.graph import Graph # Import Graph ở đây\n                # Cấu hình layout và strategy có thể lấy từ config tổng của DCMD\n                graph_layout = 'ntu-rgb+d' if n_joints == 25 else ('h36m' if n_joints == 17 else 'coco')\n                print(f\"STSE: Adjacency matrix not provided. Initializing Graph with layout='{graph_layout}'.\")\n                graph = Graph(layout=graph_layout, strategy='uniform', normalize=True)\n                # graph.A là một numpy array\n                self.adjacency_matrix = torch.from_numpy(graph.A).float().to(self.device)\n            except ImportError:\n                print(\"Warning: models.gcae.graph.Graph not found. Using fallback adjacency matrix.\")\n                # Fallback: create a simple adjacency matrix\n                adj = torch.zeros(n_joints, n_joints, dtype=torch.float32)\n                # Example: connect each joint to the next for simplicity, and self-loops\n                for i in range(n_joints -1):\n                    adj[i, i+1] = 1.0\n                    adj[i+1, i] = 1.0\n                adj.fill_diagonal_(1.0)\n                if n_joints > 0: # Normalize\n                    row_sums = adj.sum(dim=1, keepdim=True)\n                    self.adjacency_matrix = (adj / (row_sums + 1e-6)).to(self.device) # Add epsilon\n                else:\n                    self.adjacency_matrix = adj.to(self.device)\n\n        elif isinstance(adjacency_matrix, np.ndarray):\n            self.adjacency_matrix = torch.from_numpy(adjacency_matrix).float().to(self.device)\n        elif isinstance(adjacency_matrix, torch.Tensor):\n            self.adjacency_matrix = adjacency_matrix.to(self.device)\n        else:\n            raise TypeError(\"adjacency_matrix must be a NumPy array or a PyTorch tensor.\")\n\n        self.num_heads_gat = num_heads_gat\n\n        # Xác định temporal strides cho Encoder\n        # Encoder trong components.py sẽ có len(layer_channels_config) + 1 blocks\n        num_encoder_blocks = len(self.layer_channels_config) + 1\n        if encoder_temporal_strides is None:\n            self.encoder_strides = [1] * num_encoder_blocks\n            # Áp dụng stride 2 một cách hợp lý, ví dụ:\n            if num_encoder_blocks >= 2: # Cần ít nhất 2 block để có stride 2\n                self.encoder_strides[1] = 2 # Downsample ở block thứ 2\n            if num_encoder_blocks >= 4: # Thêm một lần downsample nữa nếu có đủ block\n                self.encoder_strides[3] = 2 # Downsample ở block thứ 4\n            # Đảm bảo stride cuối cùng không làm mất quá nhiều thông tin thời gian nếu không muốn\n            # Ví dụ: if num_encoder_blocks > 1 and self.encoder_strides[-1] == 2 and num_encoder_blocks % 2 != 0 :\n            # self.encoder_strides[-1] = 1 #  Không downsample ở block cuối nếu số block lẻ và block trước đó đã downsample\n        elif len(encoder_temporal_strides) == num_encoder_blocks:\n            self.encoder_strides = encoder_temporal_strides\n        else:\n            raise ValueError(f\"Length of encoder_temporal_strides ({len(encoder_temporal_strides)}) \"\n                             f\"must match the number of encoder blocks ({num_encoder_blocks}).\")\n\n        # Xác định temporal strides cho Decoder\n        # Decoder trong components.py sẽ có len(layer_channels_config) + 1 blocks (khi tính cả output_dim)\n        num_decoder_blocks = len(self.layer_channels_config) + 1\n        if decoder_temporal_strides is None:\n            # GAT_TCN_Block với Conv2d không hỗ trợ upsampling bằng stride > 1\n            self.decoder_strides = [1] * num_decoder_blocks\n        elif len(decoder_temporal_strides) == num_decoder_blocks:\n            self.decoder_strides = decoder_temporal_strides\n            if any(s > 1 for s in self.decoder_strides):\n                print(\"Warning: Decoder GAT_TCN_Block uses Conv2d; temporal_stride > 1 will not upsample time dimension.\")\n        else:\n            raise ValueError(f\"Length of decoder_temporal_strides ({len(decoder_temporal_strides)}) \"\n                             f\"must match the number of decoder blocks ({num_decoder_blocks}).\")\n\n        # Build components\n        self.build_model()\n\n    def build_model(self) -> None:\n        # Encoder\n        self.encoder = Encoder(\n            input_dim=self.input_dim, # Số channels đầu vào (ví dụ: 3 cho XYZ)\n            layer_channels=self.layer_channels_config, # List các output channels của các GAT_TCN_Block trung gian\n            hidden_dimension=self.hidden_dimension, # Output channels của GAT_TCN_Block cuối cùng trong encoder\n            n_frames=self.n_frames, # Thông tin, không dùng trực tiếp trong GAT_TCN_Block\n            n_joints=self.n_joints, # Thông tin, không dùng trực tiếp trong GAT_TCN_Block\n            dropout=self.dropout,\n            adjacency_matrix=self.adjacency_matrix, # Tensor (V,V) hoặc (num_strat, V,V)\n            num_heads_gat=self.num_heads_gat,\n            temporal_strides=self.encoder_strides # Truyền strides đã xác định\n        )\n        \n        # Bottleneck to latent space Z\n        # Input features cho bottleneck phụ thuộc vào output shape của encoder GAT_TCN stack\n        # Encoder output: (N*M, self.hidden_dimension, T_final, V)\n        # T_final phụ thuộc vào số lần downsample bằng strides\n        # Cần tính T_final một cách linh động\n        \n        # Calculate T_final after encoder\n        current_t = self.n_frames # Start with original number of frames\n        # conv_kernel_t = 9 # from TemporalConvNet in gat.py\n        # conv_pad_t = (conv_kernel_t - 1) // 2 # = 4\n\n        for s_enc in self.encoder_strides:\n            # Correct formula for Conv2d output dimension used in TemporalConvNet\n            # L_out = floor((L_in - 1)/stride + 1)\n            if current_t >= 1 and s_enc >=1 : # Ensure valid inputs for formula\n                 current_t = np.floor((current_t - 1) / s_enc).astype(int) + 1\n            else:\n                 # Handle edge cases or raise error if T becomes too small unexpectedly\n                 print(f\"Warning: Unexpected current_t ({current_t}) or s_enc ({s_enc}) during T_final calculation.\")\n                 # Fallback to ensure T is at least 1, though this might hide issues\n                 current_t = max(1, current_t)\n\n\n        self.t_final_encoder = current_t # This is the *calculated* T after all strides in encoder\n        \n        in_features_bottleneck = self.hidden_dimension * self.t_final_encoder * self.n_joints\n        self.btlnk = nn.Linear(in_features=in_features_bottleneck, out_features=self.latent_dim)\n\n    def encode(\n        self,\n        X: torch.Tensor, # Input shape: (batch, c_in, n_frames, n_joints) hoặc (batch, n_frames, n_joints, c_in)\n        return_shape: bool = False,\n        t_diffusion: torch.Tensor = None # Tham số t cho diffusion (nếu cần, hiện tại GAT_TCN_Block không dùng)\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Size]]:\n        \n        # Xử lý input shape: mong muốn (N, C, T, V) cho Encoder\n        if X.dim() == 4 and X.size(1) == self.input_dim : # (N, C, T, V)\n            N, C, T, V = X.shape\n            X_proc = X\n            num_persons = 1 # Giả định 1 người nếu không có chiều M\n        elif X.dim() == 4 and X.size(3) == self.input_dim: # (N, T, V, C)\n            N, T, V, C = X.shape\n            X_proc = X.permute(0, 3, 1, 2).contiguous() # (N, C, T, V)\n            num_persons = 1\n        elif X.dim() == 5 and X.size(1) == self.input_dim : # (N, C, T, V, M) - ít phổ biến hơn\n            N, C, T, V, M_persons = X.size()\n            X_proc = X.permute(0, 4, 1, 2, 3).contiguous().view(N * M_persons, C, T, V)\n            num_persons = M_persons # Lưu lại để reshape nếu cần\n        elif X.dim() == 5 and X.size(2) == self.input_dim : # (N, M, C, T, V) - DCMD hay dùng\n            N, M_persons, C, T, V = X.size()\n            X_proc = X.view(N * M_persons, C, T, V)\n            num_persons = M_persons\n        else:\n            raise ValueError(f\"Input X has an unexpected shape: {X.shape}. \"\n                             f\"Expected (N,C,T,V), (N,T,V,C), (N,M,C,T,V) or (N,C,T,V,M) with C={self.input_dim}\")\n\n        # Apply encoder\n        # X_proc shape: (N_actual, C_in, T_in, V)\n        encoded_features, _residuals = self.encoder(X_proc, t_diffusion) # _residuals không dùng trong STSE cơ bản\n        # encoded_features shape: (N_actual, self.hidden_dimension, T_final_encoder, V)\n        \n        X_shape_after_gat_stack = encoded_features.size() \n        \n        # Flatten and bottleneck\n        # Reshape để phù hợp với self.btlnk đã được tính toán với t_final_encoder\n        if X_shape_after_gat_stack[2] != self.t_final_encoder or X_shape_after_gat_stack[3] != self.n_joints:\n             print(f\"Warning: Encoder output T,V ({X_shape_after_gat_stack[2]},{X_shape_after_gat_stack[3]}) \"\n                   f\"differs from expected ({self.t_final_encoder},{self.n_joints}). Check strides and n_joints.\")\n\n        flattened_features = encoded_features.reshape(X_shape_after_gat_stack[0], -1) # (N_actual, self.hidden_dimension * T_final_encoder * V)\n        Z = self.btlnk(flattened_features) # (N_actual, self.latent_dim)\n        \n        if return_shape:\n            return Z, X_shape_after_gat_stack # Trả về shape trước khi flatten cho Decoder\n        return Z\n\n    def forward(self, X: torch.Tensor, t_diffusion: torch.Tensor = None) -> Tuple[torch.Tensor, None]:\n        # STSE chỉ làm nhiệm vụ encode, không decode. Output Z và None (cho X_rec)\n        Z = self.encode(X, return_shape=False, t_diffusion=t_diffusion)\n        return Z, None # Trả về Z và None để khớp với output signature của STSAE nếu cần\n\nclass STSAE(STSE): # Spatio-Temporal SpGraphAutoEncoder\n    def __init__(\n        self,\n        c_in: int,\n        h_dim: int = 32,\n        latent_dim: int = 64,\n        n_frames: int = 12,\n        n_joints: int = 17,\n        layer_channels: List[int] = [64, 128, 256], # Shared for encoder/decoder structure\n        dropout: float = 0.3,\n        adjacency_matrix=None,\n        num_heads_gat: int = 4,\n        encoder_temporal_strides: List[int] = None,\n        decoder_temporal_strides: List[int] = None,\n        device: Union[str, torch.device] = 'cpu' # Sửa DeviceObjType\n    ) -> None:\n        super(STSAE, self).__init__(\n            c_in=c_in,\n            h_dim=h_dim,\n            latent_dim=latent_dim,\n            n_frames=n_frames,\n            n_joints=n_joints,\n            layer_channels=layer_channels, # Sẽ được dùng để cấu hình encoder_strides\n            dropout=dropout,\n            device=device,\n            adjacency_matrix=adjacency_matrix,\n            num_heads_gat=num_heads_gat,\n            encoder_temporal_strides=encoder_temporal_strides, # Truyền xuống super\n            decoder_temporal_strides=decoder_temporal_strides  # Truyền xuống super (sẽ dùng trong build_decoder)\n        )\n        # Decoder\n        self.build_decoder()\n\n    def build_decoder(self) -> None:\n        # output_dim của Decoder là self.input_dim (c_in ban đầu)\n        # layer_channels cho Decoder sẽ đảo ngược của Encoder (không tính h_dim và c_in)\n        # hidden_dimension đầu vào cho Decoder là self.hidden_dimension (output của Encoder GAT_TCN stack)\n        \n        self.decoder = Decoder(\n            output_dim=self.input_dim, # Target C_out là c_in ban đầu\n            layer_channels=self.layer_channels_config, # List các channels trung gian (sẽ được đảo ngược trong Decoder)\n            hidden_dimension=self.hidden_dimension, # Input C_in cho Decoder GAT_TCN stack\n            n_frames=self.n_frames, # Thông tin, không dùng trực tiếp (T_final_encoder là T đầu vào cho Decoder)\n            n_joints=self.n_joints, # Thông tin\n            dropout=self.dropout,\n            adjacency_matrix=self.adjacency_matrix,\n            num_heads_gat=self.num_heads_gat,\n            temporal_strides=self.decoder_strides # Strides đã được xác định trong __init__ của STSE\n        )\n        \n        # Reverse bottleneck\n        # Input cho rev_btlnk là self.latent_dim\n        # Output features phải khớp với input của Decoder GAT_TCN stack trước khi reshape\n        # đó là self.hidden_dimension * self.t_final_encoder * self.n_joints\n        out_features_rev_bottleneck = self.hidden_dimension * self.t_final_encoder * self.n_joints\n        self.rev_btlnk = nn.Linear(in_features=self.latent_dim, out_features=out_features_rev_bottleneck)\n\n    def decode(self, Z: torch.Tensor, shape_before_flatten: Tuple[int], t_diffusion: torch.Tensor = None) -> torch.Tensor:\n        # Z: (N_actual, self.latent_dim)\n        # shape_before_flatten: (N_actual, self.hidden_dimension, T_final_encoder, V)\n        \n        X_from_latent = self.rev_btlnk(Z) # (N_actual, self.hidden_dimension * T_final_encoder * V)\n        \n        # Reshape để khớp với input của Decoder GAT_TCN stack\n        # (N_actual, self.hidden_dimension, self.t_final_encoder, self.n_joints)\n        reconstructed_spatial_temporal_shape = X_from_latent.view(shape_before_flatten)\n        \n        # Apply decoder\n        # X_rec shape: (N_actual, self.input_dim, T_output_decoder, V)\n        # T_output_decoder sẽ bằng T_final_encoder nếu decoder_strides toàn là 1\n        X_rec = self.decoder(reconstructed_spatial_temporal_shape, t_diffusion)\n        \n        # Xử lý output shape để trả về (N, C, T, V) hoặc (N, M, C, T, V)\n        # Hiện tại X_rec là (N_actual, self.input_dim, T_out_dec, V)\n        # Nếu num_persons được lưu từ encode, có thể reshape lại.\n        # Tuy nhiên, DCMD thường xử lý (N*M) ở ngoài rồi reshape lại.\n        return X_rec\n\n    def forward(\n        self,\n        X: torch.Tensor, # Input shape: (batch, c_in, n_frames, n_joints) hoặc (N,M,C,T,V)\n        t_diffusion: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]: # Z, X_reconstructed\n        \n        original_shape = X.shape\n        M_persons = 1 # CORRECTED LINE\n        if X.dim() == 5 and X.size(1) != self.input_dim: # (N, M, C, T, V) - Assumes C is at index 2\n            if X.size(2) == self.input_dim: # Check if C is indeed at index 2\n                 M_persons = original_shape[1]\n        elif X.dim() == 5 and X.size(1) == self.input_dim: # (N, C, T, V, M) - Assumes M is at index 4\n             if X.size(4) != self.n_joints : # A simple check if last dim could be M\n                  M_persons = original_shape[4]\n\n\n        Z, shape_before_flatten = self.encode(X, return_shape=True, t_diffusion=t_diffusion)\n        X_rec = self.decode(Z, shape_before_flatten, t_diffusion=t_diffusion)\n\n        # Reshape X_rec về lại dạng có thể có M (num_persons) nếu input ban đầu có\n        # X_rec hiện là (N_actual, self.input_dim, T_out_decoder, V)\n        # N_actual = N * M_persons\n        if M_persons > 1 and X.dim() == 5:\n            if original_shape[1] == M_persons : # (N, M, C, T, V)\n                 N = original_shape[0]\n                 C_out, T_out, V_out = X_rec.shape[1], X_rec.shape[2], X_rec.shape[3]\n                 X_rec = X_rec.view(N, M_persons, C_out, T_out, V_out)\n            elif original_shape[4] == M_persons : # (N, C, T, V, M)\n                 N, C_out, T_in_orig, V_in_orig = original_shape[0], original_shape[1], original_shape[2], original_shape[3]\n                 _, _, T_out, V_out = X_rec.shape\n                 X_rec = X_rec.view(N, C_out, T_out, V_out, M_persons)\n\n        return Z, X_rec\n\nif __name__ == '__main__':\n    # --- Example Usage ---\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Parameters\n    c_in_test = 3\n    n_frames_test = 24\n    n_joints_test = 17 # Cho Human3D (h36m layout)\n    #n_joints_test = 25 # Cho NTU-RGB+D\n\n    batch_size_test = 2\n    num_persons_test = 1 # Hoặc 2 để test trường hợp M > 1\n\n    # Model configuration\n    h_dim_test = 64         # Output channels of encoder GAT stack\n    latent_dim_test = 128   # Bottleneck Z dimension\n    # layer_channels_test = [64, 128, 256] # 3 intermediate GAT_TCN blocks in encoder before final projection to h_dim\n                                        # Encoder sẽ có 3+1 = 4 GAT_TCN blocks\n    layer_channels_test = [c_in_test*2, c_in_test*4, c_in_test*8] # Ví dụ: [6, 12, 24]\n    num_encoder_gat_tcn_blocks = len(layer_channels_test) + 1 # +1 vì block cuối ra h_dim_test\n    \n    # Encoder strides: 4 blocks -> [1,2,1,2] -> T_out = T_in / 4\n    # enc_strides = [1, 2, 1, 2] if num_encoder_gat_tcn_blocks == 4 else [1]*num_encoder_gat_tcn_blocks\n    enc_strides = [1] * num_encoder_gat_tcn_blocks\n    if num_encoder_gat_tcn_blocks >=2 : enc_strides[1] = 2\n    if num_encoder_gat_tcn_blocks >=4 : enc_strides[3] = 2\n\n\n    # Decoder strides: 4 blocks (input h_dim_test -> intermediate -> ... -> c_in_test) -> [1,1,1,1]\n    num_decoder_gat_tcn_blocks = len(layer_channels_test) + 1\n    # dec_strides = [1, 1, 1, 1] if num_decoder_gat_tcn_blocks == 4 else [1]*num_decoder_gat_tcn_blocks\n    dec_strides = [1] * num_decoder_gat_tcn_blocks\n    # If GAT_TCN_Block supported ConvTranspose, you might do:\n    # if num_decoder_gat_tcn_blocks >=2 : dec_strides[0] = 2 # Upsample at start\n    # if num_decoder_gat_tcn_blocks >=4 : dec_strides[2] = 2 # Upsample again\n\n\n    print(f\"Configuring STSAE with: \\n\"\n          f\"  Input Channels (c_in): {c_in_test}\\n\"\n          f\"  Encoder GAT Stack Output Channels (h_dim): {h_dim_test}\\n\"\n          f\"  Bottleneck Latent Dim (latent_dim): {latent_dim_test}\\n\"\n          f\"  Frames: {n_frames_test}, Joints: {n_joints_test}\\n\"\n          f\"  Intermediate Layer Channels Config: {layer_channels_test}\\n\"\n          f\"  Encoder Temporal Strides: {enc_strides}\\n\"\n          f\"  Decoder Temporal Strides: {dec_strides}\")\n\n\n    # Create STSAE model instance\n    stsae_model = STSAE(\n        c_in=c_in_test,\n        h_dim=h_dim_test,\n        latent_dim=latent_dim_test,\n        n_frames=n_frames_test,\n        n_joints=n_joints_test,\n        layer_channels=layer_channels_test,\n        dropout=0.1,\n        adjacency_matrix=None, # Let it use Graph class based on n_joints\n        num_heads_gat=2,\n        encoder_temporal_strides=enc_strides,\n        decoder_temporal_strides=dec_strides,\n        device=device\n    ).to(device)\n\n    # Create dummy input tensor\n    # DCMD thường dùng (N, M, C, T, V)\n    if num_persons_test > 1:\n        dummy_X = torch.randn(batch_size_test, num_persons_test, c_in_test, n_frames_test, n_joints_test).to(device)\n    else: # (N, C, T, V)\n        dummy_X = torch.randn(batch_size_test, c_in_test, n_frames_test, n_joints_test).to(device)\n\n    print(f\"\\nInput X shape: {dummy_X.shape}\")\n\n    # Test forward pass\n    try:\n        Z_latent, X_reconstructed = stsae_model(dummy_X)\n        print(f\"Latent Z shape: {Z_latent.shape}\") # Expected: (batch_size * num_persons, latent_dim)\n        print(f\"Reconstructed X_rec shape: {X_reconstructed.shape}\") # Expected: same as processed input to encoder\n        \n        # Check if output shape matches input shape (considering num_persons)\n        expected_rec_shape = dummy_X.shape\n        if num_persons_test > 1 and dummy_X.dim() == 5 and dummy_X.size(1) == num_persons_test: # N,M,C,T,V\n            pass # X_reconstructed should already be in N,M,C,T,V\n        elif num_persons_test == 1 and dummy_X.dim() == 4: # N,C,T,V\n             pass # X_reconstructed should already be in N,C,T,V\n        else: # Other cases might require specific reshape check not covered by current auto-reshape in STSAE forward\n            print(f\"Note: Detailed output shape check for M>1 might need more specific logic if input was not N,M,C,T,V.\")\n\n\n        assert X_reconstructed.shape == expected_rec_shape, \\\n            f\"Shape mismatch! Reconstructed: {X_reconstructed.shape}, Expected: {expected_rec_shape}\"\n        print(\"Forward pass successful and output shape matches input shape.\")\n\n    except Exception as e:\n        print(f\"Error during STSAE forward pass: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n    # Print number of parameters\n    num_params = sum(p.numel() for p in stsae_model.parameters() if p.requires_grad)\n    print(f\"Number of trainable parameters in STSAE model: {num_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:51:55.336390Z","iopub.execute_input":"2025-06-03T01:51:55.337170Z","iopub.status.idle":"2025-06-03T01:51:55.353306Z","shell.execute_reply.started":"2025-06-03T01:51:55.337125Z","shell.execute_reply":"2025-06-03T01:51:55.352560Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/stsae/stsae.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/gat.py\n\n# models/gcae/gat.py (hoặc một đường dẫn tương tự trong dự án của bạn)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.utils import dense_to_sparse # Tiện ích chuyển A sang edge_index\n\n# Giả sử bạn có một lớp Graph tương tự như trong project gốc để lấy ma trận kề\n# Ví dụ: from .graph import Graph \n# Nếu không, bạn cần truyền ma trận kề A trực tiếp vào Model_GAT\n\n# --- Utility Function ---\ndef adj_to_edge_index(adj_matrix):\n    \"\"\"\n    Chuyển một ma trận kề (dense) sang định dạng edge_index của PyG.\n    Args:\n        adj_matrix (torch.Tensor): Ma trận kề, shape (num_nodes, num_nodes)\n    Returns:\n        torch.Tensor: edge_index, shape (2, num_edges)\n    \"\"\"\n    edge_index, _ = dense_to_sparse(adj_matrix)\n    return edge_index\n\n# --- Spatial Attention Layer (GAT) ---\nclass SpatialAttentionLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heads, concat=True, dropout=0.6, leaky_relu_negative_slope=0.2):\n        super().__init__()\n        self.gat_conv = GATConv(in_channels, \n                                out_channels // num_heads if concat else out_channels,\n                                heads=num_heads, \n                                concat=concat, \n                                dropout=dropout,\n                                negative_slope=leaky_relu_negative_slope)\n        self.concat = concat\n        self.num_heads = num_heads\n        self.out_channels_per_head = out_channels // num_heads if concat else out_channels\n\n    def forward(self, x_node_features, edge_index):\n        \"\"\"\n        Args:\n            x_node_features (torch.Tensor): Features của các node. \n                                           Shape: (batch_size * num_nodes, in_channels)\n                                           hoặc (num_nodes, in_channels) nếu xử lý từng sample.\n            edge_index (torch.Tensor): Định nghĩa kết nối đồ thị. Shape: (2, num_edges)\n        Returns:\n            torch.Tensor: Output features sau GAT. Shape: (batch_size * num_nodes, out_channels)\n        \"\"\"\n        return self.gat_conv(x_node_features, edge_index)\n\n# --- Temporal Convolutional Network (TCN) Block ---\nclass TemporalConvNet(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dropout=0.2):\n        super().__init__()\n        pad = (kernel_size - 1) // 2\n        self.tcn_block = nn.Sequential(\n            nn.BatchNorm2d(in_channels), # Batch norm trên channels\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                (kernel_size, 1), # Kernel chỉ theo chiều thời gian T, không gian là 1\n                (stride, 1),      # Stride chỉ theo chiều thời gian T\n                (pad, 0),         # Padding chỉ theo chiều thời gian T\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout(dropout, inplace=True) # Dropout sau batch norm và conv\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor. Shape: (N, C_in, T, V) \n                              N: batch_size, C_in: channels, T: time_frames, V: num_nodes (vertices)\n        Returns:\n            torch.Tensor: Output tensor. Shape: (N, C_out, T_out, V)\n        \"\"\"\n        return self.tcn_block(x)\n\n# --- GAT-TCN Block ---\nclass GAT_TCN_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heads_gat, \n                 temporal_kernel_size=9, temporal_stride=1,\n                 residual=True, dropout=0.2, A_is_dense=True):\n        super().__init__()\n        self.residual = residual\n        self.A_is_dense = A_is_dense # True nếu A là ma trận kề, False nếu A đã là edge_index\n\n        # Spatial Attention Layer\n        # GAT sẽ hoạt động trên từng frame, nên input channels là in_channels\n        # Output channels của GAT sẽ là input channels cho TCN\n        self.spatial_attn = SpatialAttentionLayer(in_channels, out_channels, num_heads_gat, dropout=dropout)\n        \n        # Temporal Convolutional Layer\n        # TCN hoạt động trên output của GAT. Channels đầu vào của TCN là out_channels từ GAT.\n        self.tcn = TemporalConvNet(out_channels, out_channels, \n                                   kernel_size=temporal_kernel_size, \n                                   stride=temporal_stride, \n                                   dropout=dropout)\n\n        if not residual:\n            self.residual_conv = lambda x: 0\n        elif (in_channels == out_channels) and (temporal_stride == 1):\n            self.residual_conv = lambda x: x\n        else:\n            # Conv2d để điều chỉnh số kênh và kích thước thời gian cho residual connection\n            self.residual_conv = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(temporal_stride, 1)),\n                nn.BatchNorm2d(out_channels),\n            )\n        \n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, adj):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor. Shape: (N, C_in, T, V)\n            adj (torch.Tensor): Ma trận kề (nếu self.A_is_dense=True, shape (V,V) hoặc (num_adj_matrices, V,V))\n                               hoặc edge_index (nếu self.A_is_dense=False, shape (2, num_edges))\n        Returns:\n            torch.Tensor: Output tensor. Shape: (N, C_out, T_out, V)\n        \"\"\"\n        N, C_in, T, V = x.size()\n        res = self.residual_conv(x) # (N, C_out, T_out, V)\n\n        # --- Spatial Processing (GAT on each time frame) ---\n        # GATConv thường mong muốn input (num_total_nodes, num_node_features)\n        # Ta sẽ xử lý từng frame thời gian hoặc batch tất cả các node qua GAT\n        \n        # Chuyển x về dạng (N, T, V, C_in) để dễ lặp qua T\n        x_permuted = x.permute(0, 2, 3, 1).contiguous() # (N, T, V, C_in)\n        \n        batched_node_features = x_permuted.view(N * T * V, C_in) # (N*T*V, C_in)\n\n        # Xử lý edge_index:\n        if self.A_is_dense:\n            if adj.dim() == 3: # (num_adj_strategies, V, V), chọn 1 strategy\n                current_adj = adj[0] # Ví dụ: chọn strategy đầu tiên\n            elif adj.dim() == 2: # (V,V)\n                current_adj = adj\n            else:\n                raise ValueError(f\"Adjacency matrix `adj` has unexpected dimensions: {adj.shape}\")\n            \n            # Tạo edge_index cho một đồ thị (V,V)\n            single_graph_edge_index = adj_to_edge_index(current_adj).to(x.device)\n            \n            # Mở rộng edge_index cho N*T batches của đồ thị\n            # Mỗi đồ thị trong batch (N*T cái) có V nodes\n            # Đây là phần quan trọng và có thể cần tối ưu hóa\n            # Cách đơn giản: lặp (chậm) hoặc dùng các hàm batching của PyG (hiệu quả hơn)\n            \n            # Cách hiệu quả hơn: Tạo edge_index cho batch các đồ thị giống hệt nhau\n            num_graphs_in_batch = N * T\n            node_offsets = torch.arange(0, num_graphs_in_batch * V, step=V, device=x.device) # (N*T)\n            \n            # Mở rộng edge_index\n            batch_edge_index_0 = single_graph_edge_index[0].repeat(num_graphs_in_batch) + \\\n                                 node_offsets.repeat_interleave(single_graph_edge_index.size(1))\n            batch_edge_index_1 = single_graph_edge_index[1].repeat(num_graphs_in_batch) + \\\n                                 node_offsets.repeat_interleave(single_graph_edge_index.size(1))\n            batched_edge_index = torch.stack([batch_edge_index_0, batch_edge_index_1], dim=0) # (2, (N*T)*num_edges_single)\n\n        else: # adj đã là edge_index (giả sử nó đã được batch đúng cách hoặc là cố định)\n            if adj.size(0) != 2 :\n                 raise ValueError(f\"Provided edge_index `adj` has unexpected shape: {adj.shape}\")\n            # Nếu adj là edge_index cho 1 đồ thị, cần batch nó tương tự như trên\n            # Giả sử adj đã được batch sẵn cho N*T đồ thị\n            batched_edge_index = adj.to(x.device) \n            # Lưu ý: Nếu adj chỉ là edge_index của một đồ thị đơn lẻ, bạn cần logic batching ở đây.\n\n        # Apply GAT\n        # batched_node_features: (N*T*V, C_in)\n        # batched_edge_index: (2, (N*T)*num_edges_single)\n        gat_out_flat = self.spatial_attn(batched_node_features, batched_edge_index) # (N*T*V, C_out)\n        \n        # Reshape lại về (N, T, V, C_out)\n        x_spatial_processed = gat_out_flat.view(N, T, V, -1) # C_out sẽ được suy ra\n        \n        # Chuyển về (N, C_out, T, V) cho TCN\n        x_for_tcn = x_spatial_processed.permute(0, 3, 1, 2).contiguous()\n\n        # --- Temporal Processing (TCN) ---\n        x_temporal = self.tcn(x_for_tcn) # (N, C_out, T_out, V)\n        \n        # Residual connection and ReLU\n        x_out = self.relu(x_temporal + res)\n        return x_out\n\n# --- Main GAT-based Model ---\nclass Model_GAT(nn.Module):\n    def __init__(\n            self,\n            in_channels,           # non-default\n            num_classes,           # non-default\n            num_nodes,             # non-default\n            adjacency_matrix,      # non-default\n            num_heads_gat=8,       # default\n            dropout=0.2,           # default\n            gat_tcn_channels_config=None,  # default\n            **kwargs               # default-capturing\n        ):\n        super().__init__()\n        self.A_is_dense = True\n\n        if gat_tcn_channels_config is None:\n            # Cấu hình mặc định tương tự STGCN gốc cho dễ so sánh\n            gat_tcn_channels_config = [\n                (in_channels, 64, 1), (64, 64, 1), (64, 64, 1),\n                (64, 128, 2), (128, 128, 1), (128, 128, 1),\n                (128, 256, 2), (256, 256, 1), (256, 256, 1)\n            ]\n\n        # Lưu trữ ma trận kề (hoặc edge_index nếu đã xử lý trước)\n        # Giả sử adjacency_matrix là ma trận kề (dense)\n        # Nếu A có nhiều \"strategies\" như trong STGCN, ta sẽ chọn 1 hoặc cần logic xử lý phức tạp hơn\n        if isinstance(adjacency_matrix, list) or \\\n           (isinstance(adjacency_matrix, torch.Tensor) and adjacency_matrix.dim() == 3):\n            # Giả sử dùng strategy đầu tiên nếu có nhiều\n            self.register_buffer('A', torch.tensor(adjacency_matrix[0], dtype=torch.float32) \n                                     if isinstance(adjacency_matrix, list) else adjacency_matrix[0].float())\n        elif isinstance(adjacency_matrix, torch.Tensor) and adjacency_matrix.dim() == 2:\n            self.register_buffer('A', adjacency_matrix.float())\n        else:\n            raise ValueError(\"adjacency_matrix format not recognized or not provided.\")\n\n        self.data_bn = nn.BatchNorm1d(in_channels * num_nodes) # Chuẩn hóa ban đầu\n\n        self.gat_tcn_blocks = nn.ModuleList()\n        current_channels = in_channels\n        for i, (block_in_c, block_out_c, block_stride_t) in enumerate(gat_tcn_channels_config):\n            # Đảm bảo in_channels của block khớp với current_channels\n            actual_block_in_c = current_channels if i==0 else gat_tcn_channels_config[i-1][1]\n\n            self.gat_tcn_blocks.append(\n                GAT_TCN_Block(\n                    in_channels=actual_block_in_c,\n                    out_channels=block_out_c,\n                    num_heads_gat=num_heads_gat,\n                    temporal_stride=block_stride_t,\n                    dropout=dropout,\n                    A_is_dense=True # Cho biết self.A là ma trận kề\n                )\n            )\n            current_channels = block_out_c # Cập nhật cho block tiếp theo (ít quan trọng nếu config rõ ràng)\n\n        # Output layer (ví dụ: cho classification hoặc feature extraction)\n        # Kích thước này phụ thuộc vào nhiệm vụ cuối cùng của DCMD\n        # Ví dụ: một lớp Fully Connected sau global average pooling\n        self.pool = nn.AdaptiveAvgPool2d((1, 1)) # Pool qua T và V\n        self.fc = nn.Linear(current_channels, num_classes)\n\n\n    def forward(self, x):\n        # Input x shape: (N, C, T, V, M) hoặc (N, C, T, V)\n        # N: batch_size, C: channels (e.g., 3 for xyz), T: frames, V: joints, M: people\n        if x.dim() == 5: # N, C, T, V, M\n            N, C, T, V, M = x.size()\n            x = x.permute(0, 4, 1, 2, 3).contiguous()  # N, M, C, T, V\n            x = x.view(N * M, C, T, V) # (N*M, C, T, V)\n        elif x.dim() == 4: # (N, C, T, V)\n            N_actual, C, T, V = x.size() # N_actual có thể là N*M\n        else:\n            raise ValueError(f\"Input tensor x has unexpected dimensions: {x.shape}\")\n\n        # Data BN: (N*M, C, T, V) -> (N*M, C*V, T) -> BN -> (N*M, C, T, V)\n        x_bn_in = x.permute(0, 1, 3, 2).contiguous().view(x.size(0), C * V, T)\n        x_bn_out = self.data_bn(x_bn_in)\n        x = x_bn_out.view(x.size(0), C, V, T).permute(0, 1, 3, 2).contiguous() # (N*M, C, T, V)\n\n        # GAT-TCN Blocks\n        for block in self.gat_tcn_blocks:\n            x = block(x, self.A) # Truyền ma trận kề A (hoặc edge_index đã xử lý)\n\n        # Global Pooling và FC (ví dụ cho classification)\n        # x shape sau blocks: (N*M, C_out, T_final, V)\n        pooled_x = self.pool(x) # (N*M, C_out, 1, 1)\n        squeezed_x = pooled_x.view(pooled_x.size(0), -1) # (N*M, C_out)\n        \n        out = self.fc(squeezed_x) # (N*M, num_classes)\n\n        # Reshape lại nếu ban đầu có M (số người)\n        if x.dim() == 5 and M > 1: # Nếu input gốc có M\n             out = out.view(N, M, -1)\n\n        return out\n\n# --- Ví dụ cách sử dụng (để kiểm tra nhanh, không phải để huấn luyện) ---\nif __name__ == '__main__':\n    # --- Thông số giả định ---\n    batch_size = 2\n    in_channels_data = 3  # xyz\n    num_frames = 32\n    num_nodes_data = 25 # Số khớp\n    num_persons = 1 \n    num_classes_out = 10 # Ví dụ cho classification\n\n    # Tạo ma trận kề giả định (ví dụ: đồ thị kết nối đầy đủ đơn giản)\n    # Trong thực tế, bạn sẽ load từ graph.py hoặc file cấu hình\n    adj = torch.ones(num_nodes_data, num_nodes_data) # Kết nối đầy đủ (bao gồm self-loops)\n    # adj = torch.rand(num_nodes_data, num_nodes_data) > 0.7 # Đồ thị thưa hơn\n    # adj.fill_diagonal_(1) # Đảm bảo self-loops\n\n    # Khởi tạo model\n    gat_model = Model_GAT(\n        in_channels=in_channels_data,\n        num_classes=num_classes_out,\n        num_nodes=num_nodes_data,\n        adjacency_matrix=adj, # Truyền ma trận kề\n        num_heads_gat=4,\n        dropout=0.25,\n        # Bỏ qua gat_tcn_channels_config để dùng mặc định\n    )\n\n    # Tạo dữ liệu đầu vào giả\n    # (N, C, T, V) hoặc (N, C, T, V, M)\n    if num_persons > 1:\n        dummy_input = torch.randn(batch_size, in_channels_data, num_frames, num_nodes_data, num_persons)\n    else:\n        dummy_input = torch.randn(batch_size, in_channels_data, num_frames, num_nodes_data)\n    \n    # Đặt model ở chế độ eval để dropout không hoạt động (nếu có)\n    gat_model.eval() \n    \n    # Forward pass\n    try:\n        print(f\"Input shape: {dummy_input.shape}\")\n        output = gat_model(dummy_input)\n        print(f\"Output shape: {output.shape}\") # Mong muốn (batch_size * num_persons, num_classes_out)\n                                            # Hoặc (batch_size, num_persons, num_classes_out) nếu M > 1\n    except Exception as e:\n        print(f\"Error during forward pass: {e}\")\n        import traceback\n        traceback.print_exc()\n\n    # Kiểm tra số lượng tham số\n    num_params = sum(p.numel() for p in gat_model.parameters() if p.requires_grad)\n    print(f\"Number of trainable parameters in Model_GAT: {num_params:,}\")\n\n    # --- Ví dụ với edge_index đã được chuẩn bị trước (ít phổ biến hơn khi dùng trực tiếp A) ---\n    # print(\"\\n--- Example with pre-computed edge_index (less common for this setup) ---\")\n    # single_edge_index = adj_to_edge_index(adj) \n    # gat_model_with_edge_idx = Model_GAT(...) # Cần sửa Model_GAT để nhận is_A_dense=False và edge_index\n    # ...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:16:23.840244Z","iopub.execute_input":"2025-06-03T01:16:23.840481Z","iopub.status.idle":"2025-06-03T01:16:23.852572Z","shell.execute_reply.started":"2025-06-03T01:16:23.840466Z","shell.execute_reply":"2025-06-03T01:16:23.851716Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/DCMD-main/models/gcae/gat.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/common/components.py\nfrom typing import List, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n# import models.gcae.stsgcn as stsgcn  # Keep for legacy if needed, or remove if fully transitioned\n# Ensure GAT_TCN_Block is correctly imported from your gat.py (assuming it's in models.gcae.gat)\nfrom models.gcae.gat import GAT_TCN_Block\n# Ensure Graph is imported from your new graph.py\nfrom models.gcae.graph import Graph # This import should now work\n\nclass Encoder(nn.Module):\n    def __init__(\n            self,\n            input_dim: int,\n            layer_channels: List[int],\n            hidden_dimension: int, # This is the output dim of the last GAT_TCN_Block\n            n_frames: int, # Currently not directly used by GAT_TCN_Block layers\n            n_joints: int, # Currently not directly used by GAT_TCN_Block layers\n            dropout: float,\n            adjacency_matrix: torch.Tensor, # Expects a (V,V) or (num_strategies, V,V) tensor\n            num_heads_gat: int = 4,\n            temporal_strides: List[int] = None, # Add this to control strides per layer\n            bias: bool = True\n        ) -> None:\n        super().__init__()\n        self.input_dim = input_dim\n        \n        # The last channel in layer_channels should be hidden_dimension\n        if layer_channels[-1] != hidden_dimension:\n            # Or adjust layer_channels to ensure the final output is hidden_dimension\n            # For simplicity, let's assume layer_channels defines intermediate outputs\n            # and the last one is the input to a final projection if needed,\n            # or hidden_dimension is the output of the last GAT_TCN layer.\n             # Let's make layer_channels define all block outputs, hidden_dimension is the last one\n            if not any(lc == hidden_dimension for lc in layer_channels):\n                 # If hidden_dimension is meant to be the final output channel from the GAT_TCN stack\n                self.layer_channels = layer_channels + [hidden_dimension]\n            else: # Assume hidden_dimension is already the last element of layer_channels\n                self.layer_channels = layer_channels\n\n        else: # if layer_channels[-1] == hidden_dimension\n            self.layer_channels = layer_channels\n\n        self.internal_block_output_channels = layer_channels + [hidden_dimension]\n        self.hidden_dimension = hidden_dimension # Final output channel count\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.dropout = dropout\n        self.adjacency_matrix = adjacency_matrix # This should be a tensor\n        self.num_heads_gat = num_heads_gat\n        self.bias = bias # Bias for GAT_TCN_Block not explicitly used in GATConv/Conv2d like Linear\n\n        if temporal_strides is None:\n            self.temporal_strides = [1] * len(self.internal_block_output_channels)\n        elif len(temporal_strides) != len(self.internal_block_output_channels):\n            raise ValueError(\n                f\"Length of temporal_strides ({len(temporal_strides)}) must match \"\n                f\"the number of GAT-TCN blocks ({len(self.internal_block_output_channels)}) in Encoder.\"\n            )\n        else:\n            self.temporal_strides = temporal_strides\n\n        self.build_model()\n\n    def build_model(self):\n        input_channels = self.input_dim\n        model_layers = nn.ModuleList()\n        \n        # Ensure adjacency_matrix is a tensor before passing to blocks\n        if not isinstance(self.adjacency_matrix, torch.Tensor):\n            # Assuming it's a numpy array from Graph class if not already a tensor\n            current_adj_matrix = torch.from_numpy(self.adjacency_matrix).float()\n        else:\n            current_adj_matrix = self.adjacency_matrix\n\n        # If adjacency_matrix has multiple strategies (e.g., (num_strat, V, V))\n        # GAT_TCN_Block currently expects one (V,V) matrix if A_is_dense=True.\n        # Or it expects a pre-batched edge_index.\n        # For simplicity, if multiple A's, use the first one.\n        if current_adj_matrix.ndim == 3:\n            # print(f\"Encoder: Using the first adjacency matrix strategy. Shape: {current_adj_matrix.shape}\")\n            self.processed_adj_matrix = current_adj_matrix[0]\n        elif current_adj_matrix.ndim == 2:\n            self.processed_adj_matrix = current_adj_matrix\n        else:\n            raise ValueError(f\"Adjacency matrix has unexpected dimensions: {current_adj_matrix.shape}\")\n\n\n        for i, out_channels in enumerate(self.layer_channels):\n            model_layers.append(\n                GAT_TCN_Block(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    num_heads_gat=self.num_heads_gat,\n                    temporal_kernel_size=9, # Can be made configurable\n                    temporal_stride=self.temporal_strides[i], # Use configured stride\n                    dropout=self.dropout,\n                    A_is_dense=True # Assumes processed_adj_matrix is (V,V)\n                )\n            )\n            input_channels = out_channels\n        self.model_layers = model_layers\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        # X expected shape: (N_actual, C_in, T, V) where N_actual could be N*M\n        layers_out = [X]\n        device = X.device # Get device from input tensor\n        adj_matrix_on_device = self.processed_adj_matrix.to(device)\n\n        for layer in self.model_layers:\n            out_X = layer(layers_out[-1], adj_matrix_on_device)\n            layers_out.append(out_X)\n        return layers_out[-1], layers_out[:-1]\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        output_dim: int, # This is c_in of the original data\n        layer_channels: List[int], # Intermediate channels, should be reverse of encoder's intermediate\n        hidden_dimension: int, # This is the input dim to the Decoder (output of Encoder's GAT_TCN stack)\n        n_frames: int,\n        n_joints: int,\n        dropout: float,\n        adjacency_matrix: torch.Tensor,\n        num_heads_gat: int = 4,\n        temporal_strides: List[int] = None, # For upsampling, stride usually 1, or use ConvTranspose\n        bias: bool = True\n    ) -> None:\n        super().__init__()\n        self.output_dim = output_dim\n        # For decoder, layer_channels are typically reversed from encoder,\n        # starting from hidden_dimension and ending at output_dim\n        self.layer_channels_config = [hidden_dimension] + layer_channels[::-1] # Start with hidden_dim\n        # The final GAT_TCN block should output self.output_dim\n        self.final_layer_channels = self.layer_channels_config[:-1] # channels for GAT_TCN blocks\n        self.final_output_channel = self.layer_channels_config[-1] # target for a final projection\n        \n        # Let's adjust to make GAT_TCN_Block outputs match layer_channels\n        # Input to first GAT_TCN is hidden_dimension\n        # Output of last GAT_TCN is output_dim\n        self.layer_channels_for_gat_tcn = layer_channels[::-1] + [output_dim]\n\n\n        self.hidden_dimension = hidden_dimension # Input channel for the first GAT_TCN_Block\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.dropout = dropout\n        self.adjacency_matrix = adjacency_matrix\n        self.num_heads_gat = num_heads_gat\n        self.bias = bias\n\n        if temporal_strides is None:\n            # For decoder, strides are often 1, or if upsampling is needed via strided conv,\n            # it's usually ConvTranspose2d. GAT_TCN_Block uses Conv2d.\n            # So, for upsampling T, GAT_TCN_Block is not ideal as is.\n            # Assuming stride 1 for TCN blocks in decoder for now.\n            self.temporal_strides = [1] * len(self.layer_channels_for_gat_tcn)\n        elif len(temporal_strides) != len(self.layer_channels_for_gat_tcn):\n            # If strides are provided, they should match the number of GAT_TCN blocks\n            # This might be used if one wants to use \"fractionally strided convolutions\"\n            # conceptually, but Conv2d in TCN doesn't directly do T upsampling with stride > 1.\n            # Consider nn.ConvTranspose2d in TCN for upsampling if needed.\n            print(\"Warning: temporal_strides for Decoder with GAT_TCN_Block might not perform T-upsampling as expected.\")\n            self.temporal_strides = temporal_strides\n        else:\n            self.temporal_strides = temporal_strides\n\n        self.build_model()\n\n    def build_model(self):\n        input_channels = self.hidden_dimension # Input from bottleneck/latent space\n        model_layers = nn.ModuleList()\n\n        if not isinstance(self.adjacency_matrix, torch.Tensor):\n            current_adj_matrix = torch.from_numpy(self.adjacency_matrix).float()\n        else:\n            current_adj_matrix = self.adjacency_matrix\n\n        if current_adj_matrix.ndim == 3:\n            # print(f\"Decoder: Using the first adjacency matrix strategy. Shape: {current_adj_matrix.shape}\")\n            self.processed_adj_matrix = current_adj_matrix[0]\n        elif current_adj_matrix.ndim == 2:\n            self.processed_adj_matrix = current_adj_matrix\n        else:\n            raise ValueError(f\"Adjacency matrix has unexpected dimensions: {current_adj_matrix.shape}\")\n\n        # layer_channels_for_gat_tcn = e.g. [hidden_dim_encoder_out, intermediate1, intermediate2, final_output_dim]\n        # GAT_TCN_Block(input_channels, layer_channels_for_gat_tcn[0], ...)\n        # GAT_TCN_Block(layer_channels_for_gat_tcn[0], layer_channels_for_gat_tcn[1], ...)\n\n        current_block_input_channels = self.hidden_dimension\n        for i, out_channels_block in enumerate(self.layer_channels_for_gat_tcn):\n            model_layers.append(\n                GAT_TCN_Block(\n                    in_channels=current_block_input_channels,\n                    out_channels=out_channels_block,\n                    num_heads_gat=self.num_heads_gat,\n                    temporal_kernel_size=9, # Can be configurable\n                    temporal_stride=self.temporal_strides[i], # Typically 1 for decoder\n                    dropout=self.dropout,\n                    A_is_dense=True\n                )\n            )\n            current_block_input_channels = out_channels_block\n        self.model_layers = model_layers\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None) -> torch.Tensor:\n        # X expected shape: (N_actual, hidden_dimension_encoder, T, V)\n        device = X.device\n        adj_matrix_on_device = self.processed_adj_matrix.to(device)\n        for layer in self.model_layers:\n            X = layer(X, adj_matrix_on_device)\n        return X # Output shape: (N_actual, self.output_dim, T_final, V)\n\n\nclass DecoderResiduals(Decoder): # This class might need more significant changes for GAT\n    def build_model(self) -> None:\n        super().build_model()\n        # This out layer might not be appropriate if GAT_TCN already outputs correct channels\n        # And it operates on T dimension, which GAT_TCN doesn't change if stride=1\n        # self.out = nn.Linear(self.n_frames, self.n_frames)\n        # If the GAT_TCN_Block's last layer outputs self.output_dim, this fc might not be needed.\n        # Or if it's a final pointwise linear projection on channels:\n        # last_gat_tcn_out_channels = self.layer_channels_for_gat_tcn[-1]\n        # self.final_projection = nn.Conv2d(last_gat_tcn_out_channels, self.output_dim, 1)\n        print(\"DecoderResiduals: Review the 'out' layer and residual connections compatibility with GAT_TCN_Block.\")\n\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor, residuals: List[torch.Tensor]) -> torch.Tensor:\n        # X shape: (N*M, hidden_dim_encoder, T, V)\n        # Residuals: List of [(N*M, C_res, T_res, V_res), ...] from Encoder\n        device = X.device\n        adj_matrix_on_device = self.processed_adj_matrix.to(device)\n\n        if len(residuals) != len(self.model_layers):\n            # This can happen if encoder/decoder layer counts differ or if residuals are structured differently\n            # For skip connections, usually encoder and decoder have symmetric number of blocks/stages\n            # For now, let's assume residuals align if counts are same.\n            # print(f\"Warning: Mismatch in number of residuals ({len(residuals)}) and decoder layers ({len(self.model_layers)}). Skipping residual connections.\")\n            pass # Fallback to simple decoder forward if residuals don't match\n\n        for i, layer in enumerate(self.model_layers):\n            X = layer(X, adj_matrix_on_device)\n            if residuals: # Check if residuals list is not empty\n                 # Ensure residuals are popped in correct order (reverse of encoder's storage)\n                 # And that shapes are compatible for addition.\n                 # GAT_TCN_Block preserves T,V dims if stride=1. Channels must match.\n                res_to_add = residuals.pop() # Pops from the end, which is encoder's earlier layers if stored sequentially\n                if X.shape == res_to_add.shape:\n                    X = X + res_to_add\n                else:\n                    # print(f\"Warning: Shape mismatch for residual connection. X: {X.shape}, Res: {res_to_add.shape}. Skipping this residual.\")\n                    # Potentially add a projection layer to res_to_add if channel dims differ\n                    pass\n\n\n        # The original self.out layer was:\n        # X = self.out(X.permute(0, 1, 3, 2).contiguous()).permute(0, 1, 3, 2).contiguous()\n        # This is a linear layer applied across the time dimension for each (batch, channel, joint).\n        # It might not be necessary if the GAT_TCN_Blocks handle channel transformations appropriately.\n        # If a final channel adjustment is needed, a 1x1 Conv2d is more common after GCNs.\n        # Example: if X.shape[-3] (channels) is not self.output_dim:\n        # X = self.final_projection(X) # if final_projection is defined in build_model\n\n        return X\n\n\nclass Denoiser(nn.Module):\n    # This Denoiser is a simple MLP and doesn't depend on graph structure,\n    # so it remains unchanged.\n    def __init__(self, input_size:int, hidden_sizes:List[int], cond_size:int=None, bias:bool=True, device:Union[str, torch.DeviceObjType]='cpu') -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.cond_size = cond_size\n        self.embedding_dim = cond_size if cond_size is not None else 128 # Default if no cond_size\n        self.bias = bias\n        self.device = device\n        self.build_model()\n        \n    def build_model(self) -> None:\n        self.net = nn.ModuleList()\n        self.cond_layers = nn.ModuleList() if self.cond_size is not None else None # conditioning for t\n        self.ext_cond_layers = nn.ModuleList() if self.cond_size is not None else None # conditioning for external cond\n\n        n_layers = len(self.hidden_sizes)\n        current_size = self.input_size\n        for idx, next_dim in enumerate(self.hidden_sizes):\n            layer = nn.Linear(current_size, next_dim, bias=self.bias)\n            if idx == n_layers-1: # Last layer\n                self.net.append(layer)\n            else:\n                self.net.append(nn.Sequential(layer,\n                                              nn.BatchNorm1d(next_dim), \n                                              nn.ReLU(inplace=True)))\n            if self.cond_size is not None:\n                 # For time embedding 't'\n                self.cond_layers.append(nn.Linear(self.embedding_dim, next_dim, bias=self.bias))\n                 # For external condition 'cond' (e.g. from STSAE)\n                self.ext_cond_layers.append(nn.Linear(self.cond_size, next_dim, bias=self.bias))\n\n            current_size = next_dim\n            \n    def pos_encoding(self, t:torch.Tensor, channels:int) -> torch.Tensor:\n        # t shape: (N, 1)\n        # channels: self.embedding_dim\n        inv_freq = 1.0 / (\n            10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n        ) # shape: (channels/2)\n        pos_enc_a = torch.sin(t * inv_freq) # (N, 1) * (channels/2) -> (N, channels/2)\n        pos_enc_b = torch.cos(t * inv_freq) # (N, 1) * (channels/2) -> (N, channels/2)\n        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1) # (N, channels)\n        return pos_enc\n\n    def forward(self, X:torch.Tensor, t:torch.Tensor, cond:torch.Tensor=None) -> torch.Tensor:\n        # X: (N_actual, flattened_features) e.g. (N*M, latent_dim_from_stsae)\n        # t: (N_actual, 1) or (N_actual) for time steps\n        # cond: (N_actual, cond_size) external condition (e.g. Z from STSAE)\n        \n        if t.ndim == 1:\n            t = t.unsqueeze(-1) # Ensure t is (N_actual, 1)\n        t_emb = self.pos_encoding(t, self.embedding_dim) # (N_actual, embedding_dim)\n\n        for i in range(len(self.net)):\n            X = self.net[i](X)\n            if self.cond_layers is not None: # Add time embedding\n                time_effect = self.cond_layers[i](t_emb)\n                X = X + time_effect\n            if cond is not None and self.ext_cond_layers is not None: # Add external condition\n                ext_cond_effect = self.ext_cond_layers[i](cond)\n                X = X + ext_cond_effect\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:41:16.886919Z","iopub.execute_input":"2025-06-03T01:41:16.887265Z","iopub.status.idle":"2025-06-03T01:41:16.900131Z","shell.execute_reply.started":"2025-06-03T01:41:16.887233Z","shell.execute_reply":"2025-06-03T01:41:16.899439Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/common/components.py\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/graph.py\nimport numpy as np\n\nclass Graph():\n    \"\"\" The Graph to model the skeletons of human body/hand\n\n    Args:\n        layout (str): must be one of 'ntu-rgb+d', 'h36m', 'coco', 'hands'\n            The layout of skeletons. ('ntu-rgb+d' by default)\n        strategy (str): must be one of 'uniform', 'distance', 'spatial', 'binary'\n            The strategy to generate adjacency matrix. ('uniform' by default)\n        max_hop (int): the maximal hop graph to consider. (1 by default)\n        dilation (int): the dilation of convolution operator. (1 by default)\n        normalize (bool): whether to normalize the adjacency matrix. (True by default)\n    \"\"\"\n\n    def __init__(self,\n                 layout='ntu-rgb+d',\n                 strategy='uniform',\n                 max_hop=1,\n                 dilation=1,\n                 normalize=True):\n        self.max_hop = max_hop\n        self.dilation = dilation\n        self.normalize = normalize\n        self.num_node, self.edge, self.center = self._get_layout_info(layout)\n        self.hop_dis = self._get_hop_distance(self.num_node, self.edge, max_hop=max_hop)\n        self.A = self._get_adjacency(strategy)\n\n    def __str__(self):\n        return self.A\n\n    def _get_layout_info(self, layout):\n        if layout == 'ntu-rgb+d':\n            num_node = 25\n            self_link = [(i, i) for i in range(num_node)]\n            inward_orange_idx = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n                               (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n                               (11, 10), (12, 11), (13, 1), (14, 13),\n                               (15, 14), (16, 15), (17, 1), (18, 17),\n                               (19, 18), (20, 19), (22, 23), (23, 8),\n                               (24, 25), (25, 12)]\n            inward = [(i - 1, j - 1) for (i, j) in inward_orange_idx]\n            outward = [(j, i) for (i, j) in inward]\n            edge = self_link + inward + outward\n            center = 21 - 1\n        elif layout == 'h36m':\n            num_node = 17\n            self_link = [(i, i) for i in range(num_node)]\n            # Spine, Neck, Head, HeadTopt\n            # Left: Shoulder, Arm, ForeArm, Hand\n            # Right: Shoulder, Arm, ForeArm, Hand\n            # Hip, Left: Thigh, Shin, Foot\n            # Hip, Right: Thigh, Shin, Foot\n            edge_human36m = [(0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6), (0, 7), (7, 8),\n                        (8, 9), (9, 10), (8, 11), (11, 12), (12, 13), (8, 14),\n                        (14, 15), (15, 16)]\n            edge = self_link + edge_human36m + [(j,i) for (i,j) in edge_human36m]\n            center = 0 # Hip\n        elif layout == 'coco':\n            num_node = 17 # Original COCO has 17 keypoints\n            self_link = [(i, i) for i in range(num_node)]\n            # Nose, Leye, Reye, Lear, Rear, Lshoulder, Rshoulder, Lelbow, Relbow, Lwrist, Rwrist, Lhip, Rhip, Lknee, Rknee, Lankle, Rankle\n            coco_edges = [\n                (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n                (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Arms\n                (11, 12), (5, 11), (6, 12),  # Torso\n                (11, 13), (13, 15), (12, 14), (14, 16)  # Legs\n            ]\n            edge = self_link + coco_edges + [(j,i) for (i,j) in coco_edges]\n            center = 0 # Nose, or an implicit center derived from hips/shoulders\n        elif layout == 'hands':\n            num_node = 21 # For one hand usually\n            self_link = [(i, i) for i in range(num_node)]\n            # Wrist, Thumb1, Thumb2, Thumb3, Thumb4, Index1,...Pinky4\n            hand_edges = [\n                (0,1), (1,2), (2,3), (3,4), #Thumb\n                (0,5), (5,6), (6,7), (7,8), #Index\n                (0,9), (9,10), (10,11), (11,12), #Middle\n                (0,13), (13,14), (14,15), (15,16), #Ring\n                (0,17), (17,18), (18,19), (19,20) #Pinky\n            ]\n            edge = self_link + hand_edges + [(j,i) for (i,j) in hand_edges]\n            center = 0 # Wrist\n        else:\n            raise ValueError(f\"Do Not Exist This Layout: {layout}\")\n        return num_node, edge, center\n\n    def _get_hop_distance(self, num_node, edge, max_hop=1):\n        A = np.zeros((num_node, num_node))\n        for i, j in edge:\n            A[j, i] = 1\n            A[i, j] = 1\n\n        # compute hop steps\n        hop_dis = np.zeros((num_node, num_node)) + np.inf\n        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n        arrive_mat = (np.stack(transfer_mat) > 0)\n        for d in range(max_hop, -1, -1):\n            hop_dis[arrive_mat[d]] = d\n        return hop_dis\n\n    def _get_adjacency(self, strategy):\n        valid_hop = range(0, self.max_hop + 1, self.dilation)\n        adjacency = np.zeros((self.num_node, self.num_node))\n        for hop in valid_hop:\n            adjacency[self.hop_dis == hop] = 1\n        normalize_adjacency = np.zeros((self.num_node, self.num_node))\n        if strategy == 'uniform':\n            for hop in valid_hop:\n                normalize_adjacency[self.hop_dis == hop] = 1\n            if self.normalize:\n                D = np.sum(normalize_adjacency, axis=0)\n                normalize_adjacency = normalize_adjacency / (D + 1e-6) # Add epsilon to avoid division by zero\n            A = normalize_adjacency\n        elif strategy == 'distance':\n            for hop in valid_hop:\n                normalize_adjacency[self.hop_dis == hop] = 1.0 / (hop + 1) # Closer hops have higher weight\n            if self.normalize:\n                D = np.sum(normalize_adjacency, axis=0)\n                normalize_adjacency = normalize_adjacency / (D + 1e-6)\n            A = normalize_adjacency\n        elif strategy == 'spatial':\n            A = []\n            for hop in valid_hop:\n                a_root = np.zeros((self.num_node, self.num_node))\n                a_close = np.zeros((self.num_node, self.num_node))\n                a_further = np.zeros((self.num_node, self.num_node))\n                for i in range(self.num_node):\n                    for j in range(self.num_node):\n                        if self.hop_dis[j, i] == hop:\n                            if self.hop_dis[j, self.center] == self.hop_dis[i, self.center]:\n                                a_root[j, i] = 1\n                            elif self.hop_dis[j, self.center] > self.hop_dis[i, self.center]:\n                                a_close[j, i] = 1\n                            else:\n                                a_further[j, i] = 1\n                if self.normalize:\n                    for a in [a_root, a_close, a_further]:\n                        D = np.sum(a, axis=0)\n                        a = a / (D + 1e-6)\n                A.append(a_root)\n                A.append(a_close)\n                A.append(a_further)\n            A = np.stack(A) # Results in (num_strategies, num_node, num_node)\n        elif strategy == 'binary': # Adjacency matrix with 0 or 1\n            A = np.zeros((self.num_node, self.num_node))\n            for i,j in self.edge: # only direct connections\n                if i !=j: # exclude self-loops from basic binary unless explicitly added\n                    A[i,j] = 1\n                    A[j,i] = 1\n            # if you want self-loops in binary:\n            # for i in range(self.num_node):\n            #   A[i,i] = 1\n            # No normalization for binary usually, but can be added if needed\n        else:\n            raise ValueError(\"Do Not Exist This Strategy\")\n\n        # If A is a list of matrices (e.g. for 'spatial' strategy),\n        # and the model expects a single matrix, you might need to select one or combine them.\n        # For GAT_TCN_Block expecting A_is_dense=True and a single Adjacency:\n        if isinstance(A, np.ndarray) and A.ndim == 3:\n            # print(f\"Graph strategy '{strategy}' produced multiple adjacency matrices ({A.shape[0]}). Using the first one.\")\n            A = A[0] # Take the first one (e.g., root connections for 'spatial') or sum/mean them.\n                     # The original ST-GCN could handle multiple A's by creating parallel GCN layers.\n                     # Our GAT_TCN_Block currently uses one A.\n        return A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T01:16:36.767767Z","iopub.execute_input":"2025-06-03T01:16:36.768541Z","iopub.status.idle":"2025-06-03T01:16:36.776323Z","shell.execute_reply.started":"2025-06-03T01:16:36.768504Z","shell.execute_reply":"2025-06-03T01:16:36.775399Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/DCMD-main/models/gcae/graph.py\n","output_type":"stream"}],"execution_count":14}]}