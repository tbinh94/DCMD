{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11715637,"sourceType":"datasetVersion","datasetId":7353969},{"sourceId":11760202,"sourceType":"datasetVersion","datasetId":7382794}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/guijiejie/DCMD-main.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:23:31.491923Z","iopub.execute_input":"2025-05-26T09:23:31.492519Z","iopub.status.idle":"2025-05-26T09:23:32.382164Z","shell.execute_reply.started":"2025-05-26T09:23:31.492496Z","shell.execute_reply":"2025-05-26T09:23:32.381479Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DCMD-main'...\nremote: Enumerating objects: 74, done.\u001b[K\nremote: Counting objects: 100% (74/74), done.\u001b[K\nremote: Compressing objects: 100% (64/64), done.\u001b[K\nremote: Total 74 (delta 22), reused 48 (delta 7), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (74/74), 262.60 KiB | 4.61 MiB/s, done.\nResolving deltas: 100% (22/22), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:22:29.867999Z","iopub.execute_input":"2025-05-07T09:22:29.868367Z","iopub.status.idle":"2025-05-07T09:22:29.991158Z","shell.execute_reply.started":"2025-05-07T09:22:29.868333Z","shell.execute_reply":"2025-05-07T09:22:29.990039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pip**","metadata":{}},{"cell_type":"code","source":"!pip install appdirs==1.4.4 docker-pycreds==0.4.0 gitdb==4.0.10 gitpython==3.1.32 joblib==1.3.1 numpy==1.25.2 pathtools==0.1.2 protobuf==4.23.4 scikit-learn==1.3.0 scipy==1.11.1 sentry-sdk==1.29.2 setproctitle==1.3.2 smmap==5.0.0 threadpoolctl==3.2.0 wandb==0.15.8\n!pip install --upgrade pytorch-lightning pyyaml wandb pandas numpy matplotlib matplotlib-inline scikit-learn tqdm\n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:23:40.101573Z","iopub.execute_input":"2025-05-26T09:23:40.101899Z","iopub.status.idle":"2025-05-26T09:28:15.416893Z","shell.execute_reply.started":"2025-05-26T09:23:40.101873Z","shell.execute_reply":"2025-05-26T09:28:15.415922Z"}},"outputs":[{"name":"stdout","text":"Collecting appdirs==1.4.4\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.11/dist-packages (0.4.0)\nCollecting gitdb==4.0.10\n  Downloading gitdb-4.0.10-py3-none-any.whl.metadata (1.1 kB)\nCollecting gitpython==3.1.32\n  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\nCollecting joblib==1.3.1\n  Downloading joblib-1.3.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting numpy==1.25.2\n  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting pathtools==0.1.2\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting protobuf==4.23.4\n  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\nCollecting scikit-learn==1.3.0\n  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting scipy==1.11.1\n  Downloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentry-sdk==1.29.2\n  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata (8.8 kB)\nCollecting setproctitle==1.3.2\n  Downloading setproctitle-1.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\nCollecting smmap==5.0.0\n  Downloading smmap-5.0.0-py3-none-any.whl.metadata (4.2 kB)\nCollecting threadpoolctl==3.2.0\n  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\nCollecting wandb==0.15.8\n  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds==0.4.0) (1.17.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from sentry-sdk==1.29.2) (2025.1.31)\nRequirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.11/dist-packages (from sentry-sdk==1.29.2) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (8.1.8)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (2.32.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (7.0.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (6.0.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.15.8) (75.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.8) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.15.8) (3.10)\nDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.3.1-py3-none-any.whl (301 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading setproctitle-1.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\nDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\nDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\nDownloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pathtools\n  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=ab5f9f85d4cb90d9bc5d01e1c1866071e546e3c980be1631bf3ee63aaa2a4fb9\n  Stored in directory: /root/.cache/pip/wheels/ea/b7/8b/84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\nSuccessfully built pathtools\nInstalling collected packages: pathtools, appdirs, threadpoolctl, smmap, setproctitle, sentry-sdk, protobuf, numpy, joblib, scipy, gitdb, scikit-learn, gitpython, wandb\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: smmap\n    Found existing installation: smmap 5.0.2\n    Uninstalling smmap-5.0.2:\n      Successfully uninstalled smmap-5.0.2\n  Attempting uninstall: setproctitle\n    Found existing installation: setproctitle 1.3.4\n    Uninstalling setproctitle-1.3.4:\n      Successfully uninstalled setproctitle-1.3.4\n  Attempting uninstall: sentry-sdk\n    Found existing installation: sentry-sdk 2.21.0\n    Uninstalling sentry-sdk-2.21.0:\n      Successfully uninstalled sentry-sdk-2.21.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.4.2\n    Uninstalling joblib-1.4.2:\n      Successfully uninstalled joblib-1.4.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n  Attempting uninstall: gitdb\n    Found existing installation: gitdb 4.0.12\n    Uninstalling gitdb-4.0.12:\n      Successfully uninstalled gitdb-4.0.12\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: gitpython\n    Found existing installation: GitPython 3.1.44\n    Uninstalling GitPython-3.1.44:\n      Successfully uninstalled GitPython-3.1.44\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.6\n    Uninstalling wandb-0.19.6:\n      Successfully uninstalled wandb-0.19.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.25.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.23.4 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.0 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.23.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\nscikit-image 0.25.1 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.0 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.25.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 gitdb-4.0.10 gitpython-3.1.32 joblib-1.3.1 numpy-1.25.2 pathtools-0.1.2 protobuf-4.23.4 scikit-learn-1.3.0 scipy-1.11.1 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 threadpoolctl-3.2.0 wandb-0.15.8\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\nCollecting pytorch-lightning\n  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.15.8)\nCollecting wandb\n  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.25.2)\nCollecting numpy\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nCollecting matplotlib\n  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (0.1.7)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting scikit-learn\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.1)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.32)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.23.4)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nCollecting sentry-sdk>=2.0.0 (from wandb)\n  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline) (5.7.1)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.16)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nCollecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.19.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nDownloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.6/341.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentry-sdk, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, wandb, scikit-learn, nvidia-cusolver-cu12, matplotlib, pytorch-lightning\n  Attempting uninstall: sentry-sdk\n    Found existing installation: sentry-sdk 1.29.2\n    Uninstalling sentry-sdk-1.29.2:\n      Successfully uninstalled sentry-sdk-1.29.2\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.25.2\n    Uninstalling numpy-1.25.2:\n      Successfully uninstalled numpy-1.25.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.8\n    Uninstalling wandb-0.15.8:\n      Successfully uninstalled wandb-0.15.8\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.3.0\n    Uninstalling scikit-learn-1.3.0:\n      Successfully uninstalled scikit-learn-1.3.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.5.1\n    Uninstalling pytorch-lightning-2.5.1:\n      Successfully uninstalled pytorch-lightning-2.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.3 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.1 requires scipy>=1.11.2, but you have scipy 1.11.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed matplotlib-3.10.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1.post0 scikit-learn-1.6.1 sentry-sdk-2.29.1 wandb-0.19.11\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.3.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu124\n    Uninstalling torchaudio-2.5.1+cu124:\n      Successfully uninstalled torchaudio-2.5.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Replace code to run properly**","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml \n### Experiment configuration\n\n## General settings\nsplit: 'train' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'train_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'best.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: true\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: false # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (his+pre)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding \n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:09.675156Z","iopub.execute_input":"2025-05-26T09:40:09.675458Z","iopub.status.idle":"2025-05-26T09:40:09.682155Z","shell.execute_reply.started":"2025-05-26T09:40:09.675424Z","shell.execute_reply":"2025-05-26T09:40:09.681496Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/transformer.py\nimport torch\nimport torch.nn.functional as F\nfrom torch import layer_norm, nn\nimport numpy as np\nfrom typing import List, Tuple, Union\n\nimport math\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef set_requires_grad(nets, requires_grad=False):\n    \"\"\"Set requies_grad for all the networks.\n\n    Args:\n        nets (nn.Module | list[nn.Module]): A list of networks or a single\n            network.\n        requires_grad (bool): Whether the networks require gradients or not\n    \"\"\"\n    if not isinstance(nets, list):\n        nets = [nets]\n    for net in nets:\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass StylizationBlock(nn.Module):\n\n    def __init__(self, latent_dim, time_embed_dim, dropout):\n        super().__init__()\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, 2 * latent_dim),\n        )\n        self.norm = nn.LayerNorm(latent_dim)\n        self.out_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Linear(latent_dim, latent_dim)),\n        )\n\n    def forward(self, h, emb):\n        \"\"\"\n        h: B, T, D\n        emb: B, D\n        \"\"\"\n        # B, 1, 2D\n        emb_out = self.emb_layers(emb).unsqueeze(1)\n        # scale: B, 1, D / shift: B, 1, D\n        scale, shift = torch.chunk(emb_out, 2, dim=2)\n        # B, T, D\n        h = self.norm(h) * (1 + scale) + shift\n        h = self.out_layers(h)\n        return h\n\n\nclass FFN(nn.Module):\n\n    def __init__(self, latent_dim, ffn_dim, dropout, time_embed_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(latent_dim, ffn_dim)\n        self.linear2 = zero_module(nn.Linear(ffn_dim, latent_dim))\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n\n    def forward(self, x, emb):\n        \"\"\"\n            x: B, T, D (D=latent_dim)\n        \"\"\"\n        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        y = x + self.proj_out(y, emb)\n        return y\n\n\nclass TemporalSelfAttention(nn.Module):\n\n    def __init__(self, n_frames, latent_dim, num_head, dropout, time_embed_dim, output_attention = True):\n        super().__init__()\n        self.num_head = num_head\n        self.output_attention = output_attention\n        self.norm = nn.LayerNorm(latent_dim)\n        self.query = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.key = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.value = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.sigma_projection = nn.Linear(latent_dim, num_head, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n        n_frames = n_frames\n        self.distances = torch.zeros((n_frames, n_frames)).cuda(0)\n\n        for i in range(n_frames):\n            for j in range(n_frames):\n                self.distances[i][j] = abs(i - j)\n\n    def forward(self, x, emb):\n        \"\"\"\n        x: B, T, D (D=latent_dim)\n        \"\"\"\n        B, T, D = x.shape\n        H = self.num_head\n\n        ## series-association\n        # B, T, 1, D\n        query = self.query(self.norm(x)).unsqueeze(2)\n        # B, 1, T, D\n        key = self.key(self.norm(x)).unsqueeze(1)\n        # B, T, H, D/H\n        query = query.view(B, T, H, -1)\n        key = key.view(B, T, H, -1)\n        scale = 1. / math.sqrt(D/H)\n        # B, H, T, T\n        scores = torch.einsum('bnhd,bmhd->bhnm', query, key) / math.sqrt(D // H)\n        attention = scale * scores\n        # B, H, T, T\n        series = self.dropout(F.softmax(attention, dim=-1))\n\n        ## prior-association\n        sigma = self.sigma_projection(x).view(B, T, H)  # B, T, H\n        sigma = sigma.transpose(1, 2)  # B T H ->  B H T\n        sigma = torch.sigmoid(sigma * 5) + 1e-5\n        sigma = torch.pow(3, sigma) - 1\n        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, T)  # B, H, T, T\n        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1) # B, H, T, T\n        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2)).cuda(0) # B, H, T, T\n\n        # B, T, H, D/H\n        value = self.value(self.norm(x)).view(B, T, H, -1)\n        # B, T, D\n        y = torch.einsum('bhnm,bmhd->bnhd', series, value).reshape(B, T, D)\n        y = x + self.proj_out(y, emb)\n\n        if self.output_attention:\n            return y.contiguous(), series, prior, sigma\n        else:\n            return y.contiguous(), None\n\nclass TemporalDiffusionTransformerDecoderLayer(nn.Module):\n\n    def __init__(self,\n                 n_frames = 7,\n                 latent_dim=16,\n                 time_embed_dim=16,\n                 ffn_dim=32,\n                 num_head=4,\n                 dropout=0.5\n                 ):\n        super().__init__()\n        self.sa_block = TemporalSelfAttention(\n            n_frames, latent_dim, num_head, dropout, time_embed_dim)\n        self.ffn = FFN(latent_dim, ffn_dim, dropout, time_embed_dim)\n\n    def forward(self, x, emb):\n        x, series, prior, sigma = self.sa_block(x, emb)\n        x = self.ffn(x, emb)\n        return x, series, prior, sigma\n\n\nclass MotionTransformer(nn.Module):\n    def __init__(self,\n                 input_feats,\n                 num_frames=7,\n                 latent_dim=16,\n                 ff_size=32,\n                 num_layers=8,\n                 num_heads=8,\n                 dropout=0.2,\n                 activation=\"gelu\",\n                 output_attention = True,\n                 device: Union[str, torch.DeviceObjType] = 'cpu',\n                 inject_condition: bool = False,\n                 **kargs):\n        super().__init__()\n\n\n        self.input_feats = input_feats # 34\n        self.num_frames = num_frames\n        self.latent_dim = latent_dim\n        self.ff_size = ff_size\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.activation = activation\n        self.output_attention = output_attention\n        self.device = device\n        self.time_embed_dim = latent_dim\n        self.inject_condition = inject_condition\n\n        self.build_model()\n\n    def build_model(self):\n        self.sequence_embedding = nn.Parameter(torch.randn(self.num_frames, self.latent_dim))\n\n        # Input Embedding\n        self.joint_embed = nn.Linear(self.input_feats, self.latent_dim)\n        self.cond_embed = nn.Linear(256, self.time_embed_dim)\n\n        self.time_embed = nn.Sequential(\n            nn.Linear(self.latent_dim, self.time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(self.time_embed_dim, self.time_embed_dim),\n        )\n\n        self.temporal_decoder_blocks = nn.ModuleList()\n        for i in range(self.num_layers):\n            self.temporal_decoder_blocks.append(\n                TemporalDiffusionTransformerDecoderLayer(\n                    n_frames=self.num_frames,\n                    latent_dim=self.latent_dim,\n                    time_embed_dim=self.time_embed_dim,\n                    ffn_dim=self.ff_size,\n                    num_head=self.num_heads,\n                    dropout=self.dropout,\n                )\n            )\n        # Output Module\n        self.out = zero_module(nn.Linear(self.latent_dim, self.input_feats))\n\n\n    def forward(self, x, timesteps, condition_data:torch.Tensor=None):\n        \"\"\"\n        x: B, T, D (D=C*V)\n        \"\"\"\n        B, T = x.shape[0], x.shape[1]\n\n        # B, latent_dim\n        emb = self.time_embed(timestep_embedding(timesteps, self.latent_dim))\n\n        # Add conditioning signal\n        if self.inject_condition:\n            condition_data = self.cond_embed(condition_data)\n            emb = emb + condition_data\n\n        # B, T, latent_dim\n        h = self.joint_embed(x)\n        h = h + self.sequence_embedding.unsqueeze(0)[:, :T, :]\n        \n        i = 0\n        prelist = []\n        series_list = []\n        prior_list = []\n        sigma_list = []\n        for module in self.temporal_decoder_blocks:\n            if i < (self.num_layers // 2):\n                prelist.append(h)\n                h, series, prior, sigmas = module(h, emb) # B, T, latent_dim\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n            elif i >= (self.num_layers // 2):\n                h, series, prior, sigmas = module(h, emb)\n                h += prelist[-1]\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n                prelist.pop()\n            i += 1\n\n        # B, T, C*V\n        output = self.out(h).view(B, T, -1).contiguous()\n        if self.output_attention:\n            return output, series_list, prior_list, sigma_list\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:13.768655Z","iopub.execute_input":"2025-05-26T09:40:13.769426Z","iopub.status.idle":"2025-05-26T09:40:13.777463Z","shell.execute_reply.started":"2025-05-26T09:40:13.769401Z","shell.execute_reply":"2025-05-26T09:40:13.776953Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/transformer.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/train_DCMD.py\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport yaml\nfrom models.dcmd import DCMD\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.strategies import DDPStrategy\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\nfrom utils.ema import EMACallback\n\n\nif __name__== '__main__':\n\n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='Pose_AD_Experiment')\n    parser.add_argument('-c', '--config', type=str, required=True,\n                        default='/your_default_config_file_path')\n    \n    args = parser.parse_args()\n    config_path = args.config\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args) \n    # Save config file to ckpt_dir\n    os.system(f'cp {config_path} {os.path.join(args.ckpt_dir, \"config.yaml\")}')     \n    \n    # Set seeds    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed) \n    pl.seed_everything(args.seed)\n\n    # Set callbacks and logger\n    if (hasattr(args, 'diffusion_on_latent') and args.stage == 'pretrain'):\n        monitored_metric = 'pretrain_rec_loss'\n        metric_mode = 'min'\n    elif args.validation:\n        monitored_metric = 'AUC'\n        metric_mode = 'max'\n    else:\n        monitored_metric = 'loss'\n        metric_mode = 'min'\n    callbacks = [ModelCheckpoint(\n                    dirpath=args.ckpt_dir,\n                    save_top_k=2,\n                    save_last=True,\n                    monitor=monitored_metric,\n                    mode=metric_mode\n                )]\n\n    \n    callbacks += [EMACallback()] if args.use_ema else [] # Use to achieve exponential moving average\n    \n    if args.use_wandb:\n        callbacks += [LearningRateMonitor(logging_interval='step')]\n        wandb_logger = WandbLogger(project=args.project_name, group=args.group_name, entity=args.wandb_entity, \n                                   name=args.dir_name, config=vars(args), log_model='all')\n    else:\n        wandb_logger = False\n\n    # Get dataset and loaders\n    _, train_loader, _, val_loader = get_dataset_and_loader(args, split=args.split, validation=args.validation)\n    \n    # Initialize model and trainer\n    model = DCMD(args)\n    \n    trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices, default_root_dir=args.ckpt_dir, max_epochs=args.n_epochs, \n                         logger=wandb_logger, callbacks=callbacks, strategy=DDPStrategy(find_unused_parameters=False),\n                         log_every_n_steps=20, num_sanity_val_steps=0, deterministic=True)\n    \n    # Train the model    \n    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:19.645617Z","iopub.execute_input":"2025-05-26T09:40:19.645890Z","iopub.status.idle":"2025-05-26T09:40:19.651545Z","shell.execute_reply.started":"2025-05-26T09:40:19.645871Z","shell.execute_reply":"2025-05-26T09:40:19.650855Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/train_DCMD.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml\n### Experiment configuration\n\n## General settings\nsplit: 'test' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'test_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'last.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: false # if true, create a new directory for the current experiment\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: true # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (cond+noised)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding\n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:22.541825Z","iopub.execute_input":"2025-05-26T09:40:22.542501Z","iopub.status.idle":"2025-05-26T09:40:22.548589Z","shell.execute_reply.started":"2025-05-26T09:40:22.542478Z","shell.execute_reply":"2025-05-26T09:40:22.547948Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/dcmd.py\nimport argparse\nimport os\nfrom math import prod\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\n\nfrom models.stsae.stsae import STSAE\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nfrom utils.diffusion_utils import Diffusion\nfrom utils.eval_utils import (compute_var_matrix, filter_vectors_by_cond,\n                              get_avenue_mask, get_hr_ubnormal_mask, pad_scores, score_process)\nfrom utils.model_utils import processing_data, my_kl_loss\nfrom models.transformer import MotionTransformer\nfrom utils.tools import get_dct_matrix, generate_pad, padding_traj\n\n\nclass DCMD(pl.LightningModule):\n\n    losses = {'l1': nn.L1Loss, 'smooth_l1': nn.SmoothL1Loss, 'mse': nn.MSELoss}\n    conditioning_strategies = {'inject': 'inject'}\n\n    def __init__(self, args: argparse.Namespace) -> None:\n        \"\"\"\n        This class implements DCMD model.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n        \"\"\"\n\n        super(DCMD, self).__init__()\n\n        ## Log the hyperparameters of the model\n        self.save_hyperparameters(args)\n\n        ## Set the internal variables of the model\n        # Data parameters\n        self.n_frames = args.seg_len\n        self.num_coords = args.num_coords\n        self.n_joints = self._infer_number_of_joint(args)\n\n        ## Model parameters\n        # Main network\n        self.dropout = args.dropout\n        self.conditioning_strategy = self.conditioning_strategies[args.conditioning_strategy]\n        # Conditioning network\n        self.cond_h_dim = args.h_dim\n        self.cond_latent_dim = args.latent_dim\n        self.cond_channels = args.channels\n        self.cond_dropout = args.dropout\n\n        ## Training and inference parameters\n        self.learning_rate = args.opt_lr\n        self.loss_fn = self.losses[args.loss_fn](reduction='none')\n        self.noise_steps = args.noise_steps\n        self.aggregation_strategy = args.aggregation_strategy\n        self.n_generated_samples = args.n_generated_samples\n        self.model_return_value = args.model_return_value\n        self.gt_path = args.gt_path\n        self.split = args.split\n        self.use_hr = args.use_hr\n        self.ckpt_dir = args.ckpt_dir\n        self.save_tensors = args.save_tensors\n        self.num_transforms = args.num_transform\n        self.anomaly_score_pad_size = args.pad_size\n        self.anomaly_score_filter_kernel_size = args.filter_kernel_size\n        self.anomaly_score_frames_shift = args.frames_shift\n        self.dataset_name = args.dataset_choice\n\n        # New parameters\n        self.n_his = args.n_his\n        self.padding = args.padding\n        self.num_layers = args.num_layers\n        self.num_heads = args.num_heads\n        self.latent_dims = args.latent_dims\n        self.automatic_optimization = False\n        self.loss_1_series_weight = args.loss_1_series_weight\n        self.loss_1_prior_weight = args.loss_1_prior_weight\n        self.loss_2_series_weight = args.loss_2_series_weight\n        self.loss_2_prior_weight = args.loss_2_prior_weight\n        self.idx_pad, self.zero_index = generate_pad(self.padding, self.n_his, self.n_frames-self.n_his)\n\n        ## Set the noise scheduler for the diffusion process\n        self._set_diffusion_variables()\n\n        ## Build the model\n        self.build_model()\n\n\n    def build_model(self) -> None:\n        \"\"\"\n        Build the model according to the specified hyperparameters.\n        \"\"\"\n\n        # Prediction Model\n        pre_model = MotionTransformer(\n            input_feats=2 * self.n_joints,\n            num_frames=self.n_frames,\n            num_layers=self.num_layers,\n            num_heads=self.num_heads,\n            latent_dim=self.latent_dims,\n            dropout=self.dropout,\n            device=self.device,\n            inject_condition=(self.conditioning_strategy == 'inject')\n        )\n\n        # Reconstruction Model\n        rec_model = STSAE(\n            c_in=self.num_coords,\n            h_dim=self.cond_h_dim,\n            latent_dim=self.cond_latent_dim,\n            n_frames=self.n_his,\n            dropout=self.cond_dropout,\n            n_joints=self.n_joints,\n            layer_channels=self.cond_channels,\n            device=self.device)\n\n        self.pre_model, self.rec_model = pre_model, rec_model\n\n\n    def forward(self, input_data: List[torch.Tensor], aggr_strategy: str = None, return_: str = None) -> List[torch.Tensor]:\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n\n        ## Unpack data: tensor_data is the input data, meta_out is a list of metadata\n        tensor_data, meta_out = self._unpack_data(input_data)\n        B = tensor_data.shape[0]\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :]\n        x_0 = padding_traj(history_data, self.padding, self.idx_pad, self.zero_index)\n\n        generated_xs = []\n        # Generate m future predictions\n        for _ in range(self.n_generated_samples):\n\n            ## Reconstruction —— AE model\n            condition_embedding, rec_his_data = self.rec_model(history_data)\n\n            ## Prediction —— diffusion model\n            ## DCT transformation\n            dct_m, idct_m = get_dct_matrix(self.n_frames)\n            dct_m_all = dct_m.float().to(self.device)\n            idct_m_all = idct_m.float().to(self.device)\n            # (B, C, T, V) -> (B, T, V, C)\n            x = x_0.permute(0, 2, 3, 1).contiguous()\n            # (B, T, V, C) -> (B, T, C*V)\n            x = x.reshape([x.shape[0], self.n_frames, -1])\n            y = torch.matmul(dct_m_all, x)  # [B, T, C*V]\n\n            ## Generate gaussian noise of the same shape as the y\n            y_d = torch.randn_like(y, device=self.device)\n\n            ## (t ∈ T, T-1, ..., 1)\n            for i in reversed(range(1, self.noise_steps)):\n\n                ### Prediction (Two branches)\n                ## Set the time step\n                t = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev[0] = 0\n\n                ## Generate gaussian noise of the same shape as the predicted noise\n                noise_pre = torch.randn_like(y_d, device=self.device) if i > 1 else torch.zeros_like(y_d, device=self.device)\n\n                ## First branch\n                # Predict the noise\n                predicted_noise_pre, series, prior, _ = self.pre_model(y_d, t, condition_data=condition_embedding)\n                # Get the alpha and beta values and expand them to the shape of the predicted noise\n                alpha_pre = self._alpha[t][:, None, None]\n                alpha_hat_pre = self._alpha_hat[t][:, None, None]\n                beta_pre = self._beta[t][:, None, None]\n                # Recover the predicted sequence\n                y_d = (1 / torch.sqrt(alpha_pre)) * (y_d - ((1 - alpha_pre) / (torch.sqrt(1 - alpha_hat_pre))) * predicted_noise_pre) \\\n                    + torch.sqrt(beta_pre) * noise_pre\n                ## Second branch\n                alpha_hat_prev = self._alpha_hat[t_prev][:, None, None]\n                # Add noise\n                y_n = (torch.sqrt(alpha_hat_prev) * y) + (torch.sqrt(1 - alpha_hat_prev) * noise_pre)\n                ## Mask completion\n                # Get M values\n                mask = torch.zeros_like(x, device=self.device) # [batch, T, C*V]\n                for m in range(0, self.n_his):\n                    mask[:, m, :] = 1\n                # iDCT transformation\n                y_d_idct = torch.matmul(idct_m_all, y_d)\n                y_n_idct = torch.matmul(idct_m_all, y_n)\n                # mask-mul\n                m_mul_y_n = torch.mul(mask, y_n_idct)\n                m_mul_y_d = torch.mul((1-mask), y_d_idct)\n                # together\n                m_y = m_mul_y_d + m_mul_y_n\n                # DCT again\n                y_d = torch.matmul(dct_m_all, m_y)\n\n            # iDCT\n            pre_future_data = torch.matmul(idct_m_all, y_d)\n            # (B, T, C*V) -> (B, T, V, C)\n            pre_future_data = pre_future_data.reshape(pre_future_data.shape[0], pre_future_data.shape[1], -1, 2)\n            # (B, T, V, C) -> (B, C, T, V)\n            pre_future_data = pre_future_data.permute(0, 3, 1, 2).contiguous()\n            # select future sequences\n            pre_future_data = pre_future_data[:,:,self.n_his:,:]\n\n            ## Reconstruction + Prediction\n            xs = torch.cat((rec_his_data, pre_future_data), dim=2)\n\n            generated_xs.append(xs)\n\n        selected_x, loss_of_selected_x = self._aggregation_strategy(generated_xs, tensor_data, aggr_strategy)\n\n        return self._pack_out_data(selected_x, loss_of_selected_x, [tensor_data] + meta_out, return_=return_)\n\n\n    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.float32:\n        \"\"\"\n        Training step of the model.\n        \"\"\"\n\n        ## Get the optimizer returned in configuration_optimizers()\n        opt = self.optimizers()\n\n        ## Unpack data: tensor_data is the input data\n        tensor_data, _ = self._unpack_data(batch)\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :] # Used for rec (first n_his)\n        x_0 = tensor_data # Used for pre（all）\n\n        ## Reconstruction\n        # Encode the history data\n        condition_embedding, rec_his_data = self.rec_model(history_data)\n        # Compute the rec_loss\n        rec_loss = torch.mean(self.loss_fn(rec_his_data, history_data))\n        self.log('rec_loss', rec_loss)\n\n        ## Prediction\n        # DCT transformation\n        dct_m, _ = get_dct_matrix(self.n_frames)\n        dct_m_all = dct_m.float().to(self.device)\n        # (B, C, T, V) -> (B, T, V, C)\n        x = x_0.permute(0, 2, 3, 1).contiguous()\n        # (B, T, V, C) -> (B, T, C*V)\n        x = x.reshape([x.shape[0], self.n_frames, -1]) # [batch, T, C*V]\n        y_0 = torch.matmul(dct_m_all, x)\n\n        # Sample the time steps and corrupt the data\n        t = self.noise_scheduler.sample_timesteps(y_0.shape[0]).to(self.device)\n        y_t, pre_noise = self.noise_scheduler.noise_motion(y_0, t) # (B, T, C*(V-1))\n\n        # Predict the noise\n        pre_predicted_noise, series, prior, _ = self.pre_model(y_t, t, condition_data=condition_embedding)\n\n        # Compute the pre_loss\n        # Calculate Association discrepancy\n        series_loss = 0.0\n        prior_loss = 0.0\n        for u in range(len(prior)):\n            # Pdetach, S <-> Maximize\n            series_loss += \\\n                (torch.mean(my_kl_loss(\n                    series[u],\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach()))\n                + torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach(),\n                    series[u])))\n            # P, Sdetach <-> Minimize\n            prior_loss += \\\n                (torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)),\n                    series[u].detach()))\n                + torch.mean(my_kl_loss(\n                    series[u].detach(),\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)))))\n        series_loss = series_loss / len(prior)\n        prior_loss = prior_loss / len(prior)\n\n        pre_loss = torch.mean(self.loss_fn(pre_predicted_noise, pre_noise))\n        self.log('pre_loss', pre_loss)\n\n        ## Compute loss1 & loss2\n        loss1 = rec_loss + pre_loss \\\n                - self.loss_1_series_weight * series_loss \\\n                + self.loss_1_prior_weight * prior_loss\n        self.log('loss1', loss1)\n        loss2 = rec_loss + pre_loss \\\n                + self.loss_2_prior_weight * prior_loss \\\n                + self.loss_2_series_weight * series_loss\n        self.log('loss2', loss2)\n\n        ## Minimax strategy\n        self.manual_backward(loss1, retain_graph=True)\n        self.manual_backward(loss2)\n        opt.step()\n        opt.zero_grad()\n\n\n    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Test step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._test_output_list.append(self.forward(batch))\n        return\n\n\n    def on_test_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_test_epoch_start()\n        self._test_output_list = []\n        return\n\n\n    def on_test_epoch_end(self) -> float:\n        \"\"\"\n        Test epoch end of the model.\n\n        Returns:\n            float: test auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._test_output_list)\n        del self._test_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score)\n        return auc_score\n\n\n    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Validation step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._validation_output_list.append(self.forward(batch))\n        return\n\n\n    def on_validation_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_validation_epoch_start()\n        self._validation_output_list = []\n        return\n\n\n    def on_validation_epoch_end(self) -> float:\n        \"\"\"\n        Validation epoch end of the model.\n\n        Returns:\n            float: validation auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._validation_output_list)\n        del self._validation_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score, sync_dist=True)\n        return auc_score\n\n\n    def configure_optimizers(self) -> Dict:\n        \"\"\"\n        Configure the optimizers and the learning rate schedulers.\n\n        Returns:\n            Dict: dictionary containing the optimizers, the learning rate schedulers and the metric to monitor\n        \"\"\"\n\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, last_epoch=-1)\n\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'AUC'}\n\n\n    def post_processing(self, out: np.ndarray, gt_data: np.ndarray, trans: np.ndarray, meta: np.ndarray, frames: np.ndarray) -> float:\n        \"\"\"\n        Post processing of the model.\n\n        Args:\n            out (np.ndarray): output of the model\n            gt_data (np.ndarray): ground truth data\n            trans (np.ndarray): transformation index\n            meta (np.ndarray): metadata\n            frames (np.ndarray): frame indexes of the data\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        all_gts = [file_name for file_name in os.listdir(self.gt_path) if file_name.endswith('.npy')]\n        all_gts = sorted(all_gts)\n        scene_clips = [(int(fn.split('_')[0]), int(fn.split('_')[1].split('.')[0])) for fn in all_gts]\n        hr_ubnormal_masked_clips = get_hr_ubnormal_mask(self.split) if (self.use_hr and (self.dataset_name == 'UBnormal')) else {}\n        hr_avenue_masked_clips = get_avenue_mask() if self.dataset_name == 'HR-Avenue' else {}\n\n        num_transform = self.num_transforms\n        model_scores_transf = {}\n        dataset_gt_transf = {}\n\n        for transformation in tqdm(range(num_transform)):\n            # iterating over each transformation T\n\n            dataset_gt = []\n            model_scores = []\n            cond_transform = (trans == transformation)\n\n            out_transform, gt_data_transform, meta_transform, frames_transform = filter_vectors_by_cond([out, gt_data, meta, frames], cond_transform)\n\n            for idx in range(len(all_gts)):\n                # iterating over each clip C with transformation T\n\n                scene_idx, clip_idx = scene_clips[idx]\n\n                gt = np.load(os.path.join(self.gt_path, all_gts[idx]))\n                n_frames = gt.shape[0]\n\n                cond_scene_clip = (meta_transform[:, 0] == scene_idx) & (meta_transform[:, 1] == clip_idx)\n                out_scene_clip, gt_scene_clip, meta_scene_clip, frames_scene_clip = filter_vectors_by_cond([out_transform, gt_data_transform,\n                                                                                                            meta_transform, frames_transform],\n                                                                                                           cond_scene_clip)\n\n                figs_ids = sorted(list(set(meta_scene_clip[:, 2])))\n                error_per_person = []\n                error_per_person_max_loss = []\n\n                for fig in figs_ids:\n                    # iterating over each actor A in each clip C with transformation T\n\n                    cond_fig = (meta_scene_clip[:, 2] == fig)\n                    out_fig, _, frames_fig = filter_vectors_by_cond([out_scene_clip, gt_scene_clip, frames_scene_clip], cond_fig)\n                    loss_matrix = compute_var_matrix(out_fig, frames_fig, n_frames)\n                    fig_reconstruction_loss = np.nanmax(loss_matrix, axis=0)\n                    if self.anomaly_score_pad_size != -1:\n                        fig_reconstruction_loss = pad_scores(fig_reconstruction_loss, gt, self.anomaly_score_pad_size)\n\n                    error_per_person.append(fig_reconstruction_loss)\n                    error_per_person_max_loss.append(max(fig_reconstruction_loss))\n\n                clip_score = np.stack(error_per_person, axis=0)\n                clip_score_log = np.log1p(clip_score)\n                clip_score = np.mean(clip_score, axis=0) + (np.amax(clip_score_log, axis=0)-np.amin(clip_score_log, axis=0))\n\n                # removing the non-HR frames for UBnormal dataset\n                if (scene_idx, clip_idx) in hr_ubnormal_masked_clips:\n                    clip_score = clip_score[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n                    gt = gt[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n\n                # removing the non-HR frames for Avenue dataset\n                if clip_idx in hr_avenue_masked_clips:\n                    clip_score = clip_score[np.array(hr_avenue_masked_clips[clip_idx])==1]\n                    gt = gt[np.array(hr_avenue_masked_clips[clip_idx])==1]\n\n                # Abnormal score per frame\n                clip_score = score_process(clip_score, self.anomaly_score_frames_shift, self.anomaly_score_filter_kernel_size)\n                model_scores.append(clip_score)\n\n                dataset_gt.append(gt)\n\n            model_scores = np.concatenate(model_scores, axis=0)\n\n            dataset_gt = np.concatenate(dataset_gt, axis=0)\n\n            model_scores_transf[transformation] = model_scores\n            dataset_gt_transf[transformation] = dataset_gt\n\n        # aggregating the anomaly scores for all transformations\n        pds = np.mean(np.stack(list(model_scores_transf.values()), 0), 0)\n        gt = dataset_gt_transf[0]\n\n        # computing the AUC\n        auc = roc_auc_score(gt, pds)\n\n        return auc\n\n\n    def test_on_saved_tensors(self, split_name: str) -> float:\n        \"\"\"\n        Skip the prediction step and test the model on the saved tensors.\n\n        Args:\n            split_name (str): split name (val, test)\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        tensors = self._load_tensors(split_name, self.aggregation_strategy, self.n_generated_samples)\n        auc_score = self.post_processing(tensors['prediction'], tensors['gt_data'], tensors['trans'],\n                                         tensors['metadata'], tensors['frames'])\n        print(f'AUC score: {auc_score:.6f}')\n        return auc_score\n\n\n    ## Helper functions\n\n    def _aggregation_strategy(self, generated_xs: List[torch.Tensor], input_sequence: torch.Tensor, aggr_strategy: str) -> Tuple[torch.Tensor]:\n        \"\"\"\n        Aggregates the generated samples and returns the selected one and its reconstruction error.\n        Strategies:\n            - all: returns all the generated samples\n            - random: returns a random sample\n            - best: returns the sample with the lowest reconstruction loss\n            - worst: returns the sample with the highest reconstruction loss\n            - mean: returns the mean of the losses of the generated samples\n            - median: returns the median of the losses of the generated samples\n            - mean_pose: returns the mean of the generated samples\n            - median_pose: returns the median of the generated samples\n\n        Args:\n            generated_xs (List[torch.Tensor]): list of generated samples\n            input_sequence (torch.Tensor): ground truth sequence\n            aggr_strategy (str): aggregation strategy\n\n        Raises:\n            ValueError: if the aggregation strategy is not valid\n\n        Returns:\n            Tuple[torch.Tensor]: selected sample and its reconstruction error\n        \"\"\"\n\n        aggr_strategy = self.aggregation_strategy if aggr_strategy is None else aggr_strategy\n        if aggr_strategy == 'random':\n            return generated_xs[np.random.randint(len(generated_xs))], None # Added None as it was missing\n\n        B, repr_shape = input_sequence.shape[0], input_sequence.shape[1:]\n        compute_loss = lambda x: torch.mean(self.loss_fn(x, input_sequence).reshape(-1, prod(repr_shape)), dim=-1)\n        losses = [compute_loss(x) for x in generated_xs]\n\n        if aggr_strategy == 'all':\n            dims_idxs = list(range(2, len(repr_shape)+2))\n            dims_idxs = [1, 0] + dims_idxs\n            selected_x = torch.stack(generated_xs).permute(*dims_idxs)\n            loss_of_selected_x = torch.stack(losses).permute(1, 0)\n        elif aggr_strategy == 'mean':\n            selected_x = None\n            loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n        elif aggr_strategy == 'mean_pose':\n            selected_x = torch.mean(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'median':\n            loss_of_selected_x, _ = torch.median(torch.stack(losses), dim=0)\n            selected_x = None\n        elif aggr_strategy == 'median_pose':\n            selected_x, _ = torch.median(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'best' or aggr_strategy == 'worst':\n            strategy = (lambda x, y: x < y) if aggr_strategy == 'best' else (lambda x, y: x > y)\n            loss_of_selected_x = torch.full((B,), fill_value=(1e10 if aggr_strategy == 'best' else -1.), device=self.device)\n            selected_x = torch.zeros((B, *repr_shape)).to(self.device)\n\n            for i in range(len(generated_xs)):\n                mask = strategy(losses[i], loss_of_selected_x)\n                loss_of_selected_x[mask] = losses[i][mask]\n                selected_x[mask] = generated_xs[i][mask]\n        elif 'quantile' in aggr_strategy:\n            q = float(aggr_strategy.split(':')[-1])\n            loss_of_selected_x = torch.quantile(torch.stack(losses), q, dim=0)\n            selected_x = None\n        else:\n            raise ValueError(f'Unknown aggregation strategy {aggr_strategy}')\n\n        # Ensuring selected_x and loss_of_selected_x are always returned\n        if selected_x is None and loss_of_selected_x is None:\n             # Default to mean loss if strategy doesn't return both\n             loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n\n\n        return selected_x, loss_of_selected_x\n\n\n    def _infer_number_of_joint(self, args: argparse.Namespace) -> int:\n        \"\"\"\n        Infer the number of joints based on the dataset parameters.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n\n        Returns:\n            int: number of joints\n        \"\"\"\n\n        if args.headless:\n            joints_to_consider = 14\n        elif args.kp18_format:\n            joints_to_consider = 18\n        else:\n            joints_to_consider = 17\n        return joints_to_consider\n\n\n    def _load_tensors(self, split_name: str, aggr_strategy: str, n_gen: int) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Loads the tensors from the experiment directory.\n\n        Args:\n            split_name (str): name of the split (train, val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n\n        Returns:\n            Dict[str, torch.Tensor]: dictionary containing the tensors. The keys are inferred from the file names.\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        tensor_files = os.listdir(path)\n        tensors = {}\n        for t_file in tensor_files:\n            t_name = t_file.split('.')[0]\n            tensors[t_name] = torch.load(os.path.join(path, t_file))\n        return tensors\n\n\n    def _pack_out_data(self, selected_x: torch.Tensor, loss_of_selected_x: torch.Tensor, additional_out: List[torch.Tensor], return_: str) -> List[torch.Tensor]:\n        \"\"\"\n        Packs the output data according to the return_ argument.\n\n        Args:\n            selected_x (torch.Tensor): generated samples selected among the others according to the aggregation strategy\n            loss_of_selected_x (torch.Tensor): loss of the selected samples\n            additional_out (List[torch.Tensor]): additional output data (ground truth, meta-data, etc.)\n            return_ (str): return strategy. Can be 'pose', 'loss', 'all'\n\n        Raises:\n            ValueError: if return_ is None and self.model_return_value is None\n\n        Returns:\n            List[torch.Tensor]: output data\n        \"\"\"\n\n        if return_ is None:\n            if self.model_return_value is None:\n                raise ValueError('Either return_ or self.model_return_value must be set')\n            else:\n                return_ = self.model_return_value\n\n        if return_ == 'poses':\n            out = [selected_x]\n        elif return_ == 'loss':\n            out = [loss_of_selected_x]\n        elif return_ == 'all':\n            # Check if both are available before adding to the list\n            out = []\n            if loss_of_selected_x is not None:\n                out.append(loss_of_selected_x)\n            if selected_x is not None:\n                 out.append(selected_x)\n\n        return out + additional_out\n\n\n    def _save_tensors(self, tensors: Dict[str, torch.Tensor], split_name: str, aggr_strategy: str, n_gen: int) -> None:\n        \"\"\"\n        Saves the tensors in the experiment directory.\n\n        Args:\n            tensors (Dict[str, torch.Tensor]): tensors to save\n            split_name (str): name of the split (val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        for t_name, tensor in tensors.items():\n            torch.save(tensor, os.path.join(path, t_name + '.pt'))\n\n\n    def _set_diffusion_variables(self) -> None:\n        \"\"\"\n        Sets the diffusion variables.\n        \"\"\"\n\n        self.noise_scheduler = Diffusion(noise_steps=self.noise_steps, n_joints=self.n_joints,\n                                         device=self.device, time=self.n_frames)\n        self._beta_ = self.noise_scheduler.schedule_noise()\n        self._alpha_ = (1. - self._beta_)\n        self._alpha_hat_ = torch.cumprod(self._alpha_, dim=0)\n\n    def _unpack_data(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        Unpacks the data.\n\n        Args:\n            x (torch.Tensor): list containing the input data, the transformation index, the metadata and the actual frames.\n\n        Returns:\n            Tuple[torch.Tensor, List[torch.Tensor]]: input data, list containing the transformation index, the metadata and the actual frames.\n        \"\"\"\n        tensor_data = x[0].to(self.device)\n        transformation_idx = x[1]\n        metadata = x[2]\n        actual_frames = x[3]\n        meta_out = [transformation_idx, metadata, actual_frames]\n        return tensor_data, meta_out\n\n\n    @property\n    def _beta(self) -> torch.Tensor:\n        return self._beta_.to(self.device)\n\n\n    @property\n    def _alpha(self) -> torch.Tensor:\n        return self._alpha_.to(self.device)\n\n\n    @property\n    def _alpha_hat(self) -> torch.Tensor:\n        return self._alpha_hat_.to(self.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:26.132401Z","iopub.execute_input":"2025-05-26T09:40:26.132912Z","iopub.status.idle":"2025-05-26T09:40:26.149937Z","shell.execute_reply.started":"2025-05-26T09:40:26.132888Z","shell.execute_reply":"2025-05-26T09:40:26.149174Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/dcmd.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/eval_DCMD.py\nimport argparse\nimport os\n\nimport pytorch_lightning as pl\nimport yaml\nfrom models.dcmd import DCMD\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\n\n\n\nif __name__== '__main__':\n    \n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='DCMD')\n    parser.add_argument('-c', '--config', type=str, required=True)\n    args = parser.parse_args()\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args)\n\n    # Initialize the model\n    model = DCMD(args)\n    \n    if args.load_tensors:\n        # Load tensors and test\n        model.test_on_saved_tensors(split_name=args.split)\n    else:\n        # Load test data\n        print('Loading data and creating loaders.....')\n        ckpt_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/last.ckpt'\n        dataset, loader, _, _ = get_dataset_and_loader(args, split=args.split)\n        \n        # Initialize trainer and test\n        trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices[:1],\n                             default_root_dir=args.ckpt_dir, max_epochs=1, logger=False)\n        out = trainer.test(model, dataloaders=loader, ckpt_path=ckpt_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:38.829583Z","iopub.execute_input":"2025-05-26T09:40:38.829907Z","iopub.status.idle":"2025-05-26T09:40:38.835147Z","shell.execute_reply.started":"2025-05-26T09:40:38.829883Z","shell.execute_reply":"2025-05-26T09:40:38.834454Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/eval_DCMD.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/utils/get_robust_data.py\nimport os\nimport numpy as np\nimport pickle\n\nfrom copy import deepcopy\n\nimport torch\n\nfrom utils.data import load_trajectories, extract_global_features\nfrom utils.data import change_coordinate_system, scale_trajectories, aggregate_autoencoder_data\nfrom utils.data import input_trajectories_missing_steps\nfrom utils.preprocessing import remove_short_trajectories, aggregate_rnn_autoencoder_data\n\n\ndef save_scaler(scaler, path):\n    with open(path, 'wb') as scaler_file:\n        pickle.dump(scaler, scaler_file)\n    \n        \ndef load_scaler(path):\n    with open(path, 'rb') as scaler_file:\n        scaler = pickle.load(scaler_file)\n    return scaler\n\n\n\n# Load trajectory data and convert it into a format suitable for RNN autoencoders for training and testing of joint models\ndef data_of_combined_model(**args):\n    # General\n    exp_dir = args.get('exp_dir', '')\n    split = args.get('split', 'train')\n    normalize_pose = args.get('normalize_pose', False)\n    trajectories_path = args.get('trajectories_path', '')\n    include_global = args.get('include_global', True)\n    debug = args.get('debug', False)\n    if 'train' in split:\n      # đổi thành path khác trong folder data nếu train dataset khác\n        subfolder = '/kaggle/input/avenue/Avenue/training'\n    elif 'test' in split:\n        subfolder = '/kaggle/input/avenue/Avenue/testing'\n    else:\n        subfolder = 'validating'\n    trajectories_path = os.path.join(trajectories_path, f'{subfolder}/trajectories')\n    video_resolution = args.get('vid_res', (1080,720))\n    video_resolution = np.array(video_resolution, dtype=np.float32)\n    # Architecture\n    reconstruct_original_data = args.get('reconstruct_original_data', False) \n    input_length = args.get('seg_len', 12)\n    seg_stride = args.get('seg_stride', 1) - 1 \n    pred_length = 0 \n    # Training\n    input_missing_steps = False # args.input_missing_steps\n    \n    if normalize_pose == True:\n        global_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        local_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        out_normalisation_strategy = args.get('normalization_strategy', 'robust')\n\n\n    trajectories = load_trajectories(trajectories_path, debug=debug, split=split)\n    print('\\nLoaded %d trajectories.' % len(trajectories))\n\n    trajectories = remove_short_trajectories(trajectories, input_length=input_length,\n                                             input_gap=seg_stride, pred_length=pred_length)\n    print('\\nRemoved short trajectories. Number of trajectories left: %d.' % len(trajectories))\n\n    # trajectories, trajectories_val = split_into_train_and_test(trajectories, train_ratio=0.8, seed=42)\n\n    if input_missing_steps:\n        trajectories = input_trajectories_missing_steps(trajectories)\n        print('\\nInputted missing steps of trajectories.')\n\n    # Global\n    if include_global:\n        global_trajectories = extract_global_features(deepcopy(trajectories), video_resolution=video_resolution)\n\n        global_trajectories = change_coordinate_system(global_trajectories, video_resolution=video_resolution,\n                                                        coordinate_system='global', invert=False)\n\n        print('\\nChanged global trajectories\\'s coordinate system to global.')\n        \n        X_global, y_global, X_global_meta, y_global_meta = aggregate_rnn_autoencoder_data(global_trajectories, \n                                                                                        input_length=input_length,\n                                                                                        input_gap=seg_stride, pred_length=pred_length, \n                                                                                        return_ids=True)\n        \n        if normalize_pose == True:\n            # nếu test avenue thì dùng dòng này\n            #/content/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\n            # default: scaler_path = os.path.join(exp_dir, f'global_{global_normalisation_strategy}.pickle')\n            scaler_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle'\n            if split == 'train':\n                _, global_scaler = scale_trajectories(aggregate_autoencoder_data(global_trajectories),\n                                                    strategy=global_normalisation_strategy)\n                save_scaler(global_scaler, scaler_path)\n            else:\n                global_scaler = load_scaler(scaler_path)\n\n            X_global, _ = scale_trajectories(X_global, scaler=global_scaler, strategy=global_normalisation_strategy)\n            \n            if y_global is not None:\n                y_global, _ = scale_trajectories(y_global, scaler=global_scaler,\n                                                strategy=global_normalisation_strategy)\n                \n            print('\\nNormalised global trajectories using the %s normalisation strategy.' % global_normalisation_strategy)\n    \n    else:\n        X_global, X_global_meta = None, None\n    \n    # Local\n    local_trajectories = deepcopy(trajectories) if reconstruct_original_data else trajectories\n\n    local_trajectories = change_coordinate_system(local_trajectories, video_resolution=video_resolution,\n                                                  coordinate_system='bounding_box_centre', invert=False)\n\n    print('\\nChanged local trajectories\\'s coordinate system to bounding_box_centre.')\n\n    X_local, y_local, X_local_meta, y_local_meta = aggregate_rnn_autoencoder_data(local_trajectories, input_length=input_length, \n                                                                                  input_gap=seg_stride, pred_length=pred_length,\n                                                                                  return_ids=True)\n    \n    if normalize_pose == True:\n        #scaler_path = '/content/drive/MyDrive/DCMD-main-main/checkpoints/Avenue/test_experiment/local_robust.pickle'\n        scaler_path = os.path.join(exp_dir, f'local_{local_normalisation_strategy}.pickle')\n\n        if split == 'train':\n            _, local_scaler = scale_trajectories(aggregate_autoencoder_data(local_trajectories),\n                                                strategy=local_normalisation_strategy)\n            save_scaler(local_scaler, scaler_path)\n        else:\n            local_scaler = load_scaler(\"/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\")\n\n        X_local, _ = scale_trajectories(X_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n\n        if y_local is not None:\n            y_local, _ = scale_trajectories(y_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n        \n        print('\\nNormalised local trajectories using the %s normalisation strategy.' % local_normalisation_strategy)\n\n    # (Optional) Reconstruct the original data\n    if reconstruct_original_data:\n        print('\\nReconstruction/Prediction target is the original data.')\n        out_trajectories = trajectories\n        \n        out_trajectories = change_coordinate_system(out_trajectories, video_resolution=video_resolution,\n                                                    coordinate_system='global', invert=False)\n    \n        print('\\nChanged target trajectories\\'s coordinate system to global.')\n        \n        scaler_path = os.path.join(exp_dir, f'out_{out_normalisation_strategy}.pickle')\n    \n        if split == 'train':\n            _, out_scaler = scale_trajectories(aggregate_autoencoder_data(out_trajectories),\n                                               strategy=out_normalisation_strategy)\n            save_scaler(out_scaler, scaler_path)\n        else:\n            out_scaler = load_scaler(scaler_path)\n        \n        ######## X_out_{}, y_out_{} numpy arrays\n\n        X_out, y_out, X_out_meta, y_out_meta = aggregate_rnn_autoencoder_data(out_trajectories, input_length=input_length, \n                                                                              input_gap=seg_stride, pred_length=pred_length,\n                                                                              return_ids=True)\n\n        X_out, _ = scale_trajectories(X_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n        \n        if y_out is not None:\n            y_out, _ = scale_trajectories(y_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n            \n        print('\\nNormalised target trajectories using the %s normalisation strategy.' % out_normalisation_strategy)\n        \n            \n    if pred_length > 0:\n        \n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta), \\\n                   (y_out, y_out_meta) \n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta)\n    else:\n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta)\n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:41.292530Z","iopub.execute_input":"2025-05-26T09:40:41.293019Z","iopub.status.idle":"2025-05-26T09:40:41.301496Z","shell.execute_reply.started":"2025-05-26T09:40:41.292997Z","shell.execute_reply":"2025-05-26T09:40:41.300877Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/utils/get_robust_data.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# **Train** ","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/train_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:36:09.275300Z","iopub.execute_input":"2025-05-10T09:36:09.275923Z","iopub.status.idle":"2025-05-10T11:54:40.170701Z","shell.execute_reply.started":"2025-05-10T09:36:09.275891Z","shell.execute_reply":"2025-05-10T11:54:40.169982Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If download checkpoint needed, run this code and download checkpoints.zip file in Output","metadata":{}},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n# 2. (tuỳ chọn) kiểm tra xem zip đã tạo xong chưa\n!ls -lh /kaggle/working/checkpoints.zip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:43:30.055859Z","iopub.execute_input":"2025-05-26T09:43:30.056096Z","iopub.status.idle":"2025-05-26T09:43:30.059988Z","shell.execute_reply.started":"2025-05-26T09:43:30.056080Z","shell.execute_reply":"2025-05-26T09:43:30.059235Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/eval_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-05-26T09:43:32.799055Z","iopub.execute_input":"2025-05-26T09:43:32.799593Z","iopub.status.idle":"2025-05-26T13:05:29.018946Z","shell.execute_reply.started":"2025-05-26T09:43:32.799569Z","shell.execute_reply":"2025-05-26T13:05:29.017808Z"}},"outputs":[{"name":"stdout","text":"Loading data and creating loaders.....\n\nLoaded 2594 trajectories.\n\nRemoved short trajectories. Number of trajectories left: 1661.\n\nChanged local trajectories's coordinate system to bounding_box_centre.\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.3.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n\nNormalised local trajectories using the robust normalisation strategy.\n/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:282: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': 'loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\"].\nTesting DataLoader 0: 100%|█████████████████| 150/150 [3:21:08<00:00,  0.01it/s]\n  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n 20%|█████████                                    | 1/5 [00:00<00:03,  1.04it/s]\u001b[A\n 40%|██████████████████                           | 2/5 [00:01<00:02,  1.07it/s]\u001b[A\n 60%|███████████████████████████                  | 3/5 [00:02<00:01,  1.10it/s]\u001b[A\n 80%|████████████████████████████████████         | 4/5 [00:03<00:00,  1.09it/s]\u001b[A\n100%|█████████████████████████████████████████████| 5/5 [00:04<00:00,  1.10it/s]\u001b[A\nTesting DataLoader 0: 100%|█████████████████| 150/150 [3:21:17<00:00,  0.01it/s]\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m           AUC           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8522500395774841    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T13:10:50.210286Z","iopub.execute_input":"2025-05-26T13:10:50.210565Z","iopub.status.idle":"2025-05-26T13:10:59.549864Z","shell.execute_reply.started":"2025-05-26T13:10:50.210540Z","shell.execute_reply":"2025-05-26T13:10:59.549175Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/DCMD-main/checkpoints/ (stored 0%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/ (stored 0%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/ (stored 0%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/ (stored 0%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/prediction.pt (deflated 25%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/trans.pt (deflated 100%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/metadata.pt (deflated 90%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/gt_data.pt (deflated 89%)\nupdating: kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment/saved_tensors_test_best_50/frames.pt (deflated 96%)\n","output_type":"stream"}],"execution_count":22}]}