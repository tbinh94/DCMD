{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11715637,"sourceType":"datasetVersion","datasetId":7353969},{"sourceId":11760202,"sourceType":"datasetVersion","datasetId":7382794}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/guijiejie/DCMD-main.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:23:31.491923Z","iopub.execute_input":"2025-05-26T09:23:31.492519Z","iopub.status.idle":"2025-05-26T09:23:32.382164Z","shell.execute_reply.started":"2025-05-26T09:23:31.492496Z","shell.execute_reply":"2025-05-26T09:23:32.381479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:22:29.867999Z","iopub.execute_input":"2025-05-07T09:22:29.868367Z","iopub.status.idle":"2025-05-07T09:22:29.991158Z","shell.execute_reply.started":"2025-05-07T09:22:29.868333Z","shell.execute_reply":"2025-05-07T09:22:29.990039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pip**","metadata":{}},{"cell_type":"code","source":"!pip install appdirs==1.4.4 docker-pycreds==0.4.0 gitdb==4.0.10 gitpython==3.1.32 joblib==1.3.1 numpy==1.25.2 pathtools==0.1.2 protobuf==4.23.4 scikit-learn==1.3.0 scipy==1.11.1 sentry-sdk==1.29.2 setproctitle==1.3.2 smmap==5.0.0 threadpoolctl==3.2.0 wandb==0.15.8\n!pip install --upgrade pytorch-lightning pyyaml wandb pandas numpy matplotlib matplotlib-inline scikit-learn tqdm \n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:23:40.101573Z","iopub.execute_input":"2025-05-26T09:23:40.101899Z","iopub.status.idle":"2025-05-26T09:28:15.416893Z","shell.execute_reply.started":"2025-05-26T09:23:40.101873Z","shell.execute_reply":"2025-05-26T09:28:15.415922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Replace code to run properly**","metadata":{}},{"cell_type":"code","source":"!pip install networkx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T03:59:42.776440Z","iopub.execute_input":"2025-06-04T03:59:42.776756Z","iopub.status.idle":"2025-06-04T03:59:45.799369Z","shell.execute_reply.started":"2025-06-04T03:59:42.776735Z","shell.execute_reply":"2025-06-04T03:59:45.798639Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml \n### Experiment configuration\n\n## General settings\nsplit: 'train' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'train_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'best.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: true\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n# Cho forecasting\nuse_forecasting: true\nhidden_dim_forecast: 64\ndropout_forecast: 0.2\nnum_heads_gat: 4\nlatent_dim_forecast: 32\nlambda_forecast: 0.1\n\n# Trong config.yaml của bạn, thêm dòng tương tự:\nuse_adaptive: true\nuse_jigsaw: true\nlayer_channels: [128, 64, 128]\nemb_dim: 64\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: false # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (his+pre)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding \n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:03:30.493938Z","iopub.execute_input":"2025-06-04T05:03:30.494232Z","iopub.status.idle":"2025-06-04T05:03:30.502106Z","shell.execute_reply.started":"2025-06-04T05:03:30.494208Z","shell.execute_reply":"2025-06-04T05:03:30.501321Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/transformer.py\nimport torch\nimport torch.nn.functional as F\nfrom torch import layer_norm, nn\nimport numpy as np\nfrom typing import List, Tuple, Union\n\nimport math\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef set_requires_grad(nets, requires_grad=False):\n    \"\"\"Set requies_grad for all the networks.\n\n    Args:\n        nets (nn.Module | list[nn.Module]): A list of networks or a single\n            network.\n        requires_grad (bool): Whether the networks require gradients or not\n    \"\"\"\n    if not isinstance(nets, list):\n        nets = [nets]\n    for net in nets:\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\nclass StylizationBlock(nn.Module):\n\n    def __init__(self, latent_dim, time_embed_dim, dropout):\n        super().__init__()\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, 2 * latent_dim),\n        )\n        self.norm = nn.LayerNorm(latent_dim)\n        self.out_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(nn.Linear(latent_dim, latent_dim)),\n        )\n\n    def forward(self, h, emb):\n        \"\"\"\n        h: B, T, D\n        emb: B, D\n        \"\"\"\n        # B, 1, 2D\n        emb_out = self.emb_layers(emb).unsqueeze(1)\n        # scale: B, 1, D / shift: B, 1, D\n        scale, shift = torch.chunk(emb_out, 2, dim=2)\n        # B, T, D\n        h = self.norm(h) * (1 + scale) + shift\n        h = self.out_layers(h)\n        return h\n\n\nclass FFN(nn.Module):\n\n    def __init__(self, latent_dim, ffn_dim, dropout, time_embed_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(latent_dim, ffn_dim)\n        self.linear2 = zero_module(nn.Linear(ffn_dim, latent_dim))\n        self.activation = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n\n    def forward(self, x, emb):\n        \"\"\"\n            x: B, T, D (D=latent_dim)\n        \"\"\"\n        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n        y = x + self.proj_out(y, emb)\n        return y\n\n\nclass TemporalSelfAttention(nn.Module):\n\n    def __init__(self, n_frames, latent_dim, num_head, dropout, time_embed_dim, output_attention = True):\n        super().__init__()\n        self.num_head = num_head\n        self.output_attention = output_attention\n        self.norm = nn.LayerNorm(latent_dim)\n        self.query = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.key = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.value = nn.Linear(latent_dim, latent_dim, bias=False)\n        self.sigma_projection = nn.Linear(latent_dim, num_head, bias=False)\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = StylizationBlock(latent_dim, time_embed_dim, dropout)\n        n_frames = n_frames\n        self.distances = torch.zeros((n_frames, n_frames)).cuda(0)\n\n        for i in range(n_frames):\n            for j in range(n_frames):\n                self.distances[i][j] = abs(i - j)\n\n    def forward(self, x, emb):\n        \"\"\"\n        x: B, T, D (D=latent_dim)\n        \"\"\"\n        B, T, D = x.shape\n        H = self.num_head\n\n        ## series-association\n        # B, T, 1, D\n        query = self.query(self.norm(x)).unsqueeze(2)\n        # B, 1, T, D\n        key = self.key(self.norm(x)).unsqueeze(1)\n        # B, T, H, D/H\n        query = query.view(B, T, H, -1)\n        key = key.view(B, T, H, -1)\n        scale = 1. / math.sqrt(D/H)\n        # B, H, T, T\n        scores = torch.einsum('bnhd,bmhd->bhnm', query, key) / math.sqrt(D // H)\n        attention = scale * scores\n        # B, H, T, T\n        series = self.dropout(F.softmax(attention, dim=-1))\n\n        ## prior-association\n        sigma = self.sigma_projection(x).view(B, T, H)  # B, T, H\n        sigma = sigma.transpose(1, 2)  # B T H ->  B H T\n        sigma = torch.sigmoid(sigma * 5) + 1e-5\n        sigma = torch.pow(3, sigma) - 1\n        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, T)  # B, H, T, T\n        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1) # B, H, T, T\n        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2)).cuda(0) # B, H, T, T\n\n        # B, T, H, D/H\n        value = self.value(self.norm(x)).view(B, T, H, -1)\n        # B, T, D\n        y = torch.einsum('bhnm,bmhd->bnhd', series, value).reshape(B, T, D)\n        y = x + self.proj_out(y, emb)\n\n        if self.output_attention:\n            return y.contiguous(), series, prior, sigma\n        else:\n            return y.contiguous(), None\n\nclass TemporalDiffusionTransformerDecoderLayer(nn.Module):\n\n    def __init__(self,\n                 n_frames = 7,\n                 latent_dim=16,\n                 time_embed_dim=16,\n                 ffn_dim=32,\n                 num_head=4,\n                 dropout=0.5\n                 ):\n        super().__init__()\n        self.sa_block = TemporalSelfAttention(\n            n_frames, latent_dim, num_head, dropout, time_embed_dim)\n        self.ffn = FFN(latent_dim, ffn_dim, dropout, time_embed_dim)\n\n    def forward(self, x, emb):\n        x, series, prior, sigma = self.sa_block(x, emb)\n        x = self.ffn(x, emb)\n        return x, series, prior, sigma\n\n\nclass MotionTransformer(nn.Module):\n    def __init__(self,\n                 input_feats,\n                 num_frames=7,\n                 latent_dim=16,\n                 ff_size=32,\n                 num_layers=8,\n                 num_heads=8,\n                 dropout=0.2,\n                 activation=\"gelu\",\n                 output_attention = True,\n                 device: Union[str, torch.DeviceObjType] = 'cpu',\n                 inject_condition: bool = False,\n                 **kargs):\n        super().__init__()\n\n\n        self.input_feats = input_feats # 34\n        self.num_frames = num_frames\n        self.latent_dim = latent_dim\n        self.ff_size = ff_size\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.activation = activation\n        self.output_attention = output_attention\n        self.device = device\n        self.time_embed_dim = latent_dim\n        self.inject_condition = inject_condition\n\n        self.build_model()\n\n    def build_model(self):\n        self.sequence_embedding = nn.Parameter(torch.randn(self.num_frames, self.latent_dim))\n\n        # Input Embedding\n        self.joint_embed = nn.Linear(self.input_feats, self.latent_dim)\n        self.cond_embed = nn.Linear(256, self.time_embed_dim)\n\n        self.time_embed = nn.Sequential(\n            nn.Linear(self.latent_dim, self.time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(self.time_embed_dim, self.time_embed_dim),\n        )\n\n        self.temporal_decoder_blocks = nn.ModuleList()\n        for i in range(self.num_layers):\n            self.temporal_decoder_blocks.append(\n                TemporalDiffusionTransformerDecoderLayer(\n                    n_frames=self.num_frames,\n                    latent_dim=self.latent_dim,\n                    time_embed_dim=self.time_embed_dim,\n                    ffn_dim=self.ff_size,\n                    num_head=self.num_heads,\n                    dropout=self.dropout,\n                )\n            )\n        # Output Module\n        self.out = zero_module(nn.Linear(self.latent_dim, self.input_feats))\n\n\n    def forward(self, x, timesteps, condition_data:torch.Tensor=None):\n        \"\"\"\n        x: B, T, D (D=C*V)\n        \"\"\"\n        B, T = x.shape[0], x.shape[1]\n\n        # B, latent_dim\n        emb = self.time_embed(timestep_embedding(timesteps, self.latent_dim))\n\n        # Add conditioning signal\n        if self.inject_condition:\n            condition_data = self.cond_embed(condition_data)\n            emb = emb + condition_data\n\n        # B, T, latent_dim\n        h = self.joint_embed(x)\n        h = h + self.sequence_embedding.unsqueeze(0)[:, :T, :]\n        \n        i = 0\n        prelist = []\n        series_list = []\n        prior_list = []\n        sigma_list = []\n        for module in self.temporal_decoder_blocks:\n            if i < (self.num_layers // 2):\n                prelist.append(h)\n                h, series, prior, sigmas = module(h, emb) # B, T, latent_dim\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n            elif i >= (self.num_layers // 2):\n                h, series, prior, sigmas = module(h, emb)\n                h += prelist[-1]\n                series_list.append(series)\n                prior_list.append(prior)\n                sigma_list.append(sigmas)\n                prelist.pop()\n            i += 1\n\n        # B, T, C*V\n        output = self.out(h).view(B, T, -1).contiguous()\n        if self.output_attention:\n            return output, series_list, prior_list, sigma_list\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T03:59:53.965457Z","iopub.execute_input":"2025-06-04T03:59:53.966073Z","iopub.status.idle":"2025-06-04T03:59:53.974451Z","shell.execute_reply.started":"2025-06-04T03:59:53.966050Z","shell.execute_reply":"2025-06-04T03:59:53.973854Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/transformer.py\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/train_DCMD.py\nimport argparse\nimport os\nimport random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport yaml\nfrom models.dcmd import DCMD\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.strategies import DDPStrategy\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\nfrom utils.ema import EMACallback\n\n\nif __name__== '__main__':\n\n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='Pose_AD_Experiment')\n    parser.add_argument('-c', '--config', type=str, required=True,\n                        default='/your_default_config_file_path')\n    \n    args = parser.parse_args()\n    config_path = args.config\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args) \n    # Save config file to ckpt_dir\n    os.system(f'cp {config_path} {os.path.join(args.ckpt_dir, \"config.yaml\")}')     \n    \n    # Set seeds    \n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed) \n    pl.seed_everything(args.seed)\n\n    # Set callbacks and logger\n    if (hasattr(args, 'diffusion_on_latent') and args.stage == 'pretrain'):\n        monitored_metric = 'pretrain_rec_loss'\n        metric_mode = 'min'\n    elif args.validation:\n        monitored_metric = 'AUC'\n        metric_mode = 'max'\n    else:\n        monitored_metric = 'loss'\n        metric_mode = 'min'\n    callbacks = [ModelCheckpoint(\n                    dirpath=args.ckpt_dir,\n                    save_top_k=2,\n                    save_last=True,\n                    monitor=monitored_metric,\n                    mode=metric_mode\n                )]\n\n    \n    callbacks += [EMACallback()] if args.use_ema else [] # Use to achieve exponential moving average\n    \n    if args.use_wandb:\n        callbacks += [LearningRateMonitor(logging_interval='step')]\n        wandb_logger = WandbLogger(project=args.project_name, group=args.group_name, entity=args.wandb_entity, \n                                   name=args.dir_name, config=vars(args), log_model='all')\n    else:\n        wandb_logger = False\n\n    parser.add_argument(\n        \"--channels\",       # vẫn để channels nếu có những chỗ khác dựa vào as well,\n        type=int,\n        default=3,\n        help=\"Số kênh đầu vào (vd: xyz = 3).\"\n    )\n    # THÊM 2 dòng sau để tạo ra args.in_channels:\n    parser.add_argument(\n        \"--in_channels\",\n        type=int,\n        default=3,\n        help=\"(Thêm) Số kênh đầu vào, để tương thích với những chỗ gọi args.in_channels\"\n    )\n    \n    # Get dataset and loaders\n    _, train_loader, _, val_loader = get_dataset_and_loader(args, split=args.split, validation=args.validation)\n    \n    # Initialize model and trainer\n    model = DCMD(args)\n    \n    trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices, default_root_dir=args.ckpt_dir, max_epochs=args.n_epochs, \n                         logger=wandb_logger, callbacks=callbacks, strategy=DDPStrategy(find_unused_parameters=False),\n                         log_every_n_steps=20, num_sanity_val_steps=0, deterministic=True)\n    \n    # Train the model    \n    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:32:36.098049Z","iopub.execute_input":"2025-06-04T04:32:36.098711Z","iopub.status.idle":"2025-06-04T04:32:36.107096Z","shell.execute_reply.started":"2025-06-04T04:32:36.098686Z","shell.execute_reply":"2025-06-04T04:32:36.106258Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/train_DCMD.py\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml\n### Experiment configuration\n\n## General settings\nsplit: 'test' # data split; choices ['train', 'test']\ndebug: false # if true, load only a few data samples\nseed: 999\nvalidation: false # use validation; only for UBnormal\nuse_hr: false # for validation and test on UBnormal\n\n## Computational resources\naccelerator: 'gpu'\ndevices: [0] # indices of cuda devices to use\n\n## Paths\ndir_name: 'test_experiment' # name of the directory of the current experiment\ndata_dir: '/kaggle/input/avenue/Avenue' # path to the data\nexp_dir: '/kaggle/working/DCMD-main/checkpoints' # path to the directory that will contain the current experiment directory\ntest_path: '/kaggle/input/avenue/Avenue/testing/test_frame_mask' # path to the test data\nload_ckpt: 'last.ckpt' # name of the checkpoint to load at inference time\ncreate_experiment_dir: false # if true, create a new directory for the current experiment\n\n## WANDB configuration\nuse_wandb: false\nproject_name: \"project_name\"\nwandb_entity: \"entity_name\"\ngroup_name: \"group_name\"\nuse_ema: false\n\n##############################\n\n\n### Model's configuration\n\n## U-Net's configuration\ndropout: 0. # probability of dropout\nconditioning_strategy: 'inject'\n## Rec configuration\nh_dim: 512 # dimension of the bottleneck at the end of the encoder of the conditioning network\nlatent_dim: 256 # dimension of the latent space of the conditioning encoder\nchannels: [512,256,512] # channels for the encoder\n\n##############################\n\n\n### Training's configuration\n\n## Diffusion's configuration\nnoise_steps: 10 # how many diffusion steps to perform\n\n### Optimizer and scheduler's configuration\nn_epochs: 10\nopt_lr: 0.001\n\n## Losses' configuration\nloss_fn: 'smooth_l1' # loss function; choices ['mse', 'l1', 'smooth_l1']\n\n##############################\n\n\n### Inference's configuration\nn_generated_samples: 50 # number of samples to generate\nmodel_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss;\n                           # if 'poses', the model will return the generated poses; \n                           # if 'all', the model will return both the loss and the generated poses\naggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; \n                             # if 'mean', the mean of loss of the samples will be selected; \n                             # if 'median', the median of the loss of the samples will be selected; \n                             # if 'random', a random sample will be selected;\n                             # if 'mean_poses', the mean of the generated poses will be selected;\n                             # if 'median_poses', the median of the generated poses will be selected;\n                             # if 'all', all the generated poses will be selected\nfilter_kernel_size: 30 # size of the kernel to use for smoothing the anomaly score of each clip\nframes_shift: 6 # it compensates the shift of the anomaly score due to the sliding window; \n                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset\nsave_tensors: true # if true, save the generated tensors for faster inference\nload_tensors: false # if true, load the generated tensors for faster inference\n\n##############################\n\n\n### Dataset's configuration\n\n## Important parameters\ndataset_choice: 'HR-Avenue'\nseg_len: 7 # length of the window (cond+noised)\nvid_res: [640,360]\nbatch_size: 2048\npad_size: 12 # size of the padding\n\n## Other parameters\nheadless: false # remove the keypoints of the head\nhip_center: false # center the keypoints on the hip\nkp18_format: false # use the 18 keypoints format\nnormalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise\nnum_coords: 2 # number of coordinates to use\nnum_transform: 5 # number of transformations to apply\nnum_workers: 4\nseg_stride: 1\nseg_th: 0\nstart_offset: 0\nsymm_range: true\nuse_fitted_scaler: false\n\n## New configuration\nn_his: 3\npadding: 'LastFrame'\n## translinear configuration\nnum_layers: 6\nnum_heads: 8\nlatent_dims: 512\nloss_1_series_weight: 0.01\nloss_1_prior_weight: 0\nloss_2_series_weight: 0\nloss_2_prior_weight: 0.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:22.541825Z","iopub.execute_input":"2025-05-26T09:40:22.542501Z","iopub.status.idle":"2025-05-26T09:40:22.548589Z","shell.execute_reply.started":"2025-05-26T09:40:22.542478Z","shell.execute_reply":"2025-05-26T09:40:22.547948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/dcmd.py\nimport argparse\nimport os\nfrom math import prod\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\n\nfrom models.stsae.stsae import STSAE\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nfrom utils.diffusion_utils import Diffusion\nfrom utils.eval_utils import (compute_var_matrix, filter_vectors_by_cond,\n                              get_avenue_mask, get_hr_ubnormal_mask, pad_scores, score_process)\nfrom utils.model_utils import processing_data, my_kl_loss\nfrom models.transformer import MotionTransformer\nfrom utils.tools import get_dct_matrix, generate_pad, padding_traj\n\n\nclass DCMD(pl.LightningModule):\n\n    losses = {'l1': nn.L1Loss, 'smooth_l1': nn.SmoothL1Loss, 'mse': nn.MSELoss}\n    conditioning_strategies = {'inject': 'inject'}\n\n    def __init__(self, args: argparse.Namespace) -> None:\n        \"\"\"\n        This class implements DCMD model.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n        \"\"\"\n\n        super(DCMD, self).__init__()\n\n        ## Log the hyperparameters of the model\n        self.save_hyperparameters(args)\n\n        ## Set the internal variables of the model\n        # Data parameters\n        self.n_frames = args.seg_len\n        self.num_coords = args.num_coords\n        self.n_joints = self._infer_number_of_joint(args)\n\n        ## Model parameters\n        # Main network\n        self.dropout = args.dropout\n        self.conditioning_strategy = self.conditioning_strategies[args.conditioning_strategy]\n        # Conditioning network\n        self.cond_h_dim = args.h_dim\n        self.cond_latent_dim = args.latent_dim\n        self.cond_channels = args.channels\n        self.cond_dropout = args.dropout\n\n        ## Training and inference parameters\n        self.learning_rate = args.opt_lr\n        self.loss_fn = self.losses[args.loss_fn](reduction='none')\n        self.noise_steps = args.noise_steps\n        self.aggregation_strategy = args.aggregation_strategy\n        self.n_generated_samples = args.n_generated_samples\n        self.model_return_value = args.model_return_value\n        self.gt_path = args.gt_path\n        self.split = args.split\n        self.use_hr = args.use_hr\n        self.ckpt_dir = args.ckpt_dir\n        self.save_tensors = args.save_tensors\n        self.num_transforms = args.num_transform\n        self.anomaly_score_pad_size = args.pad_size\n        self.anomaly_score_filter_kernel_size = args.filter_kernel_size\n        self.anomaly_score_frames_shift = args.frames_shift\n        self.dataset_name = args.dataset_choice\n\n        # New parameters\n        self.n_his = args.n_his\n        self.padding = args.padding\n        self.num_layers = args.num_layers\n        self.num_heads = args.num_heads\n        self.latent_dims = args.latent_dims\n        self.automatic_optimization = False\n        self.loss_1_series_weight = args.loss_1_series_weight\n        self.loss_1_prior_weight = args.loss_1_prior_weight\n        self.loss_2_series_weight = args.loss_2_series_weight\n        self.loss_2_prior_weight = args.loss_2_prior_weight\n        self.idx_pad, self.zero_index = generate_pad(self.padding, self.n_his, self.n_frames-self.n_his)\n\n        ## Set the noise scheduler for the diffusion process\n        self._set_diffusion_variables()\n\n        ## Build the model\n        self.build_model()\n\n\n    def build_model(self) -> None:\n        \"\"\"\n        Build the model according to the specified hyperparameters.\n        \"\"\"\n\n        # Prediction Model\n        pre_model = MotionTransformer(\n            input_feats=2 * self.n_joints,\n            num_frames=self.n_frames,\n            num_layers=self.num_layers,\n            num_heads=self.num_heads,\n            latent_dim=self.latent_dims,\n            dropout=self.dropout,\n            device=self.device,\n            inject_condition=(self.conditioning_strategy == 'inject')\n        )\n\n        # Reconstruction Model\n        rec_model = STSAE(\n            c_in=self.num_coords,\n            h_dim=self.cond_h_dim,\n            latent_dim=self.cond_latent_dim,\n            n_frames=self.n_his,\n            dropout=self.cond_dropout,\n            n_joints=self.n_joints,\n            layer_channels=self.cond_channels,\n            device=self.device)\n\n        self.pre_model, self.rec_model = pre_model, rec_model\n\n\n    def forward(self, input_data: List[torch.Tensor], aggr_strategy: str = None, return_: str = None) -> List[torch.Tensor]:\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n\n        ## Unpack data: tensor_data is the input data, meta_out is a list of metadata\n        tensor_data, meta_out = self._unpack_data(input_data)\n        B = tensor_data.shape[0]\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :]\n        x_0 = padding_traj(history_data, self.padding, self.idx_pad, self.zero_index)\n\n        generated_xs = []\n        # Generate m future predictions\n        for _ in range(self.n_generated_samples):\n\n            ## Reconstruction —— AE model\n            condition_embedding, rec_his_data = self.rec_model(history_data)\n\n            ## Prediction —— diffusion model\n            ## DCT transformation\n            dct_m, idct_m = get_dct_matrix(self.n_frames)\n            dct_m_all = dct_m.float().to(self.device)\n            idct_m_all = idct_m.float().to(self.device)\n            # (B, C, T, V) -> (B, T, V, C)\n            x = x_0.permute(0, 2, 3, 1).contiguous()\n            # (B, T, V, C) -> (B, T, C*V)\n            x = x.reshape([x.shape[0], self.n_frames, -1])\n            y = torch.matmul(dct_m_all, x)  # [B, T, C*V]\n\n            ## Generate gaussian noise of the same shape as the y\n            y_d = torch.randn_like(y, device=self.device)\n\n            ## (t ∈ T, T-1, ..., 1)\n            for i in reversed(range(1, self.noise_steps)):\n\n                ### Prediction (Two branches)\n                ## Set the time step\n                t = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev[0] = 0\n\n                ## Generate gaussian noise of the same shape as the predicted noise\n                noise_pre = torch.randn_like(y_d, device=self.device) if i > 1 else torch.zeros_like(y_d, device=self.device)\n\n                ## First branch\n                # Predict the noise\n                predicted_noise_pre, series, prior, _ = self.pre_model(y_d, t, condition_data=condition_embedding)\n                # Get the alpha and beta values and expand them to the shape of the predicted noise\n                alpha_pre = self._alpha[t][:, None, None]\n                alpha_hat_pre = self._alpha_hat[t][:, None, None]\n                beta_pre = self._beta[t][:, None, None]\n                # Recover the predicted sequence\n                y_d = (1 / torch.sqrt(alpha_pre)) * (y_d - ((1 - alpha_pre) / (torch.sqrt(1 - alpha_hat_pre))) * predicted_noise_pre) \\\n                    + torch.sqrt(beta_pre) * noise_pre\n                ## Second branch\n                alpha_hat_prev = self._alpha_hat[t_prev][:, None, None]\n                # Add noise\n                y_n = (torch.sqrt(alpha_hat_prev) * y) + (torch.sqrt(1 - alpha_hat_prev) * noise_pre)\n                ## Mask completion\n                # Get M values\n                mask = torch.zeros_like(x, device=self.device) # [batch, T, C*V]\n                for m in range(0, self.n_his):\n                    mask[:, m, :] = 1\n                # iDCT transformation\n                y_d_idct = torch.matmul(idct_m_all, y_d)\n                y_n_idct = torch.matmul(idct_m_all, y_n)\n                # mask-mul\n                m_mul_y_n = torch.mul(mask, y_n_idct)\n                m_mul_y_d = torch.mul((1-mask), y_d_idct)\n                # together\n                m_y = m_mul_y_d + m_mul_y_n\n                # DCT again\n                y_d = torch.matmul(dct_m_all, m_y)\n\n            # iDCT\n            pre_future_data = torch.matmul(idct_m_all, y_d)\n            # (B, T, C*V) -> (B, T, V, C)\n            pre_future_data = pre_future_data.reshape(pre_future_data.shape[0], pre_future_data.shape[1], -1, 2)\n            # (B, T, V, C) -> (B, C, T, V)\n            pre_future_data = pre_future_data.permute(0, 3, 1, 2).contiguous()\n            # select future sequences\n            pre_future_data = pre_future_data[:,:,self.n_his:,:]\n\n            ## Reconstruction + Prediction\n            xs = torch.cat((rec_his_data, pre_future_data), dim=2)\n\n            generated_xs.append(xs)\n\n        selected_x, loss_of_selected_x = self._aggregation_strategy(generated_xs, tensor_data, aggr_strategy)\n\n        return self._pack_out_data(selected_x, loss_of_selected_x, [tensor_data] + meta_out, return_=return_)\n\n\n    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.float32:\n        \"\"\"\n        Training step of the model.\n        \"\"\"\n\n        ## Get the optimizer returned in configuration_optimizers()\n        opt = self.optimizers()\n\n        ## Unpack data: tensor_data is the input data\n        tensor_data, _ = self._unpack_data(batch)\n\n        ## Select frames to reconstruct and to predict\n        history_data = tensor_data[:, :, :self.n_his, :] # Used for rec (first n_his)\n        x_0 = tensor_data # Used for pre（all）\n\n        ## Reconstruction\n        # Encode the history data\n        condition_embedding, rec_his_data = self.rec_model(history_data)\n        # Compute the rec_loss\n        rec_loss = torch.mean(self.loss_fn(rec_his_data, history_data))\n        self.log('rec_loss', rec_loss)\n\n        ## Prediction\n        # DCT transformation\n        dct_m, _ = get_dct_matrix(self.n_frames)\n        dct_m_all = dct_m.float().to(self.device)\n        # (B, C, T, V) -> (B, T, V, C)\n        x = x_0.permute(0, 2, 3, 1).contiguous()\n        # (B, T, V, C) -> (B, T, C*V)\n        x = x.reshape([x.shape[0], self.n_frames, -1]) # [batch, T, C*V]\n        y_0 = torch.matmul(dct_m_all, x)\n\n        # Sample the time steps and corrupt the data\n        t = self.noise_scheduler.sample_timesteps(y_0.shape[0]).to(self.device)\n        y_t, pre_noise = self.noise_scheduler.noise_motion(y_0, t) # (B, T, C*(V-1))\n\n        # Predict the noise\n        pre_predicted_noise, series, prior, _ = self.pre_model(y_t, t, condition_data=condition_embedding)\n\n        # Compute the pre_loss\n        # Calculate Association discrepancy\n        series_loss = 0.0\n        prior_loss = 0.0\n        for u in range(len(prior)):\n            # Pdetach, S <-> Maximize\n            series_loss += \\\n                (torch.mean(my_kl_loss(\n                    series[u],\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach()))\n                + torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach(),\n                    series[u])))\n            # P, Sdetach <-> Minimize\n            prior_loss += \\\n                (torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)),\n                    series[u].detach()))\n                + torch.mean(my_kl_loss(\n                    series[u].detach(),\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)))))\n        series_loss = series_loss / len(prior)\n        prior_loss = prior_loss / len(prior)\n\n        pre_loss = torch.mean(self.loss_fn(pre_predicted_noise, pre_noise))\n        self.log('pre_loss', pre_loss)\n\n        ## Compute loss1 & loss2\n        loss1 = rec_loss + pre_loss \\\n                - self.loss_1_series_weight * series_loss \\\n                + self.loss_1_prior_weight * prior_loss\n        self.log('loss1', loss1)\n        loss2 = rec_loss + pre_loss \\\n                + self.loss_2_prior_weight * prior_loss \\\n                + self.loss_2_series_weight * series_loss\n        self.log('loss2', loss2)\n\n        ## Minimax strategy\n        self.manual_backward(loss1, retain_graph=True)\n        self.manual_backward(loss2)\n        opt.step()\n        opt.zero_grad()\n\n\n    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Test step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._test_output_list.append(self.forward(batch))\n        return\n\n\n    def on_test_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_test_epoch_start()\n        self._test_output_list = []\n        return\n\n\n    def on_test_epoch_end(self) -> float:\n        \"\"\"\n        Test epoch end of the model.\n\n        Returns:\n            float: test auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._test_output_list)\n        del self._test_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score)\n        return auc_score\n\n\n    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        \"\"\"\n        Validation step of the model. It saves the output of the model and the input data as\n        List[torch.Tensor]: [predicted poses and the loss, tensor_data, transformation_idx, metadata, actual_frames]\n\n        Args:\n            batch (List[torch.Tensor]): list containing the following tensors:\n                                         - tensor_data: tensor of shape (B, C, T, V) containing the input sequences\n                                         - transformation_idx\n                                         - metadata\n                                         - actual_frames\n            batch_idx (int): index of the batch\n        \"\"\"\n\n        self._validation_output_list.append(self.forward(batch))\n        return\n\n\n    def on_validation_epoch_start(self) -> None:\n        \"\"\"\n        Called when the test epoch begins.\n        \"\"\"\n\n        super().on_validation_epoch_start()\n        self._validation_output_list = []\n        return\n\n\n    def on_validation_epoch_end(self) -> float:\n        \"\"\"\n        Validation epoch end of the model.\n\n        Returns:\n            float: validation auc score\n        \"\"\"\n\n        out, gt_data, trans, meta, frames = processing_data(self._validation_output_list)\n        del self._validation_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score, sync_dist=True)\n        return auc_score\n\n\n    def configure_optimizers(self) -> Dict:\n        \"\"\"\n        Configure the optimizers and the learning rate schedulers.\n\n        Returns:\n            Dict: dictionary containing the optimizers, the learning rate schedulers and the metric to monitor\n        \"\"\"\n\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, last_epoch=-1)\n\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'AUC'}\n\n\n    def post_processing(self, out: np.ndarray, gt_data: np.ndarray, trans: np.ndarray, meta: np.ndarray, frames: np.ndarray) -> float:\n        \"\"\"\n        Post processing of the model.\n\n        Args:\n            out (np.ndarray): output of the model\n            gt_data (np.ndarray): ground truth data\n            trans (np.ndarray): transformation index\n            meta (np.ndarray): metadata\n            frames (np.ndarray): frame indexes of the data\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        all_gts = [file_name for file_name in os.listdir(self.gt_path) if file_name.endswith('.npy')]\n        all_gts = sorted(all_gts)\n        scene_clips = [(int(fn.split('_')[0]), int(fn.split('_')[1].split('.')[0])) for fn in all_gts]\n        hr_ubnormal_masked_clips = get_hr_ubnormal_mask(self.split) if (self.use_hr and (self.dataset_name == 'UBnormal')) else {}\n        hr_avenue_masked_clips = get_avenue_mask() if self.dataset_name == 'HR-Avenue' else {}\n\n        num_transform = self.num_transforms\n        model_scores_transf = {}\n        dataset_gt_transf = {}\n\n        for transformation in tqdm(range(num_transform)):\n            # iterating over each transformation T\n\n            dataset_gt = []\n            model_scores = []\n            cond_transform = (trans == transformation)\n\n            out_transform, gt_data_transform, meta_transform, frames_transform = filter_vectors_by_cond([out, gt_data, meta, frames], cond_transform)\n\n            for idx in range(len(all_gts)):\n                # iterating over each clip C with transformation T\n\n                scene_idx, clip_idx = scene_clips[idx]\n\n                gt = np.load(os.path.join(self.gt_path, all_gts[idx]))\n                n_frames = gt.shape[0]\n\n                cond_scene_clip = (meta_transform[:, 0] == scene_idx) & (meta_transform[:, 1] == clip_idx)\n                out_scene_clip, gt_scene_clip, meta_scene_clip, frames_scene_clip = filter_vectors_by_cond([out_transform, gt_data_transform,\n                                                                                                            meta_transform, frames_transform],\n                                                                                                           cond_scene_clip)\n\n                figs_ids = sorted(list(set(meta_scene_clip[:, 2])))\n                error_per_person = []\n                error_per_person_max_loss = []\n\n                for fig in figs_ids:\n                    # iterating over each actor A in each clip C with transformation T\n\n                    cond_fig = (meta_scene_clip[:, 2] == fig)\n                    out_fig, _, frames_fig = filter_vectors_by_cond([out_scene_clip, gt_scene_clip, frames_scene_clip], cond_fig)\n                    loss_matrix = compute_var_matrix(out_fig, frames_fig, n_frames)\n                    fig_reconstruction_loss = np.nanmax(loss_matrix, axis=0)\n                    if self.anomaly_score_pad_size != -1:\n                        fig_reconstruction_loss = pad_scores(fig_reconstruction_loss, gt, self.anomaly_score_pad_size)\n\n                    error_per_person.append(fig_reconstruction_loss)\n                    error_per_person_max_loss.append(max(fig_reconstruction_loss))\n\n                clip_score = np.stack(error_per_person, axis=0)\n                clip_score_log = np.log1p(clip_score)\n                clip_score = np.mean(clip_score, axis=0) + (np.amax(clip_score_log, axis=0)-np.amin(clip_score_log, axis=0))\n\n                # removing the non-HR frames for UBnormal dataset\n                if (scene_idx, clip_idx) in hr_ubnormal_masked_clips:\n                    clip_score = clip_score[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n                    gt = gt[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n\n                # removing the non-HR frames for Avenue dataset\n                if clip_idx in hr_avenue_masked_clips:\n                    clip_score = clip_score[np.array(hr_avenue_masked_clips[clip_idx])==1]\n                    gt = gt[np.array(hr_avenue_masked_clips[clip_idx])==1]\n\n                # Abnormal score per frame\n                clip_score = score_process(clip_score, self.anomaly_score_frames_shift, self.anomaly_score_filter_kernel_size)\n                model_scores.append(clip_score)\n\n                dataset_gt.append(gt)\n\n            model_scores = np.concatenate(model_scores, axis=0)\n\n            dataset_gt = np.concatenate(dataset_gt, axis=0)\n\n            model_scores_transf[transformation] = model_scores\n            dataset_gt_transf[transformation] = dataset_gt\n\n        # aggregating the anomaly scores for all transformations\n        pds = np.mean(np.stack(list(model_scores_transf.values()), 0), 0)\n        gt = dataset_gt_transf[0]\n\n        # computing the AUC\n        auc = roc_auc_score(gt, pds)\n\n        return auc\n\n\n    def test_on_saved_tensors(self, split_name: str) -> float:\n        \"\"\"\n        Skip the prediction step and test the model on the saved tensors.\n\n        Args:\n            split_name (str): split name (val, test)\n\n        Returns:\n            float: auc score\n        \"\"\"\n\n        tensors = self._load_tensors(split_name, self.aggregation_strategy, self.n_generated_samples)\n        auc_score = self.post_processing(tensors['prediction'], tensors['gt_data'], tensors['trans'],\n                                         tensors['metadata'], tensors['frames'])\n        print(f'AUC score: {auc_score:.6f}')\n        return auc_score\n\n\n    ## Helper functions\n\n    def _aggregation_strategy(self, generated_xs: List[torch.Tensor], input_sequence: torch.Tensor, aggr_strategy: str) -> Tuple[torch.Tensor]:\n        \"\"\"\n        Aggregates the generated samples and returns the selected one and its reconstruction error.\n        Strategies:\n            - all: returns all the generated samples\n            - random: returns a random sample\n            - best: returns the sample with the lowest reconstruction loss\n            - worst: returns the sample with the highest reconstruction loss\n            - mean: returns the mean of the losses of the generated samples\n            - median: returns the median of the losses of the generated samples\n            - mean_pose: returns the mean of the generated samples\n            - median_pose: returns the median of the generated samples\n\n        Args:\n            generated_xs (List[torch.Tensor]): list of generated samples\n            input_sequence (torch.Tensor): ground truth sequence\n            aggr_strategy (str): aggregation strategy\n\n        Raises:\n            ValueError: if the aggregation strategy is not valid\n\n        Returns:\n            Tuple[torch.Tensor]: selected sample and its reconstruction error\n        \"\"\"\n\n        aggr_strategy = self.aggregation_strategy if aggr_strategy is None else aggr_strategy\n        if aggr_strategy == 'random':\n            return generated_xs[np.random.randint(len(generated_xs))], None # Added None as it was missing\n\n        B, repr_shape = input_sequence.shape[0], input_sequence.shape[1:]\n        compute_loss = lambda x: torch.mean(self.loss_fn(x, input_sequence).reshape(-1, prod(repr_shape)), dim=-1)\n        losses = [compute_loss(x) for x in generated_xs]\n\n        if aggr_strategy == 'all':\n            dims_idxs = list(range(2, len(repr_shape)+2))\n            dims_idxs = [1, 0] + dims_idxs\n            selected_x = torch.stack(generated_xs).permute(*dims_idxs)\n            loss_of_selected_x = torch.stack(losses).permute(1, 0)\n        elif aggr_strategy == 'mean':\n            selected_x = None\n            loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n        elif aggr_strategy == 'mean_pose':\n            selected_x = torch.mean(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'median':\n            loss_of_selected_x, _ = torch.median(torch.stack(losses), dim=0)\n            selected_x = None\n        elif aggr_strategy == 'median_pose':\n            selected_x, _ = torch.median(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'best' or aggr_strategy == 'worst':\n            strategy = (lambda x, y: x < y) if aggr_strategy == 'best' else (lambda x, y: x > y)\n            loss_of_selected_x = torch.full((B,), fill_value=(1e10 if aggr_strategy == 'best' else -1.), device=self.device)\n            selected_x = torch.zeros((B, *repr_shape)).to(self.device)\n\n            for i in range(len(generated_xs)):\n                mask = strategy(losses[i], loss_of_selected_x)\n                loss_of_selected_x[mask] = losses[i][mask]\n                selected_x[mask] = generated_xs[i][mask]\n        elif 'quantile' in aggr_strategy:\n            q = float(aggr_strategy.split(':')[-1])\n            loss_of_selected_x = torch.quantile(torch.stack(losses), q, dim=0)\n            selected_x = None\n        else:\n            raise ValueError(f'Unknown aggregation strategy {aggr_strategy}')\n\n        # Ensuring selected_x and loss_of_selected_x are always returned\n        if selected_x is None and loss_of_selected_x is None:\n             # Default to mean loss if strategy doesn't return both\n             loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n\n\n        return selected_x, loss_of_selected_x\n\n\n    def _infer_number_of_joint(self, args: argparse.Namespace) -> int:\n        \"\"\"\n        Infer the number of joints based on the dataset parameters.\n\n        Args:\n            args (argparse.Namespace): arguments containing the hyperparameters of the model\n\n        Returns:\n            int: number of joints\n        \"\"\"\n\n        if args.headless:\n            joints_to_consider = 14\n        elif args.kp18_format:\n            joints_to_consider = 18\n        else:\n            joints_to_consider = 17\n        return joints_to_consider\n\n\n    def _load_tensors(self, split_name: str, aggr_strategy: str, n_gen: int) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Loads the tensors from the experiment directory.\n\n        Args:\n            split_name (str): name of the split (train, val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n\n        Returns:\n            Dict[str, torch.Tensor]: dictionary containing the tensors. The keys are inferred from the file names.\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        tensor_files = os.listdir(path)\n        tensors = {}\n        for t_file in tensor_files:\n            t_name = t_file.split('.')[0]\n            tensors[t_name] = torch.load(os.path.join(path, t_file))\n        return tensors\n\n\n    def _pack_out_data(self, selected_x: torch.Tensor, loss_of_selected_x: torch.Tensor, additional_out: List[torch.Tensor], return_: str) -> List[torch.Tensor]:\n        \"\"\"\n        Packs the output data according to the return_ argument.\n\n        Args:\n            selected_x (torch.Tensor): generated samples selected among the others according to the aggregation strategy\n            loss_of_selected_x (torch.Tensor): loss of the selected samples\n            additional_out (List[torch.Tensor]): additional output data (ground truth, meta-data, etc.)\n            return_ (str): return strategy. Can be 'pose', 'loss', 'all'\n\n        Raises:\n            ValueError: if return_ is None and self.model_return_value is None\n\n        Returns:\n            List[torch.Tensor]: output data\n        \"\"\"\n\n        if return_ is None:\n            if self.model_return_value is None:\n                raise ValueError('Either return_ or self.model_return_value must be set')\n            else:\n                return_ = self.model_return_value\n\n        if return_ == 'poses':\n            out = [selected_x]\n        elif return_ == 'loss':\n            out = [loss_of_selected_x]\n        elif return_ == 'all':\n            # Check if both are available before adding to the list\n            out = []\n            if loss_of_selected_x is not None:\n                out.append(loss_of_selected_x)\n            if selected_x is not None:\n                 out.append(selected_x)\n\n        return out + additional_out\n\n\n    def _save_tensors(self, tensors: Dict[str, torch.Tensor], split_name: str, aggr_strategy: str, n_gen: int) -> None:\n        \"\"\"\n        Saves the tensors in the experiment directory.\n\n        Args:\n            tensors (Dict[str, torch.Tensor]): tensors to save\n            split_name (str): name of the split (val, test)\n            aggr_strategy (str): aggregation strategy\n            n_gen (int): number of generated samples\n        \"\"\"\n\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        for t_name, tensor in tensors.items():\n            torch.save(tensor, os.path.join(path, t_name + '.pt'))\n\n\n    def _set_diffusion_variables(self) -> None:\n        \"\"\"\n        Sets the diffusion variables.\n        \"\"\"\n\n        self.noise_scheduler = Diffusion(noise_steps=self.noise_steps, n_joints=self.n_joints,\n                                         device=self.device, time=self.n_frames)\n        self._beta_ = self.noise_scheduler.schedule_noise()\n        self._alpha_ = (1. - self._beta_)\n        self._alpha_hat_ = torch.cumprod(self._alpha_, dim=0)\n\n    def _unpack_data(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        Unpacks the data.\n\n        Args:\n            x (torch.Tensor): list containing the input data, the transformation index, the metadata and the actual frames.\n\n        Returns:\n            Tuple[torch.Tensor, List[torch.Tensor]]: input data, list containing the transformation index, the metadata and the actual frames.\n        \"\"\"\n        tensor_data = x[0].to(self.device)\n        transformation_idx = x[1]\n        metadata = x[2]\n        actual_frames = x[3]\n        meta_out = [transformation_idx, metadata, actual_frames]\n        return tensor_data, meta_out\n\n\n    @property\n    def _beta(self) -> torch.Tensor:\n        return self._beta_.to(self.device)\n\n\n    @property\n    def _alpha(self) -> torch.Tensor:\n        return self._alpha_.to(self.device)\n\n\n    @property\n    def _alpha_hat(self) -> torch.Tensor:\n        return self._alpha_hat_.to(self.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:26.132401Z","iopub.execute_input":"2025-05-26T09:40:26.132912Z","iopub.status.idle":"2025-05-26T09:40:26.149937Z","shell.execute_reply.started":"2025-05-26T09:40:26.132888Z","shell.execute_reply":"2025-05-26T09:40:26.149174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/eval_DCMD.py\nimport argparse\nimport os\n\nimport pytorch_lightning as pl\nimport yaml\nfrom models.dcmd import DCMD\nfrom utils.argparser import init_args\nfrom utils.dataset import get_dataset_and_loader\n\n\n\nif __name__== '__main__':\n    \n    # Parse command line arguments and load config file\n    parser = argparse.ArgumentParser(description='DCMD')\n    parser.add_argument('-c', '--config', type=str, required=True)\n    args = parser.parse_args()\n    args = yaml.load(open(args.config), Loader=yaml.FullLoader)\n    args = argparse.Namespace(**args)\n    args = init_args(args)\n\n    # Initialize the model\n    model = DCMD(args)\n    \n    if args.load_tensors:\n        # Load tensors and test\n        model.test_on_saved_tensors(split_name=args.split)\n    else:\n        # Load test data\n        print('Loading data and creating loaders.....')\n        ckpt_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/last.ckpt'\n        dataset, loader, _, _ = get_dataset_and_loader(args, split=args.split)\n        \n        # Initialize trainer and test\n        trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices[:1],\n                             default_root_dir=args.ckpt_dir, max_epochs=1, logger=False)\n        out = trainer.test(model, dataloaders=loader, ckpt_path=ckpt_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:40:38.829583Z","iopub.execute_input":"2025-05-26T09:40:38.829907Z","iopub.status.idle":"2025-05-26T09:40:38.835147Z","shell.execute_reply.started":"2025-05-26T09:40:38.829883Z","shell.execute_reply":"2025-05-26T09:40:38.834454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/utils/get_robust_data.py\nimport os\nimport numpy as np\nimport pickle\n\nfrom copy import deepcopy\n\nimport torch\n\nfrom utils.data import load_trajectories, extract_global_features\nfrom utils.data import change_coordinate_system, scale_trajectories, aggregate_autoencoder_data\nfrom utils.data import input_trajectories_missing_steps\nfrom utils.preprocessing import remove_short_trajectories, aggregate_rnn_autoencoder_data\n\n\ndef save_scaler(scaler, path):\n    with open(path, 'wb') as scaler_file:\n        pickle.dump(scaler, scaler_file)\n    \n        \ndef load_scaler(path):\n    with open(path, 'rb') as scaler_file:\n        scaler = pickle.load(scaler_file)\n    return scaler\n\n\n\n# Load trajectory data and convert it into a format suitable for RNN autoencoders for training and testing of joint models\ndef data_of_combined_model(**args):\n    # General\n    exp_dir = args.get('exp_dir', '')\n    split = args.get('split', 'train')\n    normalize_pose = args.get('normalize_pose', False)\n    trajectories_path = args.get('trajectories_path', '')\n    include_global = args.get('include_global', True)\n    debug = args.get('debug', False)\n    if 'train' in split:\n      # đổi thành path khác trong folder data nếu train dataset khác\n        subfolder = '/kaggle/input/avenue/Avenue/training'\n    elif 'test' in split:\n        subfolder = '/kaggle/input/avenue/Avenue/testing'\n    else:\n        subfolder = 'validating'\n    trajectories_path = os.path.join(trajectories_path, f'{subfolder}/trajectories')\n    video_resolution = args.get('vid_res', (1080,720))\n    video_resolution = np.array(video_resolution, dtype=np.float32)\n    # Architecture\n    reconstruct_original_data = args.get('reconstruct_original_data', False) \n    input_length = args.get('seg_len', 12)\n    seg_stride = args.get('seg_stride', 1) - 1 \n    pred_length = 0 \n    # Training\n    input_missing_steps = False # args.input_missing_steps\n    \n    if normalize_pose == True:\n        global_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        local_normalisation_strategy = args.get('normalization_strategy', 'robust')\n        out_normalisation_strategy = args.get('normalization_strategy', 'robust')\n\n\n    trajectories = load_trajectories(trajectories_path, debug=debug, split=split)\n    print('\\nLoaded %d trajectories.' % len(trajectories))\n\n    trajectories = remove_short_trajectories(trajectories, input_length=input_length,\n                                             input_gap=seg_stride, pred_length=pred_length)\n    print('\\nRemoved short trajectories. Number of trajectories left: %d.' % len(trajectories))\n\n    # trajectories, trajectories_val = split_into_train_and_test(trajectories, train_ratio=0.8, seed=42)\n\n    if input_missing_steps:\n        trajectories = input_trajectories_missing_steps(trajectories)\n        print('\\nInputted missing steps of trajectories.')\n\n    # Global\n    if include_global:\n        global_trajectories = extract_global_features(deepcopy(trajectories), video_resolution=video_resolution)\n\n        global_trajectories = change_coordinate_system(global_trajectories, video_resolution=video_resolution,\n                                                        coordinate_system='global', invert=False)\n\n        print('\\nChanged global trajectories\\'s coordinate system to global.')\n        \n        X_global, y_global, X_global_meta, y_global_meta = aggregate_rnn_autoencoder_data(global_trajectories, \n                                                                                        input_length=input_length,\n                                                                                        input_gap=seg_stride, pred_length=pred_length, \n                                                                                        return_ids=True)\n        \n        if normalize_pose == True:\n            # nếu test avenue thì dùng dòng này\n            #/content/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\n            # default: scaler_path = os.path.join(exp_dir, f'global_{global_normalisation_strategy}.pickle')\n            scaler_path = '/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle'\n            if split == 'train':\n                _, global_scaler = scale_trajectories(aggregate_autoencoder_data(global_trajectories),\n                                                    strategy=global_normalisation_strategy)\n                save_scaler(global_scaler, scaler_path)\n            else:\n                global_scaler = load_scaler(scaler_path)\n\n            X_global, _ = scale_trajectories(X_global, scaler=global_scaler, strategy=global_normalisation_strategy)\n            \n            if y_global is not None:\n                y_global, _ = scale_trajectories(y_global, scaler=global_scaler,\n                                                strategy=global_normalisation_strategy)\n                \n            print('\\nNormalised global trajectories using the %s normalisation strategy.' % global_normalisation_strategy)\n    \n    else:\n        X_global, X_global_meta = None, None\n    \n    # Local\n    local_trajectories = deepcopy(trajectories) if reconstruct_original_data else trajectories\n\n    local_trajectories = change_coordinate_system(local_trajectories, video_resolution=video_resolution,\n                                                  coordinate_system='bounding_box_centre', invert=False)\n\n    print('\\nChanged local trajectories\\'s coordinate system to bounding_box_centre.')\n\n    X_local, y_local, X_local_meta, y_local_meta = aggregate_rnn_autoencoder_data(local_trajectories, input_length=input_length, \n                                                                                  input_gap=seg_stride, pred_length=pred_length,\n                                                                                  return_ids=True)\n    \n    if normalize_pose == True:\n        #scaler_path = '/content/drive/MyDrive/DCMD-main-main/checkpoints/Avenue/test_experiment/local_robust.pickle'\n        scaler_path = os.path.join(exp_dir, f'local_{local_normalisation_strategy}.pickle')\n\n        if split == 'train':\n            _, local_scaler = scale_trajectories(aggregate_autoencoder_data(local_trajectories),\n                                                strategy=local_normalisation_strategy)\n            save_scaler(local_scaler, scaler_path)\n        else:\n            local_scaler = load_scaler(\"/kaggle/input/checkpoints/kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment/local_robust.pickle\")\n\n        X_local, _ = scale_trajectories(X_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n\n        if y_local is not None:\n            y_local, _ = scale_trajectories(y_local, scaler=local_scaler, strategy=local_normalisation_strategy)\n        \n        print('\\nNormalised local trajectories using the %s normalisation strategy.' % local_normalisation_strategy)\n\n    # (Optional) Reconstruct the original data\n    if reconstruct_original_data:\n        print('\\nReconstruction/Prediction target is the original data.')\n        out_trajectories = trajectories\n        \n        out_trajectories = change_coordinate_system(out_trajectories, video_resolution=video_resolution,\n                                                    coordinate_system='global', invert=False)\n    \n        print('\\nChanged target trajectories\\'s coordinate system to global.')\n        \n        scaler_path = os.path.join(exp_dir, f'out_{out_normalisation_strategy}.pickle')\n    \n        if split == 'train':\n            _, out_scaler = scale_trajectories(aggregate_autoencoder_data(out_trajectories),\n                                               strategy=out_normalisation_strategy)\n            save_scaler(out_scaler, scaler_path)\n        else:\n            out_scaler = load_scaler(scaler_path)\n        \n        ######## X_out_{}, y_out_{} numpy arrays\n\n        X_out, y_out, X_out_meta, y_out_meta = aggregate_rnn_autoencoder_data(out_trajectories, input_length=input_length, \n                                                                              input_gap=seg_stride, pred_length=pred_length,\n                                                                              return_ids=True)\n\n        X_out, _ = scale_trajectories(X_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n        \n        if y_out is not None:\n            y_out, _ = scale_trajectories(y_out, scaler=out_scaler, strategy=out_normalisation_strategy)\n            \n        print('\\nNormalised target trajectories using the %s normalisation strategy.' % out_normalisation_strategy)\n        \n            \n    if pred_length > 0:\n        \n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta), \\\n                   (y_out, y_out_meta) \n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (y_global, y_global_meta), \\\n                   (y_local, y_local_meta)\n    else:\n        if reconstruct_original_data:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta), \\\n                   (X_out, X_out_meta)\n        else:\n            return (X_global, X_global_meta), \\\n                   (X_local, X_local_meta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:00:20.245953Z","iopub.execute_input":"2025-06-04T04:00:20.246690Z","iopub.status.idle":"2025-06-04T04:00:20.255891Z","shell.execute_reply.started":"2025-06-04T04:00:20.246665Z","shell.execute_reply":"2025-06-04T04:00:20.255166Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/utils/get_robust_data.py\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"# **Train** ","metadata":{}},{"cell_type":"code","source":"!touch /kaggle/working/DCMD-main/models/__init__.py\n!touch /kaggle/working/DCMD-main/models/gcae/__init__.py\n!touch /kaggle/working/DCMD-main/models/common/__init__.py\n!touch /kaggle/working/DCMD-main/models/stsae/__init__.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:04:30.262099Z","iopub.execute_input":"2025-06-04T04:04:30.262375Z","iopub.status.idle":"2025-06-04T04:04:30.716552Z","shell.execute_reply.started":"2025-06-04T04:04:30.262351Z","shell.execute_reply":"2025-06-04T04:04:30.715751Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:11:13.704963Z","iopub.execute_input":"2025-06-04T04:11:13.705650Z","iopub.status.idle":"2025-06-04T04:11:17.886960Z","shell.execute_reply.started":"2025-06-04T04:11:13.705623Z","shell.execute_reply":"2025-06-04T04:11:17.886263Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/train_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_train.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:57:23.239608Z","iopub.execute_input":"2025-06-04T05:57:23.239977Z","iopub.status.idle":"2025-06-04T05:57:51.566759Z","shell.execute_reply.started":"2025-06-04T05:57:23.239954Z","shell.execute_reply":"2025-06-04T05:57:51.566089Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Experiment directories created in /kaggle/working/DCMD-main/checkpoints/HR-Avenue/train_experiment\n\nLoaded 2786 trajectories.\n\nRemoved short trajectories. Number of trajectories left: 1735.\n\nChanged local trajectories's coordinate system to bounding_box_centre.\n\nNormalised local trajectories using the robust normalisation strategy.\nTraceback (most recent call last):\n  File \"/kaggle/working/DCMD-main/train_DCMD.py\", line 85, in <module>\n    model = DCMD(args)\n            ^^^^^^^^^^\n  File \"/kaggle/working/DCMD-main/models/dcmd.py\", line 98, in __init__\n    self.build_model()\n  File \"/kaggle/working/DCMD-main/models/dcmd.py\", line 122, in build_model\n    self.rec_model = STSAE(\n                     ^^^^^^\n  File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 79, in __init__\n    super(STSAE, self).__init__(c_in, h_dim, latent_dim, n_frames, n_joints,\n  File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 45, in __init__\n    self.build_model()\n  File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 84, in build_model\n    super().build_model()\n  File \"/kaggle/working/DCMD-main/models/stsae/stsae.py\", line 48, in build_model\n    self.encoder = build_encoder(self._cfg)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/DCMD-main/models/common/components.py\", line 33, in build_encoder\n    return EnhancedEncoder(\n           ^^^^^^^^^^^^^^^^\nTypeError: EnhancedEncoder.__init__() got an unexpected keyword argument 'emb_dim'\n","output_type":"stream"}],"execution_count":142},{"cell_type":"markdown","source":"If download checkpoint needed, run this code and download checkpoints.zip file in Output","metadata":{}},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n# 2. (tuỳ chọn) kiểm tra xem zip đã tạo xong chưa\n!ls -lh /kaggle/working/checkpoints.zip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/DCMD-main/checkpoints/HR-Avenue/test_experiment', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T09:43:30.055859Z","iopub.execute_input":"2025-05-26T09:43:30.056096Z","iopub.status.idle":"2025-05-26T09:43:30.059988Z","shell.execute_reply.started":"2025-05-26T09:43:30.056080Z","shell.execute_reply":"2025-05-26T09:43:30.059235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/DCMD-main/eval_DCMD.py --config /kaggle/working/DCMD-main/config/Avenue/dcmd_test.yaml","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-05-26T09:43:32.799055Z","iopub.execute_input":"2025-05-26T09:43:32.799593Z","iopub.status.idle":"2025-05-26T13:05:29.018946Z","shell.execute_reply.started":"2025-05-26T09:43:32.799569Z","shell.execute_reply":"2025-05-26T13:05:29.017808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Zip toàn bộ folder checkpoints\n!zip -r /kaggle/working/checkpoints.zip /kaggle/working/DCMD-main/checkpoints\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T13:10:50.210286Z","iopub.execute_input":"2025-05-26T13:10:50.210565Z","iopub.status.idle":"2025-05-26T13:10:59.549864Z","shell.execute_reply.started":"2025-05-26T13:10:50.210540Z","shell.execute_reply":"2025-05-26T13:10:59.549175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Chỉnh phương pháp**","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/stsgcn.py\n# Bạn có thể đăng code ST-SGCN gốc ở đây, hoặc để trống nếu không dùng.\nclass STSGCN:\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError(\"STSGCN chưa được định nghĩa.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:12:02.488205Z","iopub.execute_input":"2025-06-04T05:12:02.488960Z","iopub.status.idle":"2025-06-04T05:12:02.494427Z","shell.execute_reply.started":"2025-06-04T05:12:02.488932Z","shell.execute_reply":"2025-06-04T05:12:02.493576Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/gcae/stsgcn.py\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/stsae/stsae.py\n\nimport torch\nimport torch.nn as nn\nfrom typing import List, Union\nfrom argparse import Namespace\nfrom models.common.components import build_encoder, build_decoder\n\nclass STSE(nn.Module):\n    def __init__(self, c_in: int, h_dim: int, latent_dim: int, n_frames: int,\n                 n_joints: int, layer_channels: List[int], dropout: float,\n                 device: Union[str, torch.device], use_adaptive=False,\n                 use_jigsaw=False, emb_dim=None) -> None:\n        super(STSE, self).__init__()\n\n        # Save config to pass to decoder\n        self._cfg = Namespace(\n                    input_dim=c_in,\n                    in_channels=c_in,\n                    h_dim=h_dim,\n                    latent_dim=latent_dim,\n                    n_frames=n_frames,\n                    n_joints=n_joints,\n                    layer_channels=layer_channels,\n                    dropout=dropout,\n                    device=device,\n                    emb_dim=emb_dim,\n                    use_adaptive=use_adaptive,\n                    use_jigsaw=use_jigsaw\n                )\n\n        # Set attributes\n        self.input_dim = c_in\n        self.h_dim = h_dim\n        self.latent_dim = latent_dim\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.layer_channels = layer_channels\n        self.dropout = dropout\n        self.device = device\n        self.use_adaptive = use_adaptive\n        self.use_jigsaw = use_jigsaw\n        self.emb_dim = emb_dim\n\n        # Build model\n        self.build_model()\n\n    def build_model(self):\n        self.encoder = build_encoder(self._cfg)\n        self.btlnk = nn.Linear(self.h_dim * self.n_frames * self.n_joints, self.latent_dim)\n\n    def encode(self, X: torch.Tensor, return_shape: bool = False, t: torch.Tensor = None):\n        X = X.unsqueeze(4)\n        N, C, T, V, M = X.size()\n        X = X.permute(0, 4, 3, 1, 2).contiguous()\n        X = X.view(N * M, V, C, T).permute(0, 2, 3, 1).contiguous()\n\n        X, *_ = self.encoder(X, t)\n        N, C, T, V = X.size()\n        X = X.view(N, -1).contiguous()\n        X = X.view(N, M, self.h_dim, T, V).permute(0, 2, 3, 4, 1).contiguous()\n        X_shape = X.size()\n        X = X.view(N, -1).contiguous()\n\n        X = self.btlnk(X)\n\n        if return_shape:\n            return X, X_shape\n        return X\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None):\n        return self.encode(X, return_shape=False, t=t), None\n\n\nclass STSAE(STSE):\n    def __init__(self, c_in: int, h_dim: int, latent_dim: int, n_frames: int,\n                 n_joints: int, layer_channels: List[int], dropout: float,\n                 device: Union[str, torch.device], use_adaptive=False,\n                 use_jigsaw=False, emb_dim=None) -> None:\n        super(STSAE, self).__init__(c_in, h_dim, latent_dim, n_frames, n_joints,\n                                    layer_channels, dropout, device, use_adaptive,\n                                    use_jigsaw, emb_dim)\n\n    def build_model(self):\n        super().build_model()\n        self.decoder = build_decoder(self._cfg)\n        self.rev_btlnk = nn.Linear(self.latent_dim, self.h_dim * self.n_frames * self.n_joints)\n\n    def decode(self, Z: torch.Tensor, input_shape, t: torch.Tensor = None):\n        Z = self.rev_btlnk(Z)\n        N, C, T, V, M = input_shape\n        Z = Z.view(input_shape).contiguous()\n        Z = Z.permute(0, 4, 1, 2, 3).contiguous()\n        Z = Z.view(N * M, C, T, V).contiguous()\n        Z = self.decoder(Z, t)\n        return Z\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None):\n        hidden_X, X_shape = self.encode(X, return_shape=True, t=t)\n        X = self.decode(hidden_X, X_shape, t)\n        return hidden_X, X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:36:24.360518Z","iopub.execute_input":"2025-06-04T05:36:24.361225Z","iopub.status.idle":"2025-06-04T05:36:24.367914Z","shell.execute_reply.started":"2025-06-04T05:36:24.361195Z","shell.execute_reply":"2025-06-04T05:36:24.367200Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/stsae/stsae.py\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/enhanced_stgcn.py\nimport math\nimport random\nimport itertools\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport networkx as nx\nfrom networkx.algorithms.community import girvan_newman\n\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.utils import dense_to_sparse\nfrom typing import Union, List, Tuple\n\n# --- Adaptive + Jigsaw (GDN) những hàm tiện ích ---\ndef create_graph_from_adjacency_matrix(adj_matrix):\n    G = nx.DiGraph()\n    for i, targets in enumerate(adj_matrix):\n        for j in targets:\n            if i != j:\n                G.add_edge(i, j.item())\n    return G\n\ndef detect_communities_girvan_newman(G, target_communities=4):\n    try:\n        G_undirected = G.to_undirected()\n        comm_iter = girvan_newman(G_undirected)\n        for communities in itertools.islice(comm_iter, target_communities-1):\n            if len(communities) >= target_communities:\n                return communities\n    except:\n        return None\n\ndef flip_two_communities_directed(adj_matrix, communities, community_id1, community_id2):\n    flipped_adj_matrix = np.copy(adj_matrix)\n    nodes1 = list(communities[community_id1])\n    nodes2 = list(communities[community_id2])\n    min_len = min(len(nodes1), len(nodes2))\n    nodes1, nodes2 = nodes1[:min_len], nodes2[:min_len]\n    node_mapping = {node: node for node in range(len(adj_matrix))}\n    for n1, n2 in zip(nodes1, nodes2):\n        node_mapping[n1] = n2\n        node_mapping[n2] = n1\n    for i in range(len(adj_matrix)):\n        for j in range(len(adj_matrix)):\n            flipped_adj_matrix[node_mapping[i], node_mapping[j]] = adj_matrix[i, j]\n    return flipped_adj_matrix\n\ndef adjacency_matrix_to_list(flipped_adj_matrix, topk):\n    adjacency_list = []\n    for i, row in enumerate(flipped_adj_matrix):\n        connected_nodes = np.nonzero(row)[0]\n        filtered_nodes = [node for node in connected_nodes if node != i][:topk-1]\n        filtered_nodes.insert(0, i)\n        if len(filtered_nodes) < topk:\n            filtered_nodes.extend([i] * (topk - len(filtered_nodes)))\n        adjacency_list.append(filtered_nodes[:topk])\n    return adjacency_list\n\n# --- Adaptive Graph Convolution with Jigsaw (GDN) ---\n\nclass AdaptiveGraphConv(nn.Module):\n    def __init__(self, time_dim, joints_dim, embed_dim=64, topk=8, use_jigsaw=True, jigsaw_prob=0.3):\n        super().__init__()\n        self.time_dim = time_dim\n        self.joints_dim = joints_dim\n        self.embed_dim = embed_dim\n        self.topk = topk\n        self.use_jigsaw = use_jigsaw\n        self.jigsaw_prob = jigsaw_prob\n\n        self.A = nn.Parameter(torch.FloatTensor(time_dim, joints_dim, joints_dim))\n        self.T = nn.Parameter(torch.FloatTensor(joints_dim, time_dim, time_dim))\n\n        self.node_embedding = nn.Embedding(joints_dim, embed_dim)\n        self.adaptive_weight = nn.Parameter(torch.FloatTensor(1))\n\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads=4, batch_first=True)\n\n        if use_jigsaw:\n            self.puzzle_classifier = nn.Sequential(\n                nn.AdaptiveAvgPool1d(1),\n                nn.Flatten(),\n                nn.Linear(embed_dim, 32),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(32, 7)\n            )\n\n        self.init_params()\n\n    def init_params(self):\n        stdv = 1. / math.sqrt(self.A.size(1))\n        self.A.data.uniform_(-stdv, stdv)\n        stdv = 1. / math.sqrt(self.T.size(1))\n        self.T.data.uniform_(-stdv, stdv)\n        nn.init.kaiming_uniform_(self.node_embedding.weight, a=math.sqrt(5))\n        nn.init.constant_(self.adaptive_weight, 0.5)\n\n    def get_adaptive_adjacency(self, device):\n        all_embeddings = self.node_embedding(torch.arange(self.joints_dim).to(device))\n        weights = all_embeddings.view(self.joints_dim, -1)\n        cos_ji_mat = torch.matmul(weights, weights.T)\n        normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n        cos_ji_mat = cos_ji_mat / (normed_mat + 1e-8)\n        topk_indices = torch.topk(cos_ji_mat, self.topk, dim=-1)[1]\n        return topk_indices, all_embeddings\n\n    def apply_jigsaw_puzzle(self, adj_indices):\n        if not (self.training and self.use_jigsaw and random.random() < self.jigsaw_prob):\n            return adj_indices, 6\n        try:\n            X1 = adj_indices.cpu().numpy()\n            G = create_graph_from_adjacency_matrix(X1)\n            partition = detect_communities_girvan_newman(G, target_communities=4)\n            if partition is None or len(partition) < 4:\n                return adj_indices, 6\n            communities = {i: list(c) for i, c in enumerate(partition)}\n            community_pairs = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n            selected_combination = random.randint(0, 5)\n            community_id1, community_id2 = community_pairs[selected_combination]\n            adj_matrix_directed = np.zeros((len(X1), len(X1)))\n            for ii, targets in enumerate(X1):\n                for jj in targets:\n                    if ii != jj:\n                        adj_matrix_directed[ii, jj] = 1\n            flipped_adj_matrix = flip_two_communities_directed(\n                adj_matrix_directed, communities, community_id1, community_id2\n            )\n            flipped_adj_list = adjacency_matrix_to_list(flipped_adj_matrix, self.topk)\n            output = torch.tensor(flipped_adj_list, dtype=adj_indices.dtype).to(adj_indices.device)\n            return output, selected_combination\n        except:\n            return adj_indices, 6\n\n    def forward(self, X):\n        device = X.device\n        batch_size = X.shape[0]\n        adj_indices, node_embeddings = self.get_adaptive_adjacency(device)\n        adj_indices, puzzle_label = self.apply_jigsaw_puzzle(adj_indices)\n\n        adaptive_A = torch.zeros(self.time_dim, self.joints_dim, self.joints_dim, device=device)\n        for t in range(self.time_dim):\n            for i, neighbors in enumerate(adj_indices):\n                adaptive_A[t, i, neighbors] = 1.0\n        combined_A = (1 - self.adaptive_weight) * self.A + self.adaptive_weight * adaptive_A\n\n        X_temp = torch.einsum('nctv,vtq->ncqv', (X, self.T)).contiguous()\n        X_out = torch.einsum('nctv,tvw->nctw', (X_temp, combined_A)).contiguous()\n\n        puzzle_pred = None\n        if self.use_jigsaw and hasattr(self, 'puzzle_classifier'):\n            embed_features = node_embeddings.mean(dim=0, keepdim=True).expand(batch_size, -1)\n            puzzle_pred = self.puzzle_classifier(embed_features.unsqueeze(-1))\n\n        return X_out, puzzle_pred, puzzle_label\n\n\nclass Enhanced_ST_GCNN_layer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size: Union[Tuple[int], List[int]],\n                 stride, time_dim, joints_dim, dropout, bias=True,\n                 emb_dim=None, use_adaptive=True, use_jigsaw=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.stride = stride\n        self.time_dim = time_dim\n        self.joints_dim = joints_dim\n        self.dropout = dropout\n        self.bias = bias\n        self.emb_dim = emb_dim\n        self.kernel_size = kernel_size\n        self.use_adaptive = use_adaptive\n        self.use_jigsaw = use_jigsaw\n        self.build_model()\n\n    def build_model(self):\n        kernel_size = self.kernel_size  # <== FIX HERE\n        padding = ((kernel_size[0] - 1) // 2, (kernel_size[1] - 1) // 2)\n\n        if self.use_adaptive:\n            self.gcn = AdaptiveGraphConv(\n                self.time_dim, self.joints_dim,\n                embed_dim=64, topk=8, use_jigsaw=self.use_jigsaw\n            )\n        else:\n            self.gcn = ConvTemporalGraphical(self.time_dim, self.joints_dim)\n\n        self.tcn = nn.Sequential(\n            nn.Conv2d(self.in_channels, self.out_channels, kernel_size, (self.stride, self.stride), padding, bias=self.bias),\n            nn.BatchNorm2d(self.out_channels),\n            nn.Dropout(self.dropout, inplace=True),\n        )\n\n        if self.stride != 1 or self.in_channels != self.out_channels:\n            self.residual = nn.Sequential(\n                nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=(1, 1), bias=self.bias),\n                nn.BatchNorm2d(self.out_channels)\n            )\n        else:\n            self.residual = nn.Identity()\n\n        self.prelu = nn.PReLU()\n\n        if self.emb_dim is not None:\n            self.emb_layer = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(self.emb_dim, self.out_channels),\n            )\n\n    def forward(self, X, t=None):\n        res = self.residual(X)\n        if self.use_adaptive:\n            X, puzzle_pred, puzzle_label = self.gcn(X)\n        else:\n            X = self.gcn(X)\n            puzzle_pred, puzzle_label = None, None\n\n        X = self.tcn(X)\n        X = X + res\n        X = self.prelu(X)\n\n        if self.emb_dim is not None and t is not None:\n            emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, X.shape[-2], X.shape[-1]).contiguous()\n            X = X + emb\n\n        return X.contiguous(), puzzle_pred, puzzle_label\n\nclass ConvTemporalGraphical(nn.Module):\n    def __init__(self, time_dim, joints_dim):\n        super().__init__()\n        self.A = nn.Parameter(torch.FloatTensor(time_dim, joints_dim, joints_dim))\n        stdv = 1. / math.sqrt(self.A.size(1))\n        self.A.data.uniform_(-stdv, stdv)\n        self.T = nn.Parameter(torch.FloatTensor(joints_dim, time_dim, time_dim))\n        stdv = 1. / math.sqrt(self.T.size(1))\n        self.T.data.uniform_(-stdv, stdv)\n\n    def forward(self, X):\n        X = torch.einsum('nctv,vtq->ncqv', (X, self.T)).contiguous()\n        X = torch.einsum('nctv,tvw->nctw', (X, self.A)).contiguous()\n        return X\n\n\nclass EnhancedEncoder(nn.Module):\n    \"\"\"\n    Lớp Encoder “Enhanced” dùng GDN (Adaptive + Jigsaw).\n    \"\"\"\n    def __init__(self,\n                 input_dim: int,\n                 layer_channels: List[int],\n                 hidden_dimension: int,\n                 n_frames: int,\n                 n_joints: int,\n                 dropout: float,\n                 bias: bool = True,\n                 use_adaptive: bool = True,\n                 use_jigsaw: bool = True):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.layer_channels = layer_channels\n        self.hidden_dimension = hidden_dimension\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.dropout = dropout\n        self.bias = bias\n        self.use_adaptive = use_adaptive\n        self.use_jigsaw = use_jigsaw\n\n        self.build_model()\n\n    def build_model(self):\n        input_channels = self.input_dim\n        layer_channels = self.layer_channels + [self.hidden_dimension]\n        kernel_size = [1, 1]\n        stride = 1\n        model_layers = nn.ModuleList()\n\n        for i, channels in enumerate(layer_channels):\n            if i == 0 and self.use_adaptive:\n                model_layers.append(\n                    Enhanced_ST_GCNN_layer(\n                        in_channels=input_channels,\n                        out_channels=channels,\n                        kernel_size=kernel_size,\n                        stride=stride,\n                        time_dim=self.n_frames,\n                        joints_dim=self.n_joints,\n                        dropout=self.dropout,\n                        bias=self.bias,\n                        emb_dim=self.hidden_dimension,\n                        use_adaptive=True,\n                        use_jigsaw=self.use_jigsaw\n                    )\n                )\n            else:\n                model_layers.append(\n                    Enhanced_ST_GCNN_layer(\n                        in_channels=input_channels,\n                        out_channels=channels,\n                        kernel_size=kernel_size,\n                        stride=stride,\n                        time_dim=self.n_frames,\n                        joints_dim=self.n_joints,\n                        dropout=self.dropout,\n                        bias=self.bias,\n                        emb_dim=None,\n                        use_adaptive=False,\n                        use_jigsaw=False\n                    )\n                )\n            input_channels = channels\n\n        self.model_layers = model_layers\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None):\n        layers_out = [X]\n        puzzle_preds = []\n        puzzle_labels = []\n\n        for layer in self.model_layers:\n            if hasattr(layer, 'use_adaptive') and layer.use_adaptive:\n                out_X, puzzle_pred, puzzle_label = layer(layers_out[-1], t)\n                if puzzle_pred is not None:\n                    puzzle_preds.append(puzzle_pred)\n                if puzzle_label is not None:\n                    puzzle_labels.append(puzzle_label)\n            else:\n                out_X, _, _ = layer(layers_out[-1], t)\n            layers_out.append(out_X)\n\n        return layers_out[-1], layers_out[:-1], puzzle_preds, puzzle_labels\n\n\nclass EnhancedDCMDLoss(nn.Module):\n    \"\"\"\n    Loss chính + auxiliary Jigsaw puzzle.\n    \"\"\"\n    def __init__(self, main_loss_weight: float = 1.0, puzzle_loss_weight: float = 0.1):\n        super(EnhancedDCMDLoss, self).__init__()\n        self.main_loss_weight = main_loss_weight\n        self.puzzle_loss_weight = puzzle_loss_weight\n        self.puzzle_criterion = nn.CrossEntropyLoss()\n\n    def forward(self,\n                main_pred: torch.Tensor,\n                main_target: torch.Tensor,\n                puzzle_preds: List[torch.Tensor] = None,\n                puzzle_labels: List = None):\n        main_loss = F.mse_loss(main_pred, main_target)\n        total_loss = self.main_loss_weight * main_loss\n\n        puzzle_loss = torch.tensor(0.0, device=main_pred.device)\n        if puzzle_preds and puzzle_labels and len(puzzle_preds) > 0 and len(puzzle_labels) > 0:\n            for pred, label in zip(puzzle_preds, puzzle_labels):\n                if isinstance(label, (int, list)):\n                    if isinstance(label, list):\n                        label_tensor = torch.tensor(label, device=pred.device)\n                    else:\n                        label_tensor = torch.tensor([label] * pred.shape[0], device=pred.device)\n                    puzzle_loss += self.puzzle_criterion(pred, label_tensor)\n            total_loss += self.puzzle_loss_weight * puzzle_loss\n\n        return total_loss, main_loss, puzzle_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:57:15.787756Z","iopub.execute_input":"2025-06-04T05:57:15.788390Z","iopub.status.idle":"2025-06-04T05:57:15.798893Z","shell.execute_reply.started":"2025-06-04T05:57:15.788362Z","shell.execute_reply":"2025-06-04T05:57:15.798312Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/gcae/enhanced_stgcn.py\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/graph_layer.py\nimport torch\nfrom torch.nn import Parameter, Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import remove_self_loops, add_self_loops, softmax\nfrom torch_geometric.nn.inits import glorot, zeros\n\n\nclass GraphLayer(MessagePassing):\n    def __init__(self, in_channels, out_channels, heads=1, concat=True,\n                 negative_slope=0.2, dropout=0, bias=True, inter_dim=-1, **kwargs):\n        super(GraphLayer, self).__init__(aggr='add', **kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.heads = heads\n        self.concat = concat\n        self.negative_slope = negative_slope\n        self.dropout = dropout\n\n        self.__alpha__ = None\n\n        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n\n        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n\n        if bias and concat:\n            self.bias = Parameter(torch.Tensor(heads * out_channels))\n        elif bias and not concat:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        glorot(self.lin.weight)\n        glorot(self.att_i)\n        glorot(self.att_j)\n        zeros(self.att_em_i)\n        zeros(self.att_em_j)\n        zeros(self.bias)\n\n    def forward(self, x, edge_index, embedding=None, return_attention_weights=False):\n        \"\"\"\n        x: node features (num_nodes, in_channels) hoặc tuple cho graph đôi (x_src, x_dst)\n        edge_index: (2, num_edges)\n        embedding: nếu có, shape (num_nodes, emb_dim)\n        return_attention_weights: nếu True, trả thêm attention weights\n        \"\"\"\n        if torch.is_tensor(x):\n            x = self.lin(x)\n            x = (x, x)\n        else:\n            x = (self.lin(x[0]), self.lin(x[1]))\n\n        edge_index, _ = remove_self_loops(edge_index)\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n\n        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index,\n                             return_attention_weights=return_attention_weights)\n\n        if self.concat:\n            out = out.view(-1, self.heads * self.out_channels)\n        else:\n            out = out.mean(dim=1)\n\n        if self.bias is not None:\n            out = out + self.bias\n\n        if return_attention_weights:\n            alpha, self.__alpha__ = self.__alpha__, None\n            return out, (edge_index, alpha)\n        else:\n            return out\n\n    def message(self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights):\n        \"\"\"\n        x_i, x_j: (num_edges, heads * out_channels) đã reshape thành (num_edges, heads, out_channels)\n        edge_index_i: first row of edge_index\n        embedding: (num_nodes, emb_dim) hoặc None\n        edges: toàn bộ edge_index\n        return_attention_weights: bool\n        \"\"\"\n        x_i = x_i.view(-1, self.heads, self.out_channels)\n        x_j = x_j.view(-1, self.heads, self.out_channels)\n\n        if embedding is not None:\n            emb_i = embedding[edge_index_i]      # (num_edges, emb_dim)\n            emb_j = embedding[edges[0]]          # (num_edges, emb_dim)\n            emb_i = emb_i.unsqueeze(1).repeat(1, self.heads, 1)  # (num_edges, heads, emb_dim)\n            emb_j = emb_j.unsqueeze(1).repeat(1, self.heads, 1)\n\n            key_i = torch.cat((x_i, emb_i), dim=-1)\n            key_j = torch.cat((x_j, emb_j), dim=-1)\n\n            cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)  # (1, heads, out_ch+emb_dim)\n            cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n\n            alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1)  # (num_edges, heads)\n        else:\n            cat_att_i = self.att_i\n            cat_att_j = self.att_j\n            alpha = (x_i * cat_att_i).sum(-1) + (x_j * cat_att_j).sum(-1)\n\n        alpha = alpha.view(-1, self.heads, 1)\n        alpha = F.leaky_relu(alpha, self.negative_slope)\n        self.node_dim = 0\n        torch.use_deterministic_algorithms(False)\n        alpha = softmax(alpha, edge_index_i, num_nodes=size_i)\n        torch.use_deterministic_algorithms(True)\n\n        if return_attention_weights:\n            self.__alpha__ = alpha\n\n        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n        return x_j * alpha.view(-1, self.heads, 1)\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.in_channels}, {self.out_channels}, heads={self.heads})'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:46:37.696455Z","iopub.execute_input":"2025-06-04T04:46:37.696987Z","iopub.status.idle":"2025-06-04T04:46:37.703518Z","shell.execute_reply.started":"2025-06-04T04:46:37.696964Z","shell.execute_reply":"2025-06-04T04:46:37.702756Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/gcae/graph_layer.py\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/gcae/forecasting_gat.py\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.utils import dense_to_sparse\n\nfrom models.gcae.graph_layer import GraphLayer\n\n\nclass GATForecast(nn.Module):\n    \"\"\"\n    Dùng GraphLayer (GAT) + TCN để dự báo frame kế tiếp.\n    Input: x_hist: (N, C, T, V)\n    Output: pred_next: (N, C, V)\n    \"\"\"\n\n    def __init__(self,\n                 in_channels: int,\n                 num_nodes: int,\n                 hidden_dim: int,\n                 heads: int = 4,\n                 dropout: float = 0.2,\n                 use_embedding: bool = False):\n        super(GATForecast, self).__init__()\n        self.in_channels = in_channels\n        self.num_nodes = num_nodes\n        self.hidden_dim = hidden_dim\n        self.heads = heads\n        self.dropout = dropout\n        self.use_embedding = use_embedding\n\n        # 1) GraphLayer: input_dim = in_channels, out_channels = hidden_dim, heads=heads\n        self.graph_conv = GraphLayer(\n            in_channels=in_channels,\n            out_channels=hidden_dim,\n            heads=heads,\n            concat=True,\n            negative_slope=0.2,\n            dropout=dropout,\n            bias=True\n        )\n\n        # 2) Conv1d để gom thông tin theo T\n        self.tcn = nn.Sequential(\n            nn.Conv1d(\n                in_channels=hidden_dim * heads,\n                out_channels=hidden_dim * heads,\n                kernel_size=3,\n                padding=1\n            ),\n            nn.BatchNorm1d(hidden_dim * heads),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout, inplace=True)\n        )\n\n        # 3) FC map ẩn → in_channels (xyz)\n        self.fc_out = nn.Linear(hidden_dim * heads, in_channels)\n\n    def forward(self, x_hist: torch.Tensor, A: torch.Tensor, embedding: torch.Tensor = None):\n        \"\"\"\n        x_hist: (N, C, T, V)\n        A: adjacency dense (V, V)\n        embedding: nếu có, shape (V, emb_dim)\n        \"\"\"\n        N, C, T, V = x_hist.shape\n\n        # 1) Tạo batched node features: (N, T, V, C) → (N*T*V, C)\n        x_perm = x_hist.permute(0, 2, 3, 1).contiguous()  # (N, T, V, C)\n        x_nodes = x_perm.view(N * T * V, C)               # (N*T*V, C)\n\n        # 2) Tạo edge_index từ A (dense) rồi batch cho N*T\n        edge_index, _ = dense_to_sparse(A)  # (2, E)\n        num_graphs = N * T\n        node_offsets = torch.arange(0, num_graphs * V, step=V, device=A.device)\n\n        # Mở rộng edge_index\n        edge0 = edge_index[0].repeat(num_graphs) + node_offsets.repeat_interleave(edge_index.size(1))\n        edge1 = edge_index[1].repeat(num_graphs) + node_offsets.repeat_interleave(edge_index.size(1))\n        batch_edge_index = torch.stack([edge0, edge1], dim=0)  # (2, num_graphs*E)\n\n        # 3) GraphLayer\n        if self.use_embedding and (embedding is not None):\n            emb = embedding.repeat(N * T, 1)  # (N*T*V, emb_dim) — tùy kiểu embedding\n            x_gc = self.graph_conv(x_nodes, batch_edge_index, emb)  # (N*T*V, hidden_dim*heads)\n        else:\n            x_gc = self.graph_conv(x_nodes, batch_edge_index)      # (N*T*V, hidden_dim*heads)\n\n        # 4) Reshape → (N, V, hidden_dim*heads, T)\n        x_gc = x_gc.view(N, T, V, -1).permute(0, 2, 3, 1).contiguous()  # (N, V, H, T)\n        x_gc = x_gc.view(N * V, -1, T)  # (N*V, H, T)\n\n        # 5) TCN 1D theo chiều T\n        x_tcn = self.tcn(x_gc)  # (N*V, H, T)\n        feat_last = x_tcn[:, :, -1]  # lấy cuối cùng → (N*V, H)\n\n        # 6) FC → (N*V, C) → reshape về (N, V, C) → (N, C, V)\n        pred_flat = self.fc_out(feat_last)  # (N*V, C)\n        pred = pred_flat.view(N, V, C)      # (N, V, C)\n        pred = pred.permute(0, 2, 1).contiguous()  # (N, C, V)\n        return pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T04:46:41.238634Z","iopub.execute_input":"2025-06-04T04:46:41.239201Z","iopub.status.idle":"2025-06-04T04:46:41.245556Z","shell.execute_reply.started":"2025-06-04T04:46:41.239177Z","shell.execute_reply":"2025-06-04T04:46:41.244861Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/gcae/forecasting_gat.py\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/common/components.py\nimport torch\nimport torch.nn as nn\nfrom typing import List, Tuple, Union\n\n# Nếu bạn có STSGCN gốc trong stsgcn.py, import vào đây (nếu không có, đặt STSGCN = None)\ntry:\n    from models.gcae.stsgcn import STSGCN\nexcept ImportError:\n    STSGCN = None\n\n# Import Enhanced components\ntry:\n    from models.gcae.enhanced_stgcn import EnhancedEncoder, EnhancedDCMDLoss\nexcept ImportError:\n    EnhancedEncoder = None\n    EnhancedDCMDLoss = None\n\n# Import module forecasting (GATForecast) từ forecasting_gat.py\ntry:\n    from models.gcae.forecasting_gat import GATForecast\nexcept ImportError:\n    GATForecast = None\n\n\ndef build_encoder(cfg):\n    \"\"\"\n    Tạo encoder thích hợp:\n    - Nếu dùng adaptive + jigsaw thì dùng EnhancedEncoder.\n    - Nếu không thì dùng encoder gốc (bạn có thể định nghĩa riêng, hoặc mặc định trả về None).\n    \"\"\"\n    if getattr(cfg, 'use_adaptive', False):\n        from models.gcae.enhanced_stgcn import EnhancedEncoder  # đảm bảo bạn có file này\n        return EnhancedEncoder(\n            input_dim=cfg.input_dim,\n            layer_channels=cfg.layer_channels,\n            hidden_dimension=cfg.h_dim,\n            n_frames=cfg.n_frames,\n            n_joints=cfg.n_joints,\n            dropout=cfg.dropout,\n            emb_dim=getattr(cfg, 'emb_dim', None),\n            use_adaptive=True,\n            use_jigsaw=getattr(cfg, 'use_jigsaw', False)\n        )\n    else:\n        from models.gcae.stsgcn import STSGCN\n        return STSGCN(\n            input_dim=cfg.input_dim,\n            layer_channels=cfg.layer_channels,\n            hidden_dimension=cfg.h_dim,\n            n_frames=cfg.n_frames,\n            n_joints=cfg.n_joints,\n            dropout=cfg.dropout\n        )\n\n\ndef build_decoder(cfg):\n    \"\"\"\n    Nếu bạn có decoder gốc (ví dụ STSGCN-based decoder), import và trả về ở đây.\n    Nếu không, trả None.\n    \"\"\"\n    return None\n\n\ndef build_loss(cfg):\n    \"\"\"\n    Nếu cfg.use_adaptive (hoặc cfg.use_jigsaw) ⇒ trả EnhancedDCMDLoss,\n    ngược lại ⇒ trả nn.MSELoss().\n    \"\"\"\n    if getattr(cfg, 'use_adaptive', False):\n        if EnhancedDCMDLoss is None:\n            raise ImportError(\"EnhancedDCMDLoss không tìm thấy tại models/gcae/enhanced_stgcn.py\")\n        return EnhancedDCMDLoss(\n            main_loss_weight=cfg.loss_1_prior_weight,    # hoặc cfg.main_loss_weight do bạn định nghĩa\n            puzzle_loss_weight=cfg.loss_2_prior_weight   # hoặc cfg.puzzle_loss_weight\n        )\n    else:\n        return nn.MSELoss()\n\n\ndef build_forecasting(cfg):\n    \"\"\"\n    Tạo và trả GATForecast nếu cfg.use_forecasting == True.\n    \"\"\"\n    if GATForecast is None:\n        raise ImportError(\"GATForecast không tìm thấy tại models/gcae/forecasting_gat.py\")\n    return GATForecast(\n        in_channels=cfg.channels,\n        num_nodes=cfg.n_joints,\n        hidden_dim=cfg.hidden_dim_forecast,\n        heads=cfg.num_heads_gat,\n        dropout=cfg.dropout_forecast,\n        use_embedding=False\n    )\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Lớp STS-GCN gốc, dùng nếu bạn có stsgcn.ST_GCNN_layer.\n    \"\"\"\n    def __init__(self,\n                 input_dim: int,\n                 layer_channels: List[int],\n                 hidden_dimension: int,\n                 n_frames: int,\n                 n_joints: int,\n                 dropout: float,\n                 bias: bool = True) -> None:\n        super().__init__()\n\n        try:\n            import models.gcae.stsgcn as stsgcn\n        except ImportError:\n            stsgcn = None\n\n        self.input_dim = input_dim\n        self.layer_channels = layer_channels\n        self.hidden_dimension = hidden_dimension\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.dropout = dropout\n        self.bias = bias\n\n        self.build_model()\n\n    def build_model(self):\n        try:\n            import models.gcae.stsgcn as stsgcn\n        except ImportError:\n            raise ImportError(\"Không tìm thấy stsgcn.py hoặc ST_GCNN_layer\")\n\n        input_channels = self.input_dim\n        layer_channels = self.layer_channels + [self.hidden_dimension]\n        kernel_size = [1, 1]\n        stride = 1\n        model_layers = nn.ModuleList()\n        for channels in layer_channels:\n            model_layers.append(\n                stsgcn.ST_GCNN_layer(\n                    in_channels=input_channels,\n                    out_channels=channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    time_dim=self.n_frames,\n                    joints_dim=self.n_joints,\n                    dropout=self.dropout,\n                    bias=self.bias\n                )\n            )\n            input_channels = channels\n\n        self.model_layers = model_layers\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        layers_out = [X]\n        for layer in self.model_layers:\n            out_X = layer(layers_out[-1], t)\n            layers_out.append(out_X)\n        return layers_out[-1], layers_out[:-1]\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    Lớp Decoder gốc (STS-GCN) nếu dùng.\n    \"\"\"\n    def __init__(self,\n                 output_dim: int,\n                 layer_channels: List[int],\n                 hidden_dimension: int,\n                 n_frames: int,\n                 n_joints: int,\n                 dropout: float,\n                 bias: bool = True) -> None:\n        super().__init__()\n\n        try:\n            import models.gcae.stsgcn as stsgcn\n        except ImportError:\n            stsgcn = None\n\n        self.output_dim = output_dim\n        self.layer_channels = layer_channels[::-1]\n        self.hidden_dimension = hidden_dimension\n        self.n_frames = n_frames\n        self.n_joints = n_joints\n        self.dropout = dropout\n        self.bias = bias\n\n        self.build_model()\n\n    def build_model(self):\n        try:\n            import models.gcae.stsgcn as stsgcn\n        except ImportError:\n            raise ImportError(\"Không tìm thấy stsgcn.py hoặc ST_GCNN_layer\")\n\n        input_channels = self.hidden_dimension\n        layer_channels = self.layer_channels + [self.output_dim]\n        kernel_size = [1, 1]\n        stride = 1\n        model_layers = nn.ModuleList()\n        for channels in layer_channels:\n            model_layers.append(\n                stsgcn.ST_GCNN_layer(\n                    in_channels=input_channels,\n                    out_channels=channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    time_dim=self.n_frames,\n                    joints_dim=self.n_joints,\n                    dropout=self.dropout,\n                    bias=self.bias\n                )\n            )\n            input_channels = channels\n\n        self.model_layers = model_layers\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor = None) -> torch.Tensor:\n        for layer in self.model_layers:\n            X = layer(X, t)\n        return X\n\n\nclass DecoderResiduals(Decoder):\n    \"\"\"\n    Lớp Decoder có residual connections, nếu cần.\n    \"\"\"\n    def build_model(self) -> None:\n        super().build_model()\n        self.out = nn.Linear(self.n_frames, self.n_frames)\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor, residuals: List[torch.Tensor]) -> torch.Tensor:\n        for layer in self.model_layers:\n            out_X = layer(X, t)\n            X = out_X + residuals.pop()\n        X = self.out(X.permute(0, 1, 3, 2).contiguous()).permute(0, 1, 3, 2).contiguous()\n        return X\n\n\nclass Denoiser(nn.Module):\n    \"\"\"\n    Mô hình denoiser gốc cho diffusion.\n    \"\"\"\n    def __init__(self,\n                 input_size: int,\n                 hidden_sizes: List[int],\n                 cond_size: int = None,\n                 bias: bool = True,\n                 device: Union[str, torch.device] = 'cpu') -> None:\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.cond_size = cond_size\n        self.embedding_dim = self.cond_size\n        self.bias = bias\n        self.device = device\n\n        self.build_model()\n\n    def build_model(self) -> None:\n        self.net = nn.ModuleList()\n        self.cond_layers = nn.ModuleList() if self.cond_size is not None else None\n        n_layers = len(self.hidden_sizes)\n        input_size = self.input_size\n        for idx, next_dim in enumerate(self.hidden_sizes):\n            if self.cond_size is not None:\n                self.cond_layers.append(nn.Linear(self.cond_size, next_dim, bias=self.bias))\n            if idx == n_layers - 1:\n                self.net.append(nn.Linear(input_size, next_dim, bias=self.bias))\n            else:\n                self.net.append(nn.Sequential(\n                    nn.Linear(input_size, next_dim, bias=self.bias),\n                    nn.BatchNorm1d(next_dim),\n                    nn.ReLU(inplace=True)\n                ))\n                input_size = next_dim\n\n    def pos_encoding(self, t: torch.Tensor, channels: int) -> torch.Tensor:\n        inv_freq = 1.0 / (\n            10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n        ).to(t.device)\n        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n        return pos_enc\n\n    def forward(self, X: torch.Tensor, t: torch.Tensor, cond: torch.Tensor = None) -> torch.Tensor:\n        t = t.unsqueeze(-1).type(torch.float)\n        t = self.pos_encoding(t, self.embedding_dim)\n        if cond is not None:\n            cond = t + cond\n        else:\n            cond = t\n        for i in range(len(self.net)):\n            X = self.net[i](X)\n            if cond is not None:\n                X = X + self.cond_layers[i](cond)\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:21:45.866918Z","iopub.execute_input":"2025-06-04T05:21:45.867583Z","iopub.status.idle":"2025-06-04T05:21:45.877225Z","shell.execute_reply.started":"2025-06-04T05:21:45.867558Z","shell.execute_reply":"2025-06-04T05:21:45.876543Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/common/components.py\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"%%writefile /kaggle/working/DCMD-main/models/dcmd.py\nimport argparse\nimport os\nfrom math import prod\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\n\nfrom models.stsae.stsae import STSAE\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nfrom utils.diffusion_utils import Diffusion\nfrom utils.eval_utils import (compute_var_matrix, filter_vectors_by_cond,\n                              get_avenue_mask, get_hr_ubnormal_mask, pad_scores, score_process)\nfrom utils.model_utils import processing_data, my_kl_loss\nfrom models.transformer import MotionTransformer\nfrom utils.tools import get_dct_matrix, generate_pad, padding_traj\n\n\nclass DCMD(pl.LightningModule):\n\n    losses = {'l1': nn.L1Loss, 'smooth_l1': nn.SmoothL1Loss, 'mse': nn.MSELoss}\n    conditioning_strategies = {'inject': 'inject'}\n\n    def __init__(self, args: argparse.Namespace) -> None:\n        \"\"\"\n        This class implements DCMD model.\n\n        Args:\n            args (argparse.Namespace): arguments containing các hyperparameters\n        \"\"\"\n        super(DCMD, self).__init__()\n\n        ## Log hyperparameters\n        self.save_hyperparameters(args)\n\n        ## Thiết lập biến nội bộ\n        self.n_frames = args.seg_len\n        self.num_coords = args.num_coords\n        self.n_joints = self._infer_number_of_joint(args)\n\n        self.dropout = args.dropout\n        self.conditioning_strategy = self.conditioning_strategies[args.conditioning_strategy]\n        self.cond_h_dim = args.h_dim\n        self.cond_latent_dim = args.latent_dim\n        self.cond_channels = args.channels\n        self.cond_dropout = args.dropout\n\n        self.learning_rate = args.opt_lr\n        self.loss_fn = self.losses[args.loss_fn](reduction='none')\n        self.noise_steps = args.noise_steps\n        self.aggregation_strategy = args.aggregation_strategy\n        self.n_generated_samples = args.n_generated_samples\n        self.model_return_value = args.model_return_value\n        self.gt_path = args.gt_path\n        self.split = args.split\n        self.use_hr = args.use_hr\n        self.ckpt_dir = args.ckpt_dir\n        self.save_tensors = args.save_tensors\n\n        # SỬA: args.num_transform (không phải num_transforms)\n        self.num_transforms = args.num_transform\n        self.anomaly_score_pad_size = args.pad_size\n        self.anomaly_score_filter_kernel_size = args.filter_kernel_size\n        self.anomaly_score_frames_shift = args.frames_shift\n        self.dataset_name = args.dataset_choice\n\n        self.n_his = args.n_his\n        self.padding = args.padding\n        self.num_layers = args.num_layers\n        self.num_heads = args.num_heads\n        self.latent_dims = args.latent_dims\n        self.automatic_optimization = False\n        self.loss_1_series_weight = args.loss_1_series_weight\n        self.loss_1_prior_weight = args.loss_1_prior_weight\n        self.loss_2_series_weight = args.loss_2_series_weight\n        self.loss_2_prior_weight = args.loss_2_prior_weight\n        self.idx_pad, self.zero_index = generate_pad(self.padding, self.n_his, self.n_frames - self.n_his)\n\n        ## Khởi noise scheduler\n        self._set_diffusion_variables()\n\n        # Forecasting MLP: thay args.in_channels → args.channels\n        if args.use_forecasting:\n            self.fct_forecasting = nn.Sequential(\n                nn.Linear(args.channels[-1] * self.n_joints, args.latent_dim_forecast),\n                nn.ReLU(),\n            )\n\n        ## Xây dựng model\n        self.build_model()\n\n    def build_model(self) -> None:\n        \"\"\"\n        Build model theo các hyperparameters.\n        \"\"\"\n        args = self.hparams\n\n        # Prediction Model (Transformer)\n        pre_model = MotionTransformer(\n            input_feats=2 * self.n_joints,\n            num_frames=self.n_frames,\n            num_layers=self.num_layers,\n            num_heads=self.num_heads,\n            latent_dim=self.latent_dims,\n            dropout=self.dropout,\n            device=self.device,\n            inject_condition=(self.conditioning_strategy == 'inject')\n        )\n\n        # Reconstruction Model (STSAE)\n        default_layers = getattr(args, 'layer_channels', [128, 64, 128])\n        default_use_adaptive = getattr(args, 'use_adaptive', False)\n        default_use_jigsaw = getattr(args, 'use_jigsaw', False)\n        self.rec_model = STSAE(\n            c_in=args.channels,                  # dùng args.channels\n            h_dim=args.h_dim,\n            latent_dim=args.latent_dim,\n            n_frames=args.seg_len,\n            n_joints=self.n_joints,\n            layer_channels=args.layer_channels,\n            dropout=args.dropout,\n            device=self.device,\n            use_adaptive=getattr(args, 'use_adaptive', False),\n            use_jigsaw = getattr(args, 'use_jigsaw', False),\n            emb_dim = args.emb_dim\n\n        )\n\n        self.pre_model = pre_model\n        self.rec_model = self.rec_model\n\n        # Loss function: nếu dùng adaptive ⇒ EnhancedDCMDLoss, else MSELoss\n        from models.common.components import build_loss\n        self.criterion = build_loss(args)\n\n    def forward(self, input_data: List[torch.Tensor], aggr_strategy: str = None, return_: str = None) -> List[torch.Tensor]:\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n        tensor_data, meta_out = self._unpack_data(input_data)\n        B = tensor_data.shape[0]\n\n        history_data = tensor_data[:, :, :self.n_his, :]\n        x_0 = padding_traj(history_data, self.padding, self.idx_pad, self.zero_index)\n\n        generated_xs = []\n        for _ in range(self.n_generated_samples):\n\n            # Reconstruction\n            condition_embedding, rec_his_data = self.rec_model(history_data)\n\n            # Prediction (diffusion)\n            dct_m, idct_m = get_dct_matrix(self.n_frames)\n            dct_m_all = dct_m.float().to(self.device)\n            idct_m_all = idct_m.float().to(self.device)\n            x = x_0.permute(0, 2, 3, 1).contiguous()\n            x = x.reshape([x.shape[0], self.n_frames, -1])\n            y = torch.matmul(dct_m_all, x)\n\n            y_d = torch.randn_like(y, device=self.device)\n            for i in reversed(range(1, self.noise_steps)):\n                t = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev = torch.full(size=(B,), fill_value=i, dtype=torch.long, device=self.device)\n                t_prev[0] = 0\n                noise_pre = torch.randn_like(y_d, device=self.device) if i > 1 else torch.zeros_like(y_d, device=self.device)\n\n                predicted_noise_pre, series, prior, _ = self.pre_model(y_d, t, condition_data=condition_embedding)\n                alpha_pre = self._alpha[t][:, None, None]\n                alpha_hat_pre = self._alpha_hat[t][:, None, None]\n                beta_pre = self._beta[t][:, None, None]\n                y_d = (1 / torch.sqrt(alpha_pre)) * (y_d - ((1 - alpha_pre) / (torch.sqrt(1 - alpha_hat_pre))) * predicted_noise_pre) \\\n                    + torch.sqrt(beta_pre) * noise_pre\n\n                alpha_hat_prev = self._alpha_hat[t_prev][:, None, None]\n                y_n = (torch.sqrt(alpha_hat_prev) * y) + (torch.sqrt(1 - alpha_hat_prev) * noise_pre)\n\n                mask = torch.zeros_like(x, device=self.device)\n                for m in range(0, self.n_his):\n                    mask[:, m, :] = 1\n\n                y_d_idct = torch.matmul(idct_m_all, y_d)\n                y_n_idct = torch.matmul(idct_m_all, y_n)\n                m_mul_y_n = torch.mul(mask, y_n_idct)\n                m_mul_y_d = torch.mul((1 - mask), y_d_idct)\n                m_y = m_mul_y_d + m_mul_y_n\n                y_d = torch.matmul(dct_m_all, m_y)\n\n            pre_future_data = torch.matmul(idct_m_all, y_d)\n            pre_future_data = pre_future_data.reshape(pre_future_data.shape[0], pre_future_data.shape[1], -1, 2)\n            pre_future_data = pre_future_data.permute(0, 3, 1, 2).contiguous()\n            pre_future_data = pre_future_data[:, :, self.n_his:, :]\n\n            xs = torch.cat((rec_his_data, pre_future_data), dim=2)\n            generated_xs.append(xs)\n\n        selected_x, loss_of_selected_x = self._aggregation_strategy(generated_xs, tensor_data, aggr_strategy)\n        return self._pack_out_data(selected_x, loss_of_selected_x, [tensor_data] + meta_out, return_=return_)\n\n    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.float32:\n        opt = self.optimizers()\n        tensor_data, _ = self._unpack_data(batch)\n        history_data = tensor_data[:, :, :self.n_his, :]\n        x_0 = tensor_data\n\n        # Reconstruction Loss\n        condition_embedding, rec_his_data = self.rec_model(history_data)\n        rec_loss = torch.mean(self.loss_fn(rec_his_data, history_data))\n        self.log('rec_loss', rec_loss)\n\n        # Forecasting (nếu bật)\n        if self.hparams.use_forecasting:\n            # Dùng mạng forecasting để dự báo next frame từ history_data\n            from models.common.components import build_forecasting\n            if not hasattr(self, 'forecasting'):\n                self.forecasting = build_forecasting(self.hparams)\n            forecast_pred = self.forecasting(history_data, self.adjacency_matrix)  # (B, C, V)\n            flat = forecast_pred.view(forecast_pred.size(0), -1)  # (B, C*V)\n            forecast_emb = self.fct_forecasting(flat)  # (B, latent_dim_forecast)\n            gt_next = history_data[:, :, -1, :]  # last frame in history\n            loss_forecast = torch.mean(self.loss_fn(forecast_pred, gt_next))\n            self.log('loss_forecast', loss_forecast)\n        else:\n            forecast_emb = None\n            loss_forecast = 0.0\n\n        # Prediction Branch (diffusion)\n        dct_m, _ = get_dct_matrix(self.n_frames)\n        dct_m_all = dct_m.float().to(self.device)\n        x = x_0.permute(0, 2, 3, 1).contiguous()\n        x = x.reshape([x.shape[0], self.n_frames, -1])\n        y_0 = torch.matmul(dct_m_all, x)\n\n        t = self.noise_scheduler.sample_timesteps(y_0.shape[0]).to(self.device)\n        y_t, pre_noise = self.noise_scheduler.noise_motion(y_0, t)\n\n        if forecast_emb is not None:\n            cond_data = torch.cat([condition_embedding, forecast_emb], dim=-1)\n        else:\n            cond_data = condition_embedding\n\n        pre_predicted_noise, series, prior, _ = self.pre_model(y_t, t, condition_data=cond_data)\n\n        series_loss = 0.0\n        prior_loss = 0.0\n        for u in range(len(prior)):\n            series_loss += \\\n                (torch.mean(my_kl_loss(\n                    series[u],\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach()))\n                 + torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)).detach(),\n                    series[u])))\n            prior_loss += \\\n                (torch.mean(my_kl_loss(\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)),\n                    series[u].detach()))\n                 + torch.mean(my_kl_loss(\n                    series[u].detach(),\n                    (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.n_frames)))))\n\n        series_loss = series_loss / len(prior)\n        prior_loss = prior_loss / len(prior)\n\n        pre_loss = torch.mean(self.loss_fn(pre_predicted_noise, pre_noise))\n        self.log('pre_loss', pre_loss)\n\n        loss1 = rec_loss + pre_loss \\\n                - self.loss_1_series_weight * series_loss \\\n                + self.loss_1_prior_weight * prior_loss\n        self.log('loss1', loss1)\n        loss2 = rec_loss + pre_loss \\\n                + self.loss_2_prior_weight * prior_loss \\\n                + self.loss_2_series_weight * series_loss\n        self.log('loss2', loss2)\n\n        self.manual_backward(loss1, retain_graph=True)\n        self.manual_backward(loss2)\n        opt.step()\n        opt.zero_grad()\n\n    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        self._test_output_list.append(self.forward(batch))\n        return\n\n    def on_test_epoch_start(self) -> None:\n        super().on_test_epoch_start()\n        self._test_output_list = []\n        return\n\n    def on_test_epoch_end(self) -> float:\n        out, gt_data, trans, meta, frames = processing_data(self._test_output_list)\n        del self._test_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score)\n        return auc_score\n\n    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n        self._validation_output_list.append(self.forward(batch))\n        return\n\n    def on_validation_epoch_start(self) -> None:\n        super().on_validation_epoch_start()\n        self._validation_output_list = []\n        return\n\n    def on_validation_epoch_end(self) -> float:\n        out, gt_data, trans, meta, frames = processing_data(self._validation_output_list)\n        del self._validation_output_list\n        if self.save_tensors:\n            tensors = {'prediction': out, 'gt_data': gt_data,\n                       'trans': trans, 'metadata': meta, 'frames': frames}\n            self._save_tensors(tensors, split_name=self.split, aggr_strategy=self.aggregation_strategy, n_gen=self.n_generated_samples)\n        auc_score = self.post_processing(out, gt_data, trans, meta, frames)\n        self.log('AUC', auc_score, sync_dist=True)\n        return auc_score\n\n    def configure_optimizers(self) -> Dict:\n        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99, last_epoch=-1)\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'AUC'}\n\n    def post_processing(self, out: np.ndarray, gt_data: np.ndarray, trans: np.ndarray, meta: np.ndarray, frames: np.ndarray) -> float:\n        all_gts = [file_name for file_name in os.listdir(self.gt_path) if file_name.endswith('.npy')]\n        all_gts = sorted(all_gts)\n        scene_clips = [(int(fn.split('_')[0]), int(fn.split('_')[1].split('.')[0])) for fn in all_gts]\n        hr_ubnormal_masked_clips = get_hr_ubnormal_mask(self.split) if (self.use_hr and (self.dataset_name == 'UBnormal')) else {}\n        hr_avenue_masked_clips = get_avenue_mask() if self.dataset_name == 'HR-Avenue' else {}\n\n        num_transform = self.num_transforms\n        model_scores_transf = {}\n        dataset_gt_transf = {}\n\n        for transformation in tqdm(range(num_transform)):\n            dataset_gt = []\n            model_scores = []\n            cond_transform = (trans == transformation)\n\n            out_transform, gt_data_transform, meta_transform, frames_transform = filter_vectors_by_cond(\n                [out, gt_data, meta, frames], cond_transform)\n\n            for idx in range(len(all_gts)):\n                scene_idx, clip_idx = scene_clips[idx]\n                gt = np.load(os.path.join(self.gt_path, all_gts[idx]))\n                n_frames = gt.shape[0]\n\n                cond_scene_clip = (meta_transform[:, 0] == scene_idx) & (meta_transform[:, 1] == clip_idx)\n                out_scene_clip, gt_scene_clip, meta_scene_clip, frames_scene_clip = filter_vectors_by_cond(\n                    [out_transform, gt_data_transform, meta_transform, frames_transform],\n                    cond_scene_clip)\n\n                figs_ids = sorted(list(set(meta_scene_clip[:, 2])))\n                error_per_person = []\n                error_per_person_max_loss = []\n\n                for fig in figs_ids:\n                    cond_fig = (meta_scene_clip[:, 2] == fig)\n                    out_fig, _, frames_fig = filter_vectors_by_cond(\n                        [out_scene_clip, gt_scene_clip, frames_scene_clip], cond_fig)\n                    loss_matrix = compute_var_matrix(out_fig, frames_fig, n_frames)\n                    fig_reconstruction_loss = np.nanmax(loss_matrix, axis=0)\n                    if self.anomaly_score_pad_size != -1:\n                        fig_reconstruction_loss = pad_scores(fig_reconstruction_loss, gt, self.anomaly_score_pad_size)\n\n                    error_per_person.append(fig_reconstruction_loss)\n                    error_per_person_max_loss.append(max(fig_reconstruction_loss))\n\n                clip_score = np.stack(error_per_person, axis=0)\n                clip_score_log = np.log1p(clip_score)\n                clip_score = np.mean(clip_score, axis=0) + (np.amax(clip_score_log, axis=0) - np.amin(clip_score_log, axis=0))\n\n                if (scene_idx, clip_idx) in hr_ubnormal_masked_clips:\n                    clip_score = clip_score[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n                    gt = gt[hr_ubnormal_masked_clips[(scene_idx, clip_idx)]]\n\n                if clip_idx in hr_avenue_masked_clips:\n                    clip_score = clip_score[np.array(hr_avenue_masked_clips[clip_idx]) == 1]\n                    gt = gt[np.array(hr_avenue_masked_clips[clip_idx]) == 1]\n\n                clip_score = score_process(clip_score, self.anomaly_score_frames_shift, self.anomaly_score_filter_kernel_size)\n                model_scores.append(clip_score)\n                dataset_gt.append(gt)\n\n            model_scores = np.concatenate(model_scores, axis=0)\n            dataset_gt = np.concatenate(dataset_gt, axis=0)\n            model_scores_transf[transformation] = model_scores\n            dataset_gt_transf[transformation] = dataset_gt\n\n        pds = np.mean(np.stack(list(model_scores_transf.values()), 0), 0)\n        gt = dataset_gt_transf[0]\n        auc = roc_auc_score(gt, pds)\n        return auc\n\n    def test_on_saved_tensors(self, split_name: str) -> float:\n        tensors = self._load_tensors(split_name, self.aggregation_strategy, self.n_generated_samples)\n        auc_score = self.post_processing(tensors['prediction'], tensors['gt_data'], tensors['trans'],\n                                         tensors['metadata'], tensors['frames'])\n        print(f'AUC score: {auc_score:.6f}')\n        return auc_score\n\n    ## Helper functions\n\n    def _aggregation_strategy(self, generated_xs: List[torch.Tensor], input_sequence: torch.Tensor, aggr_strategy: str) -> Tuple[torch.Tensor]:\n        aggr_strategy = self.aggregation_strategy if aggr_strategy is None else aggr_strategy\n        if aggr_strategy == 'random':\n            return generated_xs[np.random.randint(len(generated_xs))], None\n\n        B, repr_shape = input_sequence.shape[0], input_sequence.shape[1:]\n        compute_loss = lambda x: torch.mean(self.loss_fn(x, input_sequence).reshape(-1, prod(repr_shape)), dim=-1)\n        losses = [compute_loss(x) for x in generated_xs]\n\n        if aggr_strategy == 'all':\n            dims_idxs = list(range(2, len(repr_shape) + 2))\n            dims_idxs = [1, 0] + dims_idxs\n            selected_x = torch.stack(generated_xs).permute(*dims_idxs)\n            loss_of_selected_x = torch.stack(losses).permute(1, 0)\n        elif aggr_strategy == 'mean':\n            selected_x = None\n            loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n        elif aggr_strategy == 'mean_pose':\n            selected_x = torch.mean(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'median':\n            loss_of_selected_x, _ = torch.median(torch.stack(losses), dim=0)\n            selected_x = None\n        elif aggr_strategy == 'median_pose':\n            selected_x, _ = torch.median(torch.stack(generated_xs), dim=0)\n            loss_of_selected_x = compute_loss(selected_x)\n        elif aggr_strategy == 'best' or aggr_strategy == 'worst':\n            strategy = (lambda x, y: x < y) if aggr_strategy == 'best' else (lambda x, y: x > y)\n            loss_of_selected_x = torch.full((B,), fill_value=(1e10 if aggr_strategy == 'best' else -1.), device=self.device)\n            selected_x = torch.zeros((B, *repr_shape)).to(self.device)\n            for i in range(len(generated_xs)):\n                mask = strategy(losses[i], loss_of_selected_x)\n                loss_of_selected_x[mask] = losses[i][mask]\n                selected_x[mask] = generated_xs[i][mask]\n        elif 'quantile' in aggr_strategy:\n            q = float(aggr_strategy.split(':')[-1])\n            loss_of_selected_x = torch.quantile(torch.stack(losses), q, dim=0)\n            selected_x = None\n        else:\n            raise ValueError(f'Unknown aggregation strategy {aggr_strategy}')\n\n        if selected_x is None and loss_of_selected_x is None:\n            loss_of_selected_x = torch.mean(torch.stack(losses), dim=0)\n\n        return selected_x, loss_of_selected_x\n\n    def _infer_number_of_joint(self, args: argparse.Namespace) -> int:\n        if args.headless:\n            joints_to_consider = 14\n        elif args.kp18_format:\n            joints_to_consider = 18\n        else:\n            joints_to_consider = 17\n        return joints_to_consider\n\n    def _load_tensors(self, split_name: str, aggr_strategy: str, n_gen: int) -> Dict[str, torch.Tensor]:\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        tensor_files = os.listdir(path)\n        tensors = {}\n        for t_file in tensor_files:\n            t_name = t_file.split('.')[0]\n            tensors[t_name] = torch.load(os.path.join(path, t_file))\n        return tensors\n\n    def _pack_out_data(self, selected_x: torch.Tensor, loss_of_selected_x: torch.Tensor, additional_out: List[torch.Tensor], return_: str) -> List[torch.Tensor]:\n        if return_ is None:\n            if self.model_return_value is None:\n                raise ValueError('Either return_ or self.model_return_value must be set')\n            else:\n                return_ = self.model_return_value\n\n        if return_ == 'poses':\n            out = [selected_x]\n        elif return_ == 'loss':\n            out = [loss_of_selected_x]\n        elif return_ == 'all':\n            out = []\n            if loss_of_selected_x is not None:\n                out.append(loss_of_selected_x)\n            if selected_x is not None:\n                out.append(selected_x)\n\n        return out + additional_out\n\n    def _save_tensors(self, tensors: Dict[str, torch.Tensor], split_name: str, aggr_strategy: str, n_gen: int) -> None:\n        name = 'saved_tensors_{}_{}_{}'.format(split_name, aggr_strategy, n_gen)\n        path = os.path.join(self.ckpt_dir, name)\n        if not os.path.exists(path):\n            os.mkdir(path)\n        for t_name, tensor in tensors.items():\n            torch.save(tensor, os.path.join(path, t_name + '.pt'))\n\n    def _set_diffusion_variables(self) -> None:\n        self.noise_scheduler = Diffusion(noise_steps=self.noise_steps, n_joints=self.n_joints,\n                                         device=self.device, time=self.n_frames)\n        self._beta_ = self.noise_scheduler.schedule_noise()\n        self._alpha_ = (1. - self._beta_)\n        self._alpha_hat_ = torch.cumprod(self._alpha_, dim=0)\n\n    def _unpack_data(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        tensor_data = x[0].to(self.device)\n        transformation_idx = x[1]\n        metadata = x[2]\n        actual_frames = x[3]\n        meta_out = [transformation_idx, metadata, actual_frames]\n        return tensor_data, meta_out\n\n    @property\n    def _beta(self) -> torch.Tensor:\n        return self._beta_.to(self.device)\n\n    @property\n    def _alpha(self) -> torch.Tensor:\n        return self._alpha_.to(self.device)\n\n    @property\n    def _alpha_hat(self) -> torch.Tensor:\n        return self._alpha_hat_.to(self.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T05:06:21.269686Z","iopub.execute_input":"2025-06-04T05:06:21.270576Z","iopub.status.idle":"2025-06-04T05:06:21.285753Z","shell.execute_reply.started":"2025-06-04T05:06:21.270536Z","shell.execute_reply":"2025-06-04T05:06:21.284887Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/DCMD-main/models/dcmd.py\n","output_type":"stream"}],"execution_count":108}]}